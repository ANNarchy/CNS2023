[
  {
    "objectID": "4-Networks.html",
    "href": "4-Networks.html",
    "title": "Networks",
    "section": "",
    "text": "Projections connect two populations (or views) in a uni-directional way.\nproj_exc = Projection(E, pop, 'exc')\nproj_inh = Projection(I, pop, 'inh')\nEach target ('exc', 'inh', 'AMPA', 'NMDA', 'GABA') can be defined as needed and will be treated differently by the post-synaptic neurons.\nFor rate-coded networks, the weighted sum of inputs for a specific target is accessed in the equations by sum(target):\n    equations=\"\"\"\n        tau * dx/dt + x = sum(exc) - sum(inh)\n\n        r = tanh(x)\n    \"\"\"\nFor spiking networks, the corresponding conductance increased after each incoming spike is accessed by g_target:\n    equations=\"\"\"\n        C * dv/dt = gL * (E_L - v) + g_exc * (E_e - v) + g_inh * (E_i - v)\n    \"\"\"\n\n\nProjections must be populated with a connectivity matrix (who is connected to who), a weight w and optionally a delay d (uniform or variable).\nSeveral patterns are predefined:\nproj.connect_all_to_all(weights=Normal(0.0, 1.0), delays=2.0, allow_self_connections=False)\n\nproj.connect_one_to_one(weights=1.0, delays=Uniform(1.0, 10.0))\n\nproj.connect_fixed_number_pre(number=20, weights=1.0)\n\nproj.connect_fixed_number_post(number=20, weights=1.0)\n\nproj.connect_fixed_probability(probability=0.2, weights=1.0)\n\nproj.connect_gaussian(amp=1.0, sigma=0.2, limit=0.001)\n\nproj.connect_dog(amp_pos=1.0, sigma_pos=0.2, amp_neg=0.3, sigma_neg=0.7, limit=0.001)\n\n\n\n\n\nBut you can also load Numpy arrays or Scipy sparse matrices. Example for synfire chains:\nw = np.array([[None]*pre.size]*post.size)\n\nfor i in range(post.size):\n    w[i, (i-1)%pre.size] = 1.0\n\nproj.connect_from_matrix(w)\nSparse variant:\nw = lil_matrix((pre.size, post.size))\n\nfor i in range(pre.size):\n    w[pre.size, (i+1)%post.size] = 1.0\n\nproj.connect_from_sparse(w)\n\n\n\n\nA pre-synaptic spike arriving to a spiking neuron increases the conductance/current g_target (e.g. g_exc or g_inh, depending on the projection).\nLIF = Neuron(\n    parameters = \"...\",\n    equations = \"\"\"\n        C * dv/dt = gL * (E_L - v) + g_exc * (E_e - v): init=E_L    \n    \"\"\",\n    spike = \"v &gt;= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\nBy default (instantaneous transmission), each incoming spike increments instantaneously g_target from the synaptic efficiency w of the corresponding synapse.\n\n    g(t) = \\sum_{t_pre} w \\times \\delta(t - t_\\text{pre})\n\nFor exponentially-decreasing or alpha-shaped synapses, ODEs have to be additionally introduced for the dynamics of the conductance/current when the neuron does not receive a spike.\n\n    \\tau_\\text{exc} \\, \\dfrac{d g(t)}{dt} + g(t) = 0\n\n\n    \\tau_\\text{exc} \\, \\dfrac{d \\alpha(t)}{dt} + \\alpha(t) = g(t)\n\n\n\n\n\n\nThe exponential numerical method should be preferred for these first-order linear ODEs, as integration is then exact.\nLIF = Neuron(\n    parameters = \"...\",\n    equations = \"\"\"\n        C * dv/dt = gL * (E_L - v) + g_exc * (E_e - v) : init=E_L   \n\n        tau_exc * dg_exc/dt = - g_exc : exponential\n        tau_exc * dalpha/dt = g_exc - alpha : exponential\n    \"\"\",\n    spike = \"v &gt;= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\n\n\nNotebook: Synaptic transmission\n\n\n\nDownload the Jupyter notebook: SynapticTransmission.ipynb or run it directly on colab.\n\n\n\n\n\n\n\n\nNotebook: COBA - Conductance-based E/I network\n\n\n\nDownload the Jupyter notebook: COBA.ipynb or run it directly on colab."
  },
  {
    "objectID": "4-Networks.html#projections",
    "href": "4-Networks.html#projections",
    "title": "Networks",
    "section": "",
    "text": "Projections connect two populations (or views) in a uni-directional way.\nproj_exc = Projection(E, pop, 'exc')\nproj_inh = Projection(I, pop, 'inh')\nEach target ('exc', 'inh', 'AMPA', 'NMDA', 'GABA') can be defined as needed and will be treated differently by the post-synaptic neurons.\nFor rate-coded networks, the weighted sum of inputs for a specific target is accessed in the equations by sum(target):\n    equations=\"\"\"\n        tau * dx/dt + x = sum(exc) - sum(inh)\n\n        r = tanh(x)\n    \"\"\"\nFor spiking networks, the corresponding conductance increased after each incoming spike is accessed by g_target:\n    equations=\"\"\"\n        C * dv/dt = gL * (E_L - v) + g_exc * (E_e - v) + g_inh * (E_i - v)\n    \"\"\"\n\n\nProjections must be populated with a connectivity matrix (who is connected to who), a weight w and optionally a delay d (uniform or variable).\nSeveral patterns are predefined:\nproj.connect_all_to_all(weights=Normal(0.0, 1.0), delays=2.0, allow_self_connections=False)\n\nproj.connect_one_to_one(weights=1.0, delays=Uniform(1.0, 10.0))\n\nproj.connect_fixed_number_pre(number=20, weights=1.0)\n\nproj.connect_fixed_number_post(number=20, weights=1.0)\n\nproj.connect_fixed_probability(probability=0.2, weights=1.0)\n\nproj.connect_gaussian(amp=1.0, sigma=0.2, limit=0.001)\n\nproj.connect_dog(amp_pos=1.0, sigma_pos=0.2, amp_neg=0.3, sigma_neg=0.7, limit=0.001)\n\n\n\n\n\nBut you can also load Numpy arrays or Scipy sparse matrices. Example for synfire chains:\nw = np.array([[None]*pre.size]*post.size)\n\nfor i in range(post.size):\n    w[i, (i-1)%pre.size] = 1.0\n\nproj.connect_from_matrix(w)\nSparse variant:\nw = lil_matrix((pre.size, post.size))\n\nfor i in range(pre.size):\n    w[pre.size, (i+1)%post.size] = 1.0\n\nproj.connect_from_sparse(w)"
  },
  {
    "objectID": "4-Networks.html#conductances-currents",
    "href": "4-Networks.html#conductances-currents",
    "title": "Networks",
    "section": "",
    "text": "A pre-synaptic spike arriving to a spiking neuron increases the conductance/current g_target (e.g. g_exc or g_inh, depending on the projection).\nLIF = Neuron(\n    parameters = \"...\",\n    equations = \"\"\"\n        C * dv/dt = gL * (E_L - v) + g_exc * (E_e - v): init=E_L    \n    \"\"\",\n    spike = \"v &gt;= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\nBy default (instantaneous transmission), each incoming spike increments instantaneously g_target from the synaptic efficiency w of the corresponding synapse.\n\n    g(t) = \\sum_{t_pre} w \\times \\delta(t - t_\\text{pre})\n\nFor exponentially-decreasing or alpha-shaped synapses, ODEs have to be additionally introduced for the dynamics of the conductance/current when the neuron does not receive a spike.\n\n    \\tau_\\text{exc} \\, \\dfrac{d g(t)}{dt} + g(t) = 0\n\n\n    \\tau_\\text{exc} \\, \\dfrac{d \\alpha(t)}{dt} + \\alpha(t) = g(t)\n\n\n\n\n\n\nThe exponential numerical method should be preferred for these first-order linear ODEs, as integration is then exact.\nLIF = Neuron(\n    parameters = \"...\",\n    equations = \"\"\"\n        C * dv/dt = gL * (E_L - v) + g_exc * (E_e - v) : init=E_L   \n\n        tau_exc * dg_exc/dt = - g_exc : exponential\n        tau_exc * dalpha/dt = g_exc - alpha : exponential\n    \"\"\",\n    spike = \"v &gt;= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\n\n\nNotebook: Synaptic transmission\n\n\n\nDownload the Jupyter notebook: SynapticTransmission.ipynb or run it directly on colab.\n\n\n\n\n\n\n\n\nNotebook: COBA - Conductance-based E/I network\n\n\n\nDownload the Jupyter notebook: COBA.ipynb or run it directly on colab."
  },
  {
    "objectID": "5-Plasticity.html",
    "href": "5-Plasticity.html",
    "title": "Synapse models",
    "section": "",
    "text": "More complex synaptic behaviors (short-term plasticity, long-term plasticity, etc) can be defined in a synapse model.\n\n\n\n\n\nSpiking synapses can define a pre_spike field, defining what happens when a pre-synaptic spike arrives at the synapse.\ng_target is an alias for the corresponding post-synaptic conductance: it will be replaced by g_exc or g_inh depending on how the synapse is used.\nBy default, a pre-synaptic spike increments the post-synaptic conductance from w: g_target += w\nDefaultSynapse = Synapse(\n    pre_spike=\"\"\"\n        g_target += w \n    \"\"\"\n)\n\nproj = Projection(pop, pop, 'exc', DefaultSynapse)\nA Synapse object can also have parameters and variables that are updated at each time step of the simulation (expensive) or when needed (event-driven).\n\n\nLet’s implement short-term plasticity (STP) (Tsodyks, Uziel and Markram, 2000). Short-term plasticity, also called dynamical synapses, refers to a phenomenon in which synaptic efficacy changes over time in a way that reflects the history of presynaptic activity1.\n\n\n\nExperimental demonstration of facilitating and depressing synapses. Source: Morrison et al. (2008)\n\n\nWe define a STP synapse, whose post-pynaptic potential (psp, define by g_target) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u.\nThe synaptic variables x and u generally follow linear ODEs:\n\\tau_\\text{rec} \\, \\dfrac{d x(t)}{dt} + x(t) = 1 \\tau_\\text{facil} \\, \\dfrac{d u(t)}{dt} + u(t) = 0.1\nWhen a pre-synaptic spike arrives at the synapse, the following updates are applied asynchronously:\nx(t) \\leftarrow (1 - u(t)) \\times x(t) u(t) \\leftarrow u(t) + 0.1 \\, (1 - u(t)) \nand the post-synaptic conductance is increased from:\ng(t) \\leftarrow g(t) + w \\, u(t) \\, x(t)\nSTP = Synapse(\n    parameters = \"\"\"\n        tau_rec = 1.0\n        tau_facil = 1.0\n        U = 0.1\n    \"\"\",\n    equations = \"\"\"\n        dx/dt = (1 - x) / tau_rec : init = 1.0, event-driven\n        du/dt = (U - u) / tau_facil : init = 0.1, event-driven   \n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w * u * x\n        x *= (1 - u)\n        u += U * (1 - u)\n    \"\"\"\n)\n\n\n\n\n\n\nNotebook: STP\n\n\n\nDownload the Jupyter notebook: STP.ipynb\nRun it directly on colab: STP.ipynb\n\n\n\n\n\n\n\n\nSTDP depends on pre-post spike timing differences. Source: Bi and Poo (2001).\n\n\nThe weight change of a synapse from a presynaptic neuron j depends on the relative timing between presynaptic spike arrivals and postsynaptic spikes2, i.e. on the the causation between the neuron’s firing patterns:\n\nIf the pre-synaptic neuron fires before the post-synaptic one, the weight is increased (long-term potentiation). Pre causes Post to fire.\nIf it fires after, the weight is decreased (long-term depression). Pre does not cause Post to fire.\n\nThe STDP (spike-timing dependent plasticity, Bi and Poo, 2001) plasticity rule describes how the weight of a synapse evolves when the pre-synaptic neuron fires at t_\\text{pre} and the post-synaptic one fires at t_\\text{post}.\n\n\\Delta w = \\begin{cases}\n    A^+ \\, \\exp (- \\dfrac{t_\\text{post} - t_\\text{pre}}{\\tau^+}) \\qquad \\text{if} \\qquad t_\\text{post} &gt; t_\\text{pre} \\\\\n     - A^- \\, \\exp (\\dfrac{t_\\text{post} - t_\\text{pre}}{\\tau^-}) \\qquad \\text{if} \\qquad t_\\text{post} &lt; t_\\text{pre} \\\\\n\\end{cases}\n\nSTDP can be implemented online using traces, which is much more efficient. More complex variants of STDP (triplet STDP) exist, but this is the main model of synaptic plasticity in spiking networks.\nSimilarly to pre_spike, post_spike defines what happens when a post-synaptic spike is emitted. This can be used to implement the event-driven version of STDP using traces.\nSTDP = Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection ; tau_minus = 20.0 : projection\n        A_plus = 0.01 : projection   ; A_minus = 0.01 : projection\n        w_min = 0.0 : projection     ; w_max = 1.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_plus  * dx/dt = -x : event-driven # pre-synaptic trace\n        tau_minus * dy/dt = -y : event-driven # post-synaptic trace\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        x += A_plus * w_max\n        w = clip(w + y, w_min , w_max)\n    \"\"\",\n    post_spike=\"\"\"\n        y -= A_minus * w_max\n        w = clip(w + x, w_min , w_max)\n    \"\"\")\n\n\n\n\n\n\nNotebook: STDP\n\n\n\nDownload the Jupyter notebook: STDP.ipynb or run it directly on colab."
  },
  {
    "objectID": "5-Plasticity.html#short-term-plasticity-stp",
    "href": "5-Plasticity.html#short-term-plasticity-stp",
    "title": "Synapse models",
    "section": "",
    "text": "Let’s implement short-term plasticity (STP) (Tsodyks, Uziel and Markram, 2000). Short-term plasticity, also called dynamical synapses, refers to a phenomenon in which synaptic efficacy changes over time in a way that reflects the history of presynaptic activity1.\n\n\n\nExperimental demonstration of facilitating and depressing synapses. Source: Morrison et al. (2008)\n\n\nWe define a STP synapse, whose post-pynaptic potential (psp, define by g_target) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u.\nThe synaptic variables x and u generally follow linear ODEs:\n\\tau_\\text{rec} \\, \\dfrac{d x(t)}{dt} + x(t) = 1 \\tau_\\text{facil} \\, \\dfrac{d u(t)}{dt} + u(t) = 0.1\nWhen a pre-synaptic spike arrives at the synapse, the following updates are applied asynchronously:\nx(t) \\leftarrow (1 - u(t)) \\times x(t) u(t) \\leftarrow u(t) + 0.1 \\, (1 - u(t)) \nand the post-synaptic conductance is increased from:\ng(t) \\leftarrow g(t) + w \\, u(t) \\, x(t)\nSTP = Synapse(\n    parameters = \"\"\"\n        tau_rec = 1.0\n        tau_facil = 1.0\n        U = 0.1\n    \"\"\",\n    equations = \"\"\"\n        dx/dt = (1 - x) / tau_rec : init = 1.0, event-driven\n        du/dt = (U - u) / tau_facil : init = 0.1, event-driven   \n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w * u * x\n        x *= (1 - u)\n        u += U * (1 - u)\n    \"\"\"\n)\n\n\n\n\n\n\nNotebook: STP\n\n\n\nDownload the Jupyter notebook: STP.ipynb\nRun it directly on colab: STP.ipynb"
  },
  {
    "objectID": "5-Plasticity.html#spike-timing-dependent-plasticity-stdp",
    "href": "5-Plasticity.html#spike-timing-dependent-plasticity-stdp",
    "title": "Synapse models",
    "section": "",
    "text": "STDP depends on pre-post spike timing differences. Source: Bi and Poo (2001).\n\n\nThe weight change of a synapse from a presynaptic neuron j depends on the relative timing between presynaptic spike arrivals and postsynaptic spikes2, i.e. on the the causation between the neuron’s firing patterns:\n\nIf the pre-synaptic neuron fires before the post-synaptic one, the weight is increased (long-term potentiation). Pre causes Post to fire.\nIf it fires after, the weight is decreased (long-term depression). Pre does not cause Post to fire.\n\nThe STDP (spike-timing dependent plasticity, Bi and Poo, 2001) plasticity rule describes how the weight of a synapse evolves when the pre-synaptic neuron fires at t_\\text{pre} and the post-synaptic one fires at t_\\text{post}.\n\n\\Delta w = \\begin{cases}\n    A^+ \\, \\exp (- \\dfrac{t_\\text{post} - t_\\text{pre}}{\\tau^+}) \\qquad \\text{if} \\qquad t_\\text{post} &gt; t_\\text{pre} \\\\\n     - A^- \\, \\exp (\\dfrac{t_\\text{post} - t_\\text{pre}}{\\tau^-}) \\qquad \\text{if} \\qquad t_\\text{post} &lt; t_\\text{pre} \\\\\n\\end{cases}\n\nSTDP can be implemented online using traces, which is much more efficient. More complex variants of STDP (triplet STDP) exist, but this is the main model of synaptic plasticity in spiking networks.\nSimilarly to pre_spike, post_spike defines what happens when a post-synaptic spike is emitted. This can be used to implement the event-driven version of STDP using traces.\nSTDP = Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection ; tau_minus = 20.0 : projection\n        A_plus = 0.01 : projection   ; A_minus = 0.01 : projection\n        w_min = 0.0 : projection     ; w_max = 1.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_plus  * dx/dt = -x : event-driven # pre-synaptic trace\n        tau_minus * dy/dt = -y : event-driven # post-synaptic trace\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        x += A_plus * w_max\n        w = clip(w + y, w_min , w_max)\n    \"\"\",\n    post_spike=\"\"\"\n        y -= A_minus * w_max\n        w = clip(w + x, w_min , w_max)\n    \"\"\")\n\n\n\n\n\n\nNotebook: STDP\n\n\n\nDownload the Jupyter notebook: STDP.ipynb or run it directly on colab."
  },
  {
    "objectID": "5-Plasticity.html#footnotes",
    "href": "5-Plasticity.html#footnotes",
    "title": "Synapse models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttp://www.scholarpedia.org/article/Short-term_synaptic_plasticity↩︎\nhttp://www.scholarpedia.org/article/Spike-timing_dependent_plasticity↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simulation of spiking and BOLD signals using the ANNarchy simulator",
    "section": "",
    "text": "ANNarchy (Artificial Neural Networks architect) is a neuro-simulator for rate-coded and spiking neural networks in Python.\nThe source code is available at https://github.com/ANNarchy/ANNarchy, while the full documentation is at: https://annarchy.github.io.\n\nProgram\n\n14:00 - 15:30 : Introduction to ANNarchy and spiking neural networks\n15:30 - 16:00 : Coffee break\n16:00 - 17:20 : BOLD monitoring and fitting"
  },
  {
    "objectID": "1-Rationale.html",
    "href": "1-Rationale.html",
    "title": "Why ANNarchy?",
    "section": "",
    "text": "There already exist several neuro-simulators for computational neuroscience, each focusing on different levels of description (multi-compartmental, spiking, rate-coded, etc.) or supported hardware (CPU, GPU).\nSome simulators provide fixed libraries of neural and synaptic models:\n\nNEURON https://neuron.yale.edu/neuron/\n\nMulti-compartmental models, spiking neurons (CPU)\n\nGENESIS http://genesis-sim.org/\n\nMulti-compartmental models, spiking neurons (CPU)\n\nNEST https://nest-initiative.org/\n\nSpiking neurons (CPU, GPU)\n\nGeNN https://genn-team.github.io/genn/\n\nSpiking neurons (GPU, CPU)\n\nCARLsim https://sites.socsci.uci.edu/~jkrichma/CARLsim/\n\nSpiking neurons (GPU, CPU)\n\nAuryn https://fzenke.net/auryn/doku.php\n\nSpiking neurons (CPU)\n\n\nSome, such as ANNarchy, rely instead on code generation, allowing virtual any model to be implemented:\n\nBrian https://briansimulator.org/\n\nSpiking neurons (CPU)\n\nBrian2CUDA https://github.com/brian-team/brian2cuda\n\nSpiking neurons (GPU)\n\nANNarchy https://github.com/ANNarchy/ANNarchy\n\nRate-coded and spiking neurons (CPU, GPU)"
  },
  {
    "objectID": "1-Rationale.html#neuro-simulators",
    "href": "1-Rationale.html#neuro-simulators",
    "title": "Why ANNarchy?",
    "section": "",
    "text": "There already exist several neuro-simulators for computational neuroscience, each focusing on different levels of description (multi-compartmental, spiking, rate-coded, etc.) or supported hardware (CPU, GPU).\nSome simulators provide fixed libraries of neural and synaptic models:\n\nNEURON https://neuron.yale.edu/neuron/\n\nMulti-compartmental models, spiking neurons (CPU)\n\nGENESIS http://genesis-sim.org/\n\nMulti-compartmental models, spiking neurons (CPU)\n\nNEST https://nest-initiative.org/\n\nSpiking neurons (CPU, GPU)\n\nGeNN https://genn-team.github.io/genn/\n\nSpiking neurons (GPU, CPU)\n\nCARLsim https://sites.socsci.uci.edu/~jkrichma/CARLsim/\n\nSpiking neurons (GPU, CPU)\n\nAuryn https://fzenke.net/auryn/doku.php\n\nSpiking neurons (CPU)\n\n\nSome, such as ANNarchy, rely instead on code generation, allowing virtual any model to be implemented:\n\nBrian https://briansimulator.org/\n\nSpiking neurons (CPU)\n\nBrian2CUDA https://github.com/brian-team/brian2cuda\n\nSpiking neurons (GPU)\n\nANNarchy https://github.com/ANNarchy/ANNarchy\n\nRate-coded and spiking neurons (CPU, GPU)"
  },
  {
    "objectID": "1-Rationale.html#key-features-of-annarchy",
    "href": "1-Rationale.html#key-features-of-annarchy",
    "title": "Why ANNarchy?",
    "section": "Key features of ANNarchy",
    "text": "Key features of ANNarchy\n\nSimulation of both rate-coded and spiking neural networks.\nOnly local biologically realistic mechanisms are possible (no backpropagation).\nEquation-oriented description of neural/synaptic dynamics (à la Brian).\nCode generation in C++, parallelized using OpenMP on CPU and CUDA on GPU (MPI is coming).\nSeveral sparse matrix formats to optimize the implementation of a specific network on CPUs and GPUs, which are selected using heuristics and auto-tuining.\nSynaptic, intrinsic and structural plasticity mechanisms.\nBOLD monitoring from spiking or rate-coded neural networks.\n\nMaith et al. (2022)\nBOLD Monitoring in the Neural Simulator ANNarchy.\nFrontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966\nMaith et al. (2020)\nA computational model-based analysis of basal ganglia pathway changes in Parkinson’s disease inferred from resting-state fMRI.\nEuropean Journal of Neuroscience. doi:10.1111/ejn.14868"
  },
  {
    "objectID": "1-Rationale.html#parallel-performance",
    "href": "1-Rationale.html#parallel-performance",
    "title": "Why ANNarchy?",
    "section": "Parallel performance",
    "text": "Parallel performance\nMost importantly, ANNarchy is fast on shared memory systems utilizing multi-core CPUs and GPUs. Depending on the model, it is comparable to Brian2 and faster than NEST on standard consumer PCs.\n\n\n\nCOBA Benchmark with 4000 neurons using an AMD Ryzen7 2700X / NVIDIA RTX 2060. See Dinkelbach et al. (2019).\n\n\n\n\n\nCOBA Benchmark with varying neurons using an AMD Ryzen7 2700X / NVIDIA RTX 2060. See Dinkelbach et al. (2019).\n\n\n\n\n\nCOBA Benchmark with varying neurons using an AMD Ryzen7 2700X / NVIDIA RTX 2060. See Dinkelbach et al. (2019)."
  },
  {
    "objectID": "7-BOLD.html",
    "href": "7-BOLD.html",
    "title": "BOLD signals",
    "section": "",
    "text": "The provided BOLD models follow the Balloon model (Buxton et al., 1998) with the different variations studied in (Stephan et al., 2007). Those models all compute the vascular response to neural activity through a dampened oscillator:\n\n    \\frac{ds}{dt} = \\phi \\, I_\\text{CBF} - \\kappa \\, s - \\gamma \\, (f_{in} - 1)\n\n\n    \\frac{df_{in}}{dt} = s\n\nThis allows to compute the oxygen extraction fraction through the inflated balloon model:\n\n    E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} }\n\nThe (normalized) venous blood volume is computed as:\n\n    \\tau_0 \\, \\frac{dv}{dt} = (f_{in} - f_{out})\n\n\n    f_{out} = v^{\\frac{1}{\\alpha}}\n\nThe level of deoxyhemoglobin into the venous compartment is computed by:\n\n    \\tau_0 \\, \\frac{dq}{dt} = f_{in} \\, \\frac{E}{E_0} - \\frac{q}{v} \\, f_{out}\n\nUsing the two signals v and q, there are two ways to compute the corresponding BOLD signal:\n\nN: Non-linear BOLD equation:\n\n\n    BOLD = v_0 \\, ( k_1 \\, (1-q) + k_2 \\, (1- \\dfrac{q}{v}) + k_3 \\, (1 - v) )\n\n\nL: Linear BOLD equation:\n\n\n    BOLD = v_0 \\, ((k_1 + k_2) \\, (1 - q) + (k_3 - k_2) \\, (1 - v))\n\nAdditionally, the three coefficients k_1, k_2, k_3 can be computed in two different ways:\n\nC: classical coefficients from (Buxton et al., 1998):\n\nk_1            = (1 - v_0) \\, 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\nk_2            = 2 \\, E_0\nk_3            = 1 - \\epsilon\n\nR: revised coefficients from (Obata et al., 2004):\n\nk_1            = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\nk_2            = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\nk_3            = 1 - \\epsilon\nThis makes a total of four different BOLD model (RN, RL, CN, CL) which are provided by the extension.\nDefining a custom model is easy in ANNarchy, as we have linear first-order ODES as with rate-coded neurons!\nballoon_RN = BoldModel(\n    parameters = \"\"\"\n        phi       = 1.0         ;   kappa     = 1/1.54\n        gamma     = 1/2.46      ;   E_0       = 0.34\n        tau       = 0.98        ;   alpha     = 0.33\n        V_0       = 0.02        ;   v_0       = 40.3\n        TE        = 40/1000.    ;   epsilon   = 1.43\n        r_0       = 25.         ;   second    = 1000.0\n    \"\"\",\n    equations = \"\"\"\n        # CBF input\n        I_CBF          = sum(I_CBF)       \n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  \n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        # Balloon model\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear BOLD equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))\n    \"\"\",\n    inputs=['I_CBF']\n)\nThe BoldModel can then be passed to a BoldMonitor that will map the output variable (r) of one or more populations onto the input I_CBF and apply the balloon model:\nm_bold = BoldMonitor(\n    \n    populations = [pop], # recorded populations\n    \n    bold_model = balloon_RN(), # BOLD model to use (default is balloon_RN)\n    \n    mapping = {'I_CBF': 'r'}, # mapping from pop.r to I_CBF\n    \n    normalize_input = 2000,  # time window to compute baseline.\n    \n    recorded_variables = [\"I_CBF\", \"BOLD\"]  # variables to be recorded\n)\nThe BoldMonitorcan be used as a regular Monitor to record the BOLD signal during a (long) simulation:\nsimulate(20000)     # 20s with low noise\n\nbold_data = m_bold.get(\"BOLD\")\n\n\n\n\n\n\nNotebook: BOLD monitoring\n\n\n\nDownload the Jupyter notebook: BoldMonitoring.ipynb or run it directly on colab.\n\n\n\n\n\n\n\n\nNotebook: BOLD models\n\n\n\nDownload the Jupyter notebook: BoldModel.ipynb or run it directly on colab.\n\n\n\n\n\n\n\n\nNotebook: BOLD search\n\n\n\nDownload the Jupyter notebook: BoldSearch.ipynb or run it directly on colab.\n\n\n\n\n\nMaith et al. (2020)\nA computational model-based analysis of basal ganglia pathway changes in Parkinson’s disease inferred from resting-state fMRI.\nEuropean Journal of Neuroscience. doi:10.1111/ejn.14868\n\n\n\n\n\n\n\n\n\n\n\nNotebook: BOLD monitoring using a basal ganglia model\n\n\n\nDownload the Jupyter notebook: BoldParkinson.ipynb or run it directly on colab."
  },
  {
    "objectID": "7-BOLD.html#balloon-models",
    "href": "7-BOLD.html#balloon-models",
    "title": "BOLD signals",
    "section": "",
    "text": "The provided BOLD models follow the Balloon model (Buxton et al., 1998) with the different variations studied in (Stephan et al., 2007). Those models all compute the vascular response to neural activity through a dampened oscillator:\n\n    \\frac{ds}{dt} = \\phi \\, I_\\text{CBF} - \\kappa \\, s - \\gamma \\, (f_{in} - 1)\n\n\n    \\frac{df_{in}}{dt} = s\n\nThis allows to compute the oxygen extraction fraction through the inflated balloon model:\n\n    E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} }\n\nThe (normalized) venous blood volume is computed as:\n\n    \\tau_0 \\, \\frac{dv}{dt} = (f_{in} - f_{out})\n\n\n    f_{out} = v^{\\frac{1}{\\alpha}}\n\nThe level of deoxyhemoglobin into the venous compartment is computed by:\n\n    \\tau_0 \\, \\frac{dq}{dt} = f_{in} \\, \\frac{E}{E_0} - \\frac{q}{v} \\, f_{out}\n\nUsing the two signals v and q, there are two ways to compute the corresponding BOLD signal:\n\nN: Non-linear BOLD equation:\n\n\n    BOLD = v_0 \\, ( k_1 \\, (1-q) + k_2 \\, (1- \\dfrac{q}{v}) + k_3 \\, (1 - v) )\n\n\nL: Linear BOLD equation:\n\n\n    BOLD = v_0 \\, ((k_1 + k_2) \\, (1 - q) + (k_3 - k_2) \\, (1 - v))\n\nAdditionally, the three coefficients k_1, k_2, k_3 can be computed in two different ways:\n\nC: classical coefficients from (Buxton et al., 1998):\n\nk_1            = (1 - v_0) \\, 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\nk_2            = 2 \\, E_0\nk_3            = 1 - \\epsilon\n\nR: revised coefficients from (Obata et al., 2004):\n\nk_1            = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\nk_2            = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\nk_3            = 1 - \\epsilon\nThis makes a total of four different BOLD model (RN, RL, CN, CL) which are provided by the extension.\nDefining a custom model is easy in ANNarchy, as we have linear first-order ODES as with rate-coded neurons!\nballoon_RN = BoldModel(\n    parameters = \"\"\"\n        phi       = 1.0         ;   kappa     = 1/1.54\n        gamma     = 1/2.46      ;   E_0       = 0.34\n        tau       = 0.98        ;   alpha     = 0.33\n        V_0       = 0.02        ;   v_0       = 40.3\n        TE        = 40/1000.    ;   epsilon   = 1.43\n        r_0       = 25.         ;   second    = 1000.0\n    \"\"\",\n    equations = \"\"\"\n        # CBF input\n        I_CBF          = sum(I_CBF)       \n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  \n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        # Balloon model\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear BOLD equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))\n    \"\"\",\n    inputs=['I_CBF']\n)\nThe BoldModel can then be passed to a BoldMonitor that will map the output variable (r) of one or more populations onto the input I_CBF and apply the balloon model:\nm_bold = BoldMonitor(\n    \n    populations = [pop], # recorded populations\n    \n    bold_model = balloon_RN(), # BOLD model to use (default is balloon_RN)\n    \n    mapping = {'I_CBF': 'r'}, # mapping from pop.r to I_CBF\n    \n    normalize_input = 2000,  # time window to compute baseline.\n    \n    recorded_variables = [\"I_CBF\", \"BOLD\"]  # variables to be recorded\n)\nThe BoldMonitorcan be used as a regular Monitor to record the BOLD signal during a (long) simulation:\nsimulate(20000)     # 20s with low noise\n\nbold_data = m_bold.get(\"BOLD\")\n\n\n\n\n\n\nNotebook: BOLD monitoring\n\n\n\nDownload the Jupyter notebook: BoldMonitoring.ipynb or run it directly on colab.\n\n\n\n\n\n\n\n\nNotebook: BOLD models\n\n\n\nDownload the Jupyter notebook: BoldModel.ipynb or run it directly on colab.\n\n\n\n\n\n\n\n\nNotebook: BOLD search\n\n\n\nDownload the Jupyter notebook: BoldSearch.ipynb or run it directly on colab."
  },
  {
    "objectID": "7-BOLD.html#basal-ganglia-pathway-changes-in-parkinsons-disease",
    "href": "7-BOLD.html#basal-ganglia-pathway-changes-in-parkinsons-disease",
    "title": "BOLD signals",
    "section": "",
    "text": "Maith et al. (2020)\nA computational model-based analysis of basal ganglia pathway changes in Parkinson’s disease inferred from resting-state fMRI.\nEuropean Journal of Neuroscience. doi:10.1111/ejn.14868\n\n\n\n\n\n\n\n\n\n\n\nNotebook: BOLD monitoring using a basal ganglia model\n\n\n\nDownload the Jupyter notebook: BoldParkinson.ipynb or run it directly on colab."
  },
  {
    "objectID": "6-Additional.html",
    "href": "6-Additional.html",
    "title": "Additional features",
    "section": "",
    "text": "ANNarchy provides standard models from the PyNN interface:\nhttps://neuralensemble.org/PyNN/\n\n\n\nLeakyIntegrator\nIzhikevich\nIF_curr_exp\nIF_cond_exp\nIF_curr_alpha\nIF_cond_alpha\nHH_cond_exp\nEIF_cond_exp_isfa_ista\nEIF_cond_alpha_isfa_ista\n\n\n\n\n\nHebb\nOja\niBCM\nSTP\nSTDP\n\n\n\n\n\nSee https://annarchy.github.io/manual/Inputs.html\n\n\ninputs = np.array(\n    [\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n    ]\n)\n\ninp = TimedArray(rates=inputs, \n    schedule=[0., 10., 30., 60., 100., 150., 210., 280., 360., 450.])\n\ncompile()\n\nsimulate(500.)\n\n\n\ninp = Population(100, Neuron(equations=\"r = sin(t)\"))\n\npop = Population(100, Izhikevich)\n\nproj = CurrentInjection(inp, pop, 'exc')\nproj.connect_current()\n\n\n\nspike_times = [\n  [  10 + 20*i for i in range(100)]\n  [  11 + 20*i for i in range(100)]\n]\n\npop = SpikeSourceArray(spike_times=spike_times)\n\n\n\npop = PoissonPopulation(100, rates=30.)\n\npop = PoissonPopulation(100, rates=np.linspace(0.0, 100.0, 100))\n\npop = PoissonPopulation(\n    geometry=100,\n    parameters = \"\"\"\n        amp = 100.0\n        frequency = 50.0\n    \"\"\",\n    rates=\"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\"\n)\nor from a TimedArray.\n\n\n\npop_poisson = PoissonPopulation(200, rates=10.)\npop_corr    = HomogeneousCorrelatedSpikeTrains(200, rates=10., corr=0.3, tau=10.)\n\n\n\n\n\n\n\n\n\n\n\n\npop = Population( ... , stop_condition = \"r &gt; 1.0\")\n\ncompile()\n\nt = simulate_until(max_duration=1000.0, populations=pop)\n\n\n\npop1 = PoissonPopulation(100, rates=10.0)\npop2 = Population(100, Izhikevich)\nproj = Projection(pop1, pop2, 'exc')\nproj.connect_fixed_probability(weights=5.0, probability=0.2)\nm = Monitor(pop2, 'spike')\n\ncompile()\n\ndef simulation(idx, net):\n    net.get(pop1).rates = 10. * idx\n    net.simulate(1000.)\n    return net.get(m).raster_plot()\n\nresults = parallel_run(method=simulation, number=3)\n\nt1, n1 = results[0]\nt2, n2 = results[1]\nt3, n3 = results[2]\n\n\n\n\nCreatingSynapse = Synapse(\n    parameters = \" ... \",\n    equations = \" ... \",\n    creating = \"pre.mean_r * post.mean_r &gt; 0.7 : proba = 0.5, w = 1.0\"\n)\n\nPruningSynapse = Synapse(\n    parameters = \" T = 100000 : int, projection \",\n    equations = \"\"\"\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n              else :\n                age + 1 : init = 0, int\"\"\",\n    pruning = \"age &gt; T : proba = 0.5\"\n)\n\n\n\nfrom ANNarchy import *\nfrom ANNarchy.extensions.convolution import *\n\ninp = Population(geometry=(100, 100), neuron=Neuron(parameters=\"r = 0.0\"))\nconv = Population(geometry=(100, 100), neuron=Neuron(equations=\"r = sum(exc)\"))\npool = Population(geometry=(50, 50), neuron=Neuron(equations=\"r = sum(exc)\"))\n\nproj_conv = Convolution(inp, conv, 'exc')\nproj_conv.connect_filter(\n    [\n        [-1., 0., 1.],\n        [-1., 0., 1.],\n        [-1., 0., 1.]\n    ])\n\nproj_pool = Pooling(conv, pool, 'exc', operation='max') # max-pooling\nproj_pool.connect_pooling() # extent=(2, 2) is implicit"
  },
  {
    "objectID": "6-Additional.html#standard-models",
    "href": "6-Additional.html#standard-models",
    "title": "Additional features",
    "section": "",
    "text": "ANNarchy provides standard models from the PyNN interface:\nhttps://neuralensemble.org/PyNN/\n\n\n\nLeakyIntegrator\nIzhikevich\nIF_curr_exp\nIF_cond_exp\nIF_curr_alpha\nIF_cond_alpha\nHH_cond_exp\nEIF_cond_exp_isfa_ista\nEIF_cond_alpha_isfa_ista\n\n\n\n\n\nHebb\nOja\niBCM\nSTP\nSTDP"
  },
  {
    "objectID": "6-Additional.html#input-populations",
    "href": "6-Additional.html#input-populations",
    "title": "Additional features",
    "section": "",
    "text": "See https://annarchy.github.io/manual/Inputs.html\n\n\ninputs = np.array(\n    [\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n    ]\n)\n\ninp = TimedArray(rates=inputs, \n    schedule=[0., 10., 30., 60., 100., 150., 210., 280., 360., 450.])\n\ncompile()\n\nsimulate(500.)\n\n\n\ninp = Population(100, Neuron(equations=\"r = sin(t)\"))\n\npop = Population(100, Izhikevich)\n\nproj = CurrentInjection(inp, pop, 'exc')\nproj.connect_current()\n\n\n\nspike_times = [\n  [  10 + 20*i for i in range(100)]\n  [  11 + 20*i for i in range(100)]\n]\n\npop = SpikeSourceArray(spike_times=spike_times)\n\n\n\npop = PoissonPopulation(100, rates=30.)\n\npop = PoissonPopulation(100, rates=np.linspace(0.0, 100.0, 100))\n\npop = PoissonPopulation(\n    geometry=100,\n    parameters = \"\"\"\n        amp = 100.0\n        frequency = 50.0\n    \"\"\",\n    rates=\"amp * (1.0 + sin(2*pi*frequency*t/1000.0) )/2.0\"\n)\nor from a TimedArray.\n\n\n\npop_poisson = PoissonPopulation(200, rates=10.)\npop_corr    = HomogeneousCorrelatedSpikeTrains(200, rates=10., corr=0.3, tau=10.)"
  },
  {
    "objectID": "6-Additional.html#simulation-tools",
    "href": "6-Additional.html#simulation-tools",
    "title": "Additional features",
    "section": "",
    "text": "pop = Population( ... , stop_condition = \"r &gt; 1.0\")\n\ncompile()\n\nt = simulate_until(max_duration=1000.0, populations=pop)\n\n\n\npop1 = PoissonPopulation(100, rates=10.0)\npop2 = Population(100, Izhikevich)\nproj = Projection(pop1, pop2, 'exc')\nproj.connect_fixed_probability(weights=5.0, probability=0.2)\nm = Monitor(pop2, 'spike')\n\ncompile()\n\ndef simulation(idx, net):\n    net.get(pop1).rates = 10. * idx\n    net.simulate(1000.)\n    return net.get(m).raster_plot()\n\nresults = parallel_run(method=simulation, number=3)\n\nt1, n1 = results[0]\nt2, n2 = results[1]\nt3, n3 = results[2]"
  },
  {
    "objectID": "6-Additional.html#structural-plasticity",
    "href": "6-Additional.html#structural-plasticity",
    "title": "Additional features",
    "section": "",
    "text": "CreatingSynapse = Synapse(\n    parameters = \" ... \",\n    equations = \" ... \",\n    creating = \"pre.mean_r * post.mean_r &gt; 0.7 : proba = 0.5, w = 1.0\"\n)\n\nPruningSynapse = Synapse(\n    parameters = \" T = 100000 : int, projection \",\n    equations = \"\"\"\n        age = if pre.r * post.r &gt; 0.0 : \n                0\n              else :\n                age + 1 : init = 0, int\"\"\",\n    pruning = \"age &gt; T : proba = 0.5\"\n)"
  },
  {
    "objectID": "6-Additional.html#convolutions-and-pooling",
    "href": "6-Additional.html#convolutions-and-pooling",
    "title": "Additional features",
    "section": "",
    "text": "from ANNarchy import *\nfrom ANNarchy.extensions.convolution import *\n\ninp = Population(geometry=(100, 100), neuron=Neuron(parameters=\"r = 0.0\"))\nconv = Population(geometry=(100, 100), neuron=Neuron(equations=\"r = sum(exc)\"))\npool = Population(geometry=(50, 50), neuron=Neuron(equations=\"r = sum(exc)\"))\n\nproj_conv = Convolution(inp, conv, 'exc')\nproj_conv.connect_filter(\n    [\n        [-1., 0., 1.],\n        [-1., 0., 1.],\n        [-1., 0., 1.]\n    ])\n\nproj_pool = Pooling(conv, pool, 'exc', operation='max') # max-pooling\nproj_pool.connect_pooling() # extent=(2, 2) is implicit"
  },
  {
    "objectID": "3-Neurons.html",
    "href": "3-Neurons.html",
    "title": "Neuron models",
    "section": "",
    "text": "The two types of neuron models supported by ANNarchy are the rate-coded neuron and the (point) spiking neuron, based on the integrate-and-fire model. More complex models such as the Hodgkin-Huxley model are possible, but less practical.\n\n\nRate-coded neuron\n\n\n\n\n\n\n    \\tau \\, \\frac{d x(t)}{dt} + x(t) = \\sum_{i=1}^d w_{i, j} \\, r_i(t) + b\n \n    r(t) = f(v(t))\n\n\nLeaky integrate-and-fire (LIF) neuron.\n\n\n\n\n\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n \n    \\text{if} \\; v(t) &gt; V_T \\; \\text{emit a spike and reset.}\n\n\n\nBoth neuron models have an internal variable for each neuron (membrane potential) which follows an ordinary differential equation (ODE).\nThe main difference is how they transmit information: rate-coded neurons transmit continuously their instantaneous firing rate, while spiking neurons implement a binary spike transmission mechanism (membrane potential over a threshold) followed by a reset.\n\n\n\nLet’s use a simple noisy leaky integrator with the tanh transfer function:\n\n    \\tau \\, \\frac{d x(t)}{dt} + x(t) = I(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nIn ANNarchy, neural dynamics are described by the equation-oriented interface:\nfrom ANNarchy import *\n\nLeaky = Neuron(\n    parameters = \"\"\"\n        I = 0.0                     # Input (neuron-specific)\n        tau = 30.0   : population   # Time constant \n        noise = 0.01 : population   # Noise level\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = I + noise * Uniform(-1, 1)  : init=0.0\n \n        r = tanh(x)\n    \"\"\"\n)\n\n\nAll parameters used in the equations must be declared in the Neuron definition. Parameters can have one value per neuron in the population (default) or be common to all neurons (flag population or projection). Parameters and variables are double floats by default, but the type can be specified (int, bool).\n\n\n\nVariables are evaluated at each time step in the order of their declaration, except for coupled ODEs. The output variable of a rate-coded neuron must be named r.\nVariables can be updated with assignments (=, +=, etc) or by defining first order ODEs. The math C library symbols can be used (tanh, cos, exp, etc).\nInitial values at the start of the simulation can be specified with init (default: 0.0). Lower/higher bounds on the values of the variables can be set with the min/max flags:\nr = x : min=0.0 # ReLU\nAdditive noise can be drawn from several distributions, including Uniform, Normal, LogNormal, Exponential, Gamma…\n\n\n\nequations=\"\"\"\n    tau * dx/dt + x = I  : explicit\n    r = tanh(x)\n\"\"\"\nFirst-order ODEs are parsed and manipulated using sympy to obtain the derivative:\n# All equivalent:\ntau * dx/dt + x = I\ntau * dx/dt = I - x\ndx/dt = (I - x)/tau\nThe generated C++ code applies a numerical method (fixed step size dt) for all neural and synaptic variables. The step size dt is 1 ms by default, but this can changed in the setup() function:\nsetup(dt=0.1)\nSeveral numerical methods are available:\n\nexplicit: Explicit (forward) Euler (default).\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _x = (I[i] - x[i])/tau;\n    // tau * dx/dt + x = I\n    x[i] += dt*_x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nimplicit: Implicit (backward) Euler.\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _x = (I[i]*dt + tau*x[i])/(dt + tau);\n    // tau * dx/dt + x = I\n    x[i] = _x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nexponential: Exponential Euler (exact for linear ODE).\n\ndouble __stepsize_x = 1.0 - exp( -dt/tau);\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _x = __stepsize_x*(I[i] - x[i]);\n    // tau * dx/dt + x = I\n    x[i] = _x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nmidpoint: Midpoint.\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _k_x = dt*((I[i] - x[i])/tau);\n    double _x = (-(x[i] + 0.5*_k_x ) + I[i])/tau;\n    // tau * dx/dt + x = I\n    x[i] += dt*_x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nrk4: Runge-Kutta fourth-order (RK4).\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _k1_x = ((I[i] - x[i])/tau);\n    double _k2_x = ((-(x[i] + 0.5 * dt * _k1_x ) + I[i])/tau);\n    double _k3_x = ((-(x[i] + 0.5 * dt * _k2_x ) + I[i])/tau);\n    double _k4_x = ((-(x[i] + dt * _k3_x ) + I[i])/tau);\n    // tau * dx/dt + x = I\n    x[i] += dt/6.0 * ( _k1_x + (_k2_x+_k2_x) + (_k3_x+_k3_x) + _k4_x);\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nevent-driven: Event-driven (only for spiking synapses):\n\nfor(int i = 0; i &lt; size_pre; i++){\n    for(int j = 0; i &lt; size_post; i++){\n        // tau_plus  * dx/dt = -x\n        x[i][j] *= exp(dt*(_last_event[i][j] - (t-1))/(tau_plus));\n        // tau_minus * dy/dt = -y\n        y[i][j] *= exp(dt*(_last_event[i][j] - (t-1))/(tau_minus));\n    }\n}\nSee https://annarchy.github.io/manual/NumericalMethods for more explanations.\n\n\n\nPopulations are creating by specifying a number of neurons and a neuron type:\npop = Population(1000, Leaky)\nFor visualization purposes or when using convolutional layers, a tuple geometry can be passed instead of the size:\npop = Population((100, 100), Leaky)\nAll parameters and variables become attributes of the population (read and write) as numpy arrays:\npop.tau = np.linspace(20.0, 40.0, 1000)\npop.r = np.tanh(pop.v)\nSlices of populations are called PopulationView and can be addressed separately:\npop = Population(1000, Leaky)\nE = pop[:800]\nI = pop[800:]\n\n\n\nOnce all populations (and projections) are created, you have to generate to the C++ code and compile it with:\ncompile()\nThis creates a subfolder annarchy/ where the code is generated, compiled, linked and instantiated. You can then manipulate all parameters/variables from Python thanks to the Cython bindings.\nA simulation is simply run for a fixed duration in milliseconds with:\nsimulate(1000.) # 1 second\nYou can also run a simulation until a criteria is filled, check https://annarchy.github.io/manual/Simulation/#early-stopping\n\n\n\nBy default, a simulation is run in C++ without interaction with Python. You may want to record some variables (neural or synaptic) during the simulation with a Monitor:\nm = Monitor(pop, ['x', 'r'])\n\nn = Monitor(proj, ['w'])\nAfter the simulation, you can retrieve the recordings with:\nrecorded_v = m.get('v')\n\nrecorded_r = m.get('r')\n\nrecorded_w = n.get('w')\n\n\n\n\n\n\nWarning\n\n\n\n\nCalling get() flushes the underlying arrays.\nRecording projections at each time stepcan quickly fill up the RAM…\n\n\n\n\n\n\n\n\n\nNotebook: Rate-coded neuron\n\n\n\nDownload the Jupyter notebook: RC.ipynb or run it directly on colab.\n\n\n\n\n\n\nSpiking neurons are defined with the same interface as rate-coded ones, but they must also define two additional fields:\n\nspike: condition for emitting a spike (typically when the membrane potential exceeds a threshold).\nreset: what happens after a spike is emitted (at the start of the refractory period).\n\nA refractory period in ms can also be specified.\n\n\n\n\n\nExample of the Leaky Integrate-and-Fire:\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n\n\n    \\text{if} \\; v(t) &gt; V_T \\; \\text{emit a spike and reset.}\n\nLIF = Neuron(\n    parameters = \"\"\"\n        C = 200.\n        g_L = 10.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        I = 0.25\n    \"\"\",\n    equations = \"\"\"\n        C * dv/dt = g_L * (E_L - v) + I : init=E_L     \n    \"\"\",\n    spike = \"v &gt;= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\n\n\nNotebook: AdEx neuron - Adaptive exponential Integrate-and-fire\n\n\n\nDownload the Jupyter notebook: AdEx.ipynb or run it directly on colab."
  },
  {
    "objectID": "3-Neurons.html#rate-coded-vs.-spiking-neurons",
    "href": "3-Neurons.html#rate-coded-vs.-spiking-neurons",
    "title": "Neuron models",
    "section": "",
    "text": "The two types of neuron models supported by ANNarchy are the rate-coded neuron and the (point) spiking neuron, based on the integrate-and-fire model. More complex models such as the Hodgkin-Huxley model are possible, but less practical.\n\n\nRate-coded neuron\n\n\n\n\n\n\n    \\tau \\, \\frac{d x(t)}{dt} + x(t) = \\sum_{i=1}^d w_{i, j} \\, r_i(t) + b\n \n    r(t) = f(v(t))\n\n\nLeaky integrate-and-fire (LIF) neuron.\n\n\n\n\n\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n \n    \\text{if} \\; v(t) &gt; V_T \\; \\text{emit a spike and reset.}\n\n\n\nBoth neuron models have an internal variable for each neuron (membrane potential) which follows an ordinary differential equation (ODE).\nThe main difference is how they transmit information: rate-coded neurons transmit continuously their instantaneous firing rate, while spiking neurons implement a binary spike transmission mechanism (membrane potential over a threshold) followed by a reset."
  },
  {
    "objectID": "3-Neurons.html#rate-coded-neurons",
    "href": "3-Neurons.html#rate-coded-neurons",
    "title": "Neuron models",
    "section": "",
    "text": "Let’s use a simple noisy leaky integrator with the tanh transfer function:\n\n    \\tau \\, \\frac{d x(t)}{dt} + x(t) = I(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nIn ANNarchy, neural dynamics are described by the equation-oriented interface:\nfrom ANNarchy import *\n\nLeaky = Neuron(\n    parameters = \"\"\"\n        I = 0.0                     # Input (neuron-specific)\n        tau = 30.0   : population   # Time constant \n        noise = 0.01 : population   # Noise level\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = I + noise * Uniform(-1, 1)  : init=0.0\n \n        r = tanh(x)\n    \"\"\"\n)\n\n\nAll parameters used in the equations must be declared in the Neuron definition. Parameters can have one value per neuron in the population (default) or be common to all neurons (flag population or projection). Parameters and variables are double floats by default, but the type can be specified (int, bool).\n\n\n\nVariables are evaluated at each time step in the order of their declaration, except for coupled ODEs. The output variable of a rate-coded neuron must be named r.\nVariables can be updated with assignments (=, +=, etc) or by defining first order ODEs. The math C library symbols can be used (tanh, cos, exp, etc).\nInitial values at the start of the simulation can be specified with init (default: 0.0). Lower/higher bounds on the values of the variables can be set with the min/max flags:\nr = x : min=0.0 # ReLU\nAdditive noise can be drawn from several distributions, including Uniform, Normal, LogNormal, Exponential, Gamma…\n\n\n\nequations=\"\"\"\n    tau * dx/dt + x = I  : explicit\n    r = tanh(x)\n\"\"\"\nFirst-order ODEs are parsed and manipulated using sympy to obtain the derivative:\n# All equivalent:\ntau * dx/dt + x = I\ntau * dx/dt = I - x\ndx/dt = (I - x)/tau\nThe generated C++ code applies a numerical method (fixed step size dt) for all neural and synaptic variables. The step size dt is 1 ms by default, but this can changed in the setup() function:\nsetup(dt=0.1)\nSeveral numerical methods are available:\n\nexplicit: Explicit (forward) Euler (default).\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _x = (I[i] - x[i])/tau;\n    // tau * dx/dt + x = I\n    x[i] += dt*_x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nimplicit: Implicit (backward) Euler.\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _x = (I[i]*dt + tau*x[i])/(dt + tau);\n    // tau * dx/dt + x = I\n    x[i] = _x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nexponential: Exponential Euler (exact for linear ODE).\n\ndouble __stepsize_x = 1.0 - exp( -dt/tau);\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _x = __stepsize_x*(I[i] - x[i]);\n    // tau * dx/dt + x = I\n    x[i] = _x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nmidpoint: Midpoint.\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _k_x = dt*((I[i] - x[i])/tau);\n    double _x = (-(x[i] + 0.5*_k_x ) + I[i])/tau;\n    // tau * dx/dt + x = I\n    x[i] += dt*_x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nrk4: Runge-Kutta fourth-order (RK4).\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _k1_x = ((I[i] - x[i])/tau);\n    double _k2_x = ((-(x[i] + 0.5 * dt * _k1_x ) + I[i])/tau);\n    double _k3_x = ((-(x[i] + 0.5 * dt * _k2_x ) + I[i])/tau);\n    double _k4_x = ((-(x[i] + dt * _k3_x ) + I[i])/tau);\n    // tau * dx/dt + x = I\n    x[i] += dt/6.0 * ( _k1_x + (_k2_x+_k2_x) + (_k3_x+_k3_x) + _k4_x);\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nevent-driven: Event-driven (only for spiking synapses):\n\nfor(int i = 0; i &lt; size_pre; i++){\n    for(int j = 0; i &lt; size_post; i++){\n        // tau_plus  * dx/dt = -x\n        x[i][j] *= exp(dt*(_last_event[i][j] - (t-1))/(tau_plus));\n        // tau_minus * dy/dt = -y\n        y[i][j] *= exp(dt*(_last_event[i][j] - (t-1))/(tau_minus));\n    }\n}\nSee https://annarchy.github.io/manual/NumericalMethods for more explanations.\n\n\n\nPopulations are creating by specifying a number of neurons and a neuron type:\npop = Population(1000, Leaky)\nFor visualization purposes or when using convolutional layers, a tuple geometry can be passed instead of the size:\npop = Population((100, 100), Leaky)\nAll parameters and variables become attributes of the population (read and write) as numpy arrays:\npop.tau = np.linspace(20.0, 40.0, 1000)\npop.r = np.tanh(pop.v)\nSlices of populations are called PopulationView and can be addressed separately:\npop = Population(1000, Leaky)\nE = pop[:800]\nI = pop[800:]\n\n\n\nOnce all populations (and projections) are created, you have to generate to the C++ code and compile it with:\ncompile()\nThis creates a subfolder annarchy/ where the code is generated, compiled, linked and instantiated. You can then manipulate all parameters/variables from Python thanks to the Cython bindings.\nA simulation is simply run for a fixed duration in milliseconds with:\nsimulate(1000.) # 1 second\nYou can also run a simulation until a criteria is filled, check https://annarchy.github.io/manual/Simulation/#early-stopping\n\n\n\nBy default, a simulation is run in C++ without interaction with Python. You may want to record some variables (neural or synaptic) during the simulation with a Monitor:\nm = Monitor(pop, ['x', 'r'])\n\nn = Monitor(proj, ['w'])\nAfter the simulation, you can retrieve the recordings with:\nrecorded_v = m.get('v')\n\nrecorded_r = m.get('r')\n\nrecorded_w = n.get('w')\n\n\n\n\n\n\nWarning\n\n\n\n\nCalling get() flushes the underlying arrays.\nRecording projections at each time stepcan quickly fill up the RAM…\n\n\n\n\n\n\n\n\n\nNotebook: Rate-coded neuron\n\n\n\nDownload the Jupyter notebook: RC.ipynb or run it directly on colab."
  },
  {
    "objectID": "3-Neurons.html#spiking-neurons",
    "href": "3-Neurons.html#spiking-neurons",
    "title": "Neuron models",
    "section": "",
    "text": "Spiking neurons are defined with the same interface as rate-coded ones, but they must also define two additional fields:\n\nspike: condition for emitting a spike (typically when the membrane potential exceeds a threshold).\nreset: what happens after a spike is emitted (at the start of the refractory period).\n\nA refractory period in ms can also be specified.\n\n\n\n\n\nExample of the Leaky Integrate-and-Fire:\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n\n\n    \\text{if} \\; v(t) &gt; V_T \\; \\text{emit a spike and reset.}\n\nLIF = Neuron(\n    parameters = \"\"\"\n        C = 200.\n        g_L = 10.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        I = 0.25\n    \"\"\",\n    equations = \"\"\"\n        C * dv/dt = g_L * (E_L - v) + I : init=E_L     \n    \"\"\",\n    spike = \"v &gt;= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\n\n\nNotebook: AdEx neuron - Adaptive exponential Integrate-and-fire\n\n\n\nDownload the Jupyter notebook: AdEx.ipynb or run it directly on colab."
  },
  {
    "objectID": "notebooks/BoldParkinson.html",
    "href": "notebooks/BoldParkinson.html",
    "title": "BOLD monitoring using a basal ganglia model",
    "section": "",
    "text": "Download the Jupyter notebook: BoldParkinson.ipynb or run it directly on colab.\nPartial reproduction of:\nMaith, O., Escudero, F. V., Dinkelbach, H. Ü., Baladron, J., Horn, A., Irmen, F., et al. \nA computational model-based analysis of basal ganglia pathway changes in Parkinson’s disease inferred from resting-state fMRI.\nEuropean Journal of Neuroscience. doi:10.1111/ejn.14868\n#!pip install ANNarchy\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\n\n\nfrom ANNarchy import *\nfrom ANNarchy.extensions.bold import BoldMonitor, BoldModel\nfrom ANNarchy.extensions.bold.NormProjection import NormProjection\n\nclear()\n\nANNarchy 4.7 (4.7.2.5) on darwin (posix)."
  },
  {
    "objectID": "notebooks/BoldParkinson.html#data",
    "href": "notebooks/BoldParkinson.html#data",
    "title": "BOLD monitoring using a basal ganglia model",
    "section": "Data",
    "text": "Data\nBelow is the correlation matrix of the fMRI resting states of six areas :\n\nGPe\nGPI\nSTN\nCortex\nStriatum\nThalamus\n\nfor a given patient. We want to fit some unknown hyperparameters of a basal ganglia model to this particular data.\n\ncorr_mat_experiment = [\n    [ 1.         , 0.7404208  , 0.43226027 ,-0.1593407  , 0.71059084 , 0.5998113 ],\n    [ 0.7404208  , 1.         , 0.5438787  ,-0.12846166 , 0.5740793  , 0.58351475],\n    [ 0.43226027 , 0.5438787  , 1.         ,-0.23731637 , 0.3270468  , 0.4103379 ],\n    [-0.1593407  ,-0.12846166 ,-0.23731637 , 1.         ,-0.08617731 ,-0.23299724],\n    [ 0.71059084 , 0.5740793  , 0.3270468  ,-0.08617731 , 1.         , 0.6047172 ],\n    [ 0.5998113  , 0.58351475 , 0.4103379  ,-0.23299724 , 0.6047172  , 1.        ],\n]\n\nplt.figure()\nplt.imshow(corr_mat_experiment, vmin=-1, vmax=1, cmap=\"bwr\")\nplt.title(\"Data\")\nplt.xticks([0, 1, 2, 3, 4, 5], labels=[\"GPe\", \"GPi\", \"STN\", \"Cortex\", \"Striatum\", \"Thalamus\"], rotation='vertical')\nplt.yticks([0, 1, 2, 3, 4, 5], labels=[\"GPe\", \"GPi\", \"STN\", \"Cortex\", \"Striatum\", \"Thalamus\"])\nplt.colorbar()\nplt.show()"
  },
  {
    "objectID": "notebooks/BoldParkinson.html#neuro-computational-model",
    "href": "notebooks/BoldParkinson.html#neuro-computational-model",
    "title": "BOLD monitoring using a basal ganglia model",
    "section": "Neuro-computational model",
    "text": "Neuro-computational model\nLet’s first define the neuron models used in the network, as variations of the Izhikevich neuron:\n\nHybrid_neuron = Neuron(\n    parameters=\"\"\"\n    a = 0.0   \n    b = 0.0   \n    c = 0.0   \n    d = 0.0   \n    n0 = 140. \n    n1 = 5.0  \n    n2 = 0.04 \n    I = 0.0   \n    tau_ampa = 10 \n    tau_gaba = 10 \n    E_ampa = 0.0 \n    E_gaba = -90.0 \n    tau_syn = 1.0\n    SteadyStateNoise = 0.0\n    baseline = 0.0\n    tau_noise=200.\n\"\"\",\n    equations=\"\"\"\n    dg_ampa/dt = -g_ampa/tau_ampa : init = 0\n    dg_gaba/dt = -g_gaba/tau_gaba : init = 0\n    dslowNoise/dt = (-slowNoise+SteadyStateNoise)/tau_noise : init = 0\n    dv/dt = n2*v*v+n1*v+n0 - u  + I - g_ampa*(v-E_ampa) - g_gaba*(v-E_gaba) + slowNoise + baseline : init = -70.\n    du/dt = a*(b*(v)-u) : init = -18.55\n    tau_syn*dsyn/dt = -syn \n\"\"\",\n    spike=\"\"\"\n    v&gt;=30\n\"\"\",\n    reset=\"\"\"\n    v = c\n    u = u+d\n\"\"\",\n    refractory=10.0,\n)\n\nStriatum_neuron = Neuron(\n    parameters=\"\"\"\n    a = 0.05       \n    b = -20.0      \n    c = -55.0      \n    d = 377        \n    n0 = 61.65119  \n    n1 = 2.594639  \n    n2 = 0.022799  \n    I = 0.0        \n    tau_ampa = 10  \n    tau_gaba = 10  \n    E_ampa = 0.0   \n    E_gaba = -90.0 \n    Vr = -80.      \n    C  = 50.       \n    tau_syn = 1.0\n    SteadyStateNoise = 0.0\n    tau_noise=200.\n    baseline=0.0\n\"\"\",\n    equations=\"\"\"\n    dg_ampa/dt = -g_ampa/tau_ampa : init = 0\n    dg_gaba/dt = -g_gaba/tau_gaba : init = 0 \n    dslowNoise/dt = (-slowNoise+SteadyStateNoise)/tau_noise : init = 0\n    dv/dt = n2*v*v+n1*v+n0 - u/C  + I/C - g_ampa*(v-E_ampa) - g_gaba*(v-E_gaba)  + slowNoise + baseline : init = -70.\n    du/dt = a*(b*(v-Vr)-u) : init = -18.55\n    tau_syn*dsyn/dt = -syn \n\"\"\",\n    spike=\"\"\"\n    v&gt;=40\n\"\"\",\n    reset=\"\"\"\n    v = c\n    u = u+d\n\"\"\",\n    refractory=10.0,\n)\n\nHere are some fixed parameters for the simulation:\n\n# Some global definitions\ntimestep = 0.1\nseedVal = 10\nsetup(dt=timestep, suppress_warnings=True, seed=seedVal)\nrng = np.random.default_rng(seed=seedVal)\n\n# General parameters\npopulation_size = 200\nnumberOfNeuronsCortex = 600\n\n# synaptic activity noise (max-)amplitude\nnoiseSYAC = 0.05  # 0.15\n\n# how often changes the noise\nnoiseFrequency = 1000.0  # every x ms\n\n# times in ms\ninitial_simulate = 15000.0\nsimulationduration = 250000.0\n\n# baselines\nCortex_baseline = 50\nCortexInhib_baseline = 0\nSD1_baseline = 0\nSD2_baseline = 0\nGPi_baseline = 30\nGPe_baseline = 12\nSTN_baseline = 3\nThalamus_baseline = 3.5\n\n# noise in population\nCortex_noise_mean = 0\nCortex_noise_sd = 10\nSD1_noise_mean = 0\nSD1_noise_sd = 2\nSD2_noise_mean = 0\nSD2_noise_sd = 2\nGPi_noise_mean = 0\nGPi_noise_sd = 3\nGPe_noise_mean = 0\nGPe_noise_sd = 5\nSTN_noise_mean = 0\nSTN_noise_sd = 2\nThalamus_noise_mean = 0\nThalamus_noise_sd = 2\n\n# noise change over time\nCortex_noise_delta = 5\nSD1_noise_delta = 0\nSD2_noise_delta = 0\nGPi_noise_delta = 0\nGPe_noise_delta = 0\nSTN_noise_delta = 0\nThalamus_noise_delta = 0\n\nand here are the parameters that were found as a result of the hyperparameter optimization for this data sample:\n\nfitted_con_params = {\n    \"D1GPi_probs\": 0.17029162198305128,\n    \"D2GPe_probs\": 0.4074640914797783,\n    \"GPeSTN_probs\": 0.18478520214557648,\n    \"STNGPe_probs\": 0.4119542151689529,\n    \"STNGPi_probs\": 0.2458414003252983,\n    \"GPeGPi_probs\": 0.32317305132746693,\n    \"GPeGPe_probs\": 0.3238955348730087,\n    \"GPiGPi_probs\": 0.31747561842203137,\n    \"GPiThal_probs\": 0.25937364250421524,\n    \"ThalSD2_probs\": 0.1753483198583126,\n    \"ThalSD1_probs\": 0.12791343107819556,\n    \"SD1SD1_probs\": 0.44881010577082636,\n    \"SD2SD2_probs\": 0.20492168664932248,\n    \"CSD1_probs\": 0.19523878321051596,\n    \"CSD2_probs\": 0.10281952470540998,\n    \"CSTN_probs\": 0.16828824281692503,\n    \"V1Inh_probs\": 0.41263159513473513,\n    \"InhV1_probs\": 0.21300644502043722,\n    \"InhInh_probs\": 0.34024816900491717,\n    \"D1GPi_weights\": 0.009132171887904406,\n    \"D2GPe_weights\": 0.007663531694561243,\n    \"GPeSTN_weights\": 0.006760342903435231,\n    \"STNGPe_weights\": 0.010754598900675774,\n    \"STNGPi_weights\": 0.013566167578101158,\n    \"GPeGPi_weights\": 0.006238304376602173,\n    \"GPeGPe_weights\": 0.007731672264635564,\n    \"GPiGPi_weights\": 0.011985597293823957,\n    \"GPiThal_weights\": 0.006650131419301033,\n    \"ThalSD2_weights\": 0.006984027139842511,\n    \"ThalSD1_weights\": 0.012284493483603,\n    \"SD1SD1_weights\": 0.008113504592329264,\n    \"SD2SD2_weights\": 0.013962633926421403,\n    \"CSD1_weights\": 0.013000000000000001,\n    \"CSD2_weights\": 0.010670950431376695,\n    \"CSTN_weights\": 0.01283267229795456,\n    \"V1Inh_weights\": 0.00956317337229848,\n    \"InhV1_weights\": 0.012076521553099155,\n    \"InhInh_weights\": 0.009441683050245047\n}\n\nWe create the populations:\n\n# Cortex\npopV1 = Population(geometry=numberOfNeuronsCortex, neuron=Hybrid_neuron, name=\"V1\")\npopInhibit = Population(geometry=numberOfNeuronsCortex / 4, neuron=Hybrid_neuron)\n\npopV1.a = 0.02\npopV1.b = 0.2\npopV1.c = -72\npopV1.d = 6\npopV1.I = 0.0\npopV1.v = -72\npopV1.u = -14\npopV1.baseline = Cortex_baseline\n\npopInhibit.a = 0.02\npopInhibit.b = 0.2\npopInhibit.c = -72\npopInhibit.d = 6\npopInhibit.I = 0.0\npopInhibit.v = -72\npopInhibit.u = -14\npopInhibit.baseline = CortexInhib_baseline\n\n# other layers\nSD1 = Population(population_size, Striatum_neuron, name=\"SD1\")\nSD2 = Population(population_size, Striatum_neuron, name=\"SD2\")\nGPi = Population(population_size, Hybrid_neuron, name=\"GPi\")\nGPe = Population(population_size, Hybrid_neuron, name=\"GPe\")\nSTN = Population(population_size, Hybrid_neuron, name=\"STN\")\nThalamus = Population(population_size, Hybrid_neuron, name=\"Thalamus\")\n\n# GPi parameters\nGPi.a = 0.005\nGPi.b = 0.585\nGPi.c = -65.0\nGPi.d = 4.0\nGPi.I = 0.0  # 10\nGPi.baseline = GPi_baseline  # 2.5\n\n# GPe parameters\nGPe.a = 0.005\nGPe.b = 0.585\nGPe.c = -65\nGPe.d = 4\nGPe.I = 0.0\nGPe.baseline = GPe_baseline  # 1.2\n\n# STN parameters\nSTN.a = 0.005\nSTN.b = 0.265\nSTN.c = -65\nSTN.d = 2.0\nSTN.I = 0.0\nSTN.baseline = STN_baseline\n\n# Thalamus parameters\nThalamus.a = 0.02\nThalamus.b = 0.25\nThalamus.c = -65\nThalamus.d = 0.05\nThalamus.I = 0.0\nThalamus.baseline = Thalamus_baseline\n\nand the projections.\nNote: We use NormProjection instead of Projection for backwards compatibility…\n\nprojV1_Inhib = NormProjection(\n    pre=popV1,\n    post=popInhibit,\n    target=\"ampa\",\n    name=\"V1Inh\",\n    variable=\"syn\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"V1Inh_probs\"],\n    weights=fitted_con_params[\"V1Inh_weights\"],\n    force_multiple_weights=True,\n)\n\nprojInhib_V1 = NormProjection(\n    pre=popInhibit,\n    post=popV1,\n    target=\"gaba\",\n    variable=\"syn\",\n    name=\"InhV1\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"InhV1_probs\"],\n    weights=fitted_con_params[\"InhV1_weights\"],\n    force_multiple_weights=True,\n)\n\nprojInhib_Lat = NormProjection(\n    pre=popInhibit,\n    post=popInhibit,\n    target=\"gaba\",\n    variable=\"syn\",\n    name=\"InhInh\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"InhInh_probs\"],\n    weights=fitted_con_params[\"InhInh_weights\"],\n    force_multiple_weights=True,\n)\n\nD1GPi = NormProjection(\n    pre=SD1,\n    post=GPi,\n    target=\"gaba\",\n    variable=\"syn\",\n    name=\"D1GPi\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"D1GPi_probs\"],\n    weights=fitted_con_params[\"D1GPi_weights\"],\n    force_multiple_weights=True,\n)\n\nD2GPe = NormProjection(\n    pre=SD2,\n    post=GPe,\n    target=\"gaba\",\n    variable=\"syn\",\n    name=\"D2GPe\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"D2GPe_probs\"],\n    weights=fitted_con_params[\"D2GPe_weights\"],\n    force_multiple_weights=True,\n)\n\nGPeSTN = NormProjection(\n    pre=GPe,\n    post=STN,\n    target=\"gaba\",\n    variable=\"syn\",\n    name=\"GPeSTN\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"GPeSTN_probs\"],\n    weights=fitted_con_params[\"GPeSTN_weights\"],\n    force_multiple_weights=True,\n)\n\nSTNGPe = NormProjection(\n    pre=STN,\n    post=GPe,\n    target=\"ampa\",\n    variable=\"syn\",\n    name=\"STNGPe\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"STNGPe_probs\"],\n    weights=fitted_con_params[\"STNGPe_weights\"],\n    force_multiple_weights=True,\n)\n\nSTNGPi = NormProjection(\n    pre=STN,\n    post=GPi,\n    target=\"ampa\",\n    variable=\"syn\",\n    name=\"STNGPi\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"STNGPi_probs\"],\n    weights=fitted_con_params[\"STNGPi_weights\"],\n    force_multiple_weights=True,\n)\n\nGPeGPi = NormProjection(\n    pre=GPe,\n    post=GPi,\n    target=\"gaba\",\n    variable=\"syn\",\n    name=\"GPeGPi\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"GPeGPi_probs\"],\n    weights=fitted_con_params[\"GPeGPi_weights\"],\n    force_multiple_weights=True,\n)\n\nGPeGPe = NormProjection(\n    pre=GPe,\n    post=GPe,\n    target=\"gaba\",\n    variable=\"syn\",\n    name=\"GPeGPe\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"GPeGPe_probs\"],\n    weights=fitted_con_params[\"GPeGPe_weights\"],\n    force_multiple_weights=True,\n)\n\nGPiGPi = NormProjection(\n    pre=GPi,\n    post=GPi,\n    target=\"gaba\",\n    variable=\"syn\",\n    name=\"GPiGPi\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"GPiGPi_probs\"],\n    weights=fitted_con_params[\"GPiGPi_weights\"],\n    force_multiple_weights=True,\n)\n\n\nGPiThal = NormProjection(\n    pre=GPi,\n    post=Thalamus,\n    target=\"gaba\",\n    variable=\"syn\",\n    name=\"GPiThal\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"GPiThal_probs\"],\n    weights=fitted_con_params[\"GPiThal_weights\"],\n    force_multiple_weights=True,\n)\n\nThalSD2 = NormProjection(\n    pre=Thalamus,\n    post=SD2,\n    target=\"ampa\",\n    variable=\"syn\",\n    name=\"ThalSD2\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"ThalSD2_probs\"],\n    weights=fitted_con_params[\"ThalSD2_weights\"],\n    force_multiple_weights=True,\n)\n\nThalSD1 = NormProjection(\n    pre=Thalamus,\n    post=SD1,\n    target=\"ampa\",\n    variable=\"syn\",\n    name=\"ThalSD1\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"ThalSD1_probs\"],\n    weights=fitted_con_params[\"ThalSD1_weights\"],\n    force_multiple_weights=True,\n)\n\nSD1SD1 = NormProjection(\n    pre=SD1,\n    post=SD1,\n    target=\"gaba\",\n    variable=\"syn\",\n    name=\"SD1SD1\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"SD1SD1_probs\"],\n    weights=fitted_con_params[\"SD1SD1_weights\"],\n    force_multiple_weights=True,\n)\n\nSD2SD2 = NormProjection(\n    pre=SD2,\n    post=SD2,\n    target=\"gaba\",\n    variable=\"syn\",\n    name=\"SD2SD2\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"SD2SD2_probs\"],\n    weights=fitted_con_params[\"SD2SD2_weights\"],\n    force_multiple_weights=True,\n)\n\nCSD1 = NormProjection(\n    pre=popV1,\n    post=SD1,\n    target=\"ampa\",\n    variable=\"syn\",\n    name=\"CSD1\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"CSD1_probs\"],\n    weights=fitted_con_params[\"CSD1_weights\"],\n    force_multiple_weights=True,\n)\n\nCSD2 = NormProjection(\n    pre=popV1,\n    post=SD2,\n    target=\"ampa\",\n    variable=\"syn\",\n    name=\"CSD2\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"CSD2_probs\"],\n    weights=fitted_con_params[\"CSD2_weights\"],\n    force_multiple_weights=True,\n)\n\nCSTN = NormProjection(\n    pre=popV1,\n    post=STN,\n    target=\"ampa\",\n    variable=\"syn\",\n    name=\"CSTN\",\n).connect_fixed_probability(\n    probability=fitted_con_params[\"CSTN_probs\"],\n    weights=fitted_con_params[\"CSTN_weights\"],\n    force_multiple_weights=True,\n)"
  },
  {
    "objectID": "notebooks/BoldParkinson.html#bold-monitoring",
    "href": "notebooks/BoldParkinson.html#bold-monitoring",
    "title": "BOLD monitoring using a basal ganglia model",
    "section": "Bold monitoring",
    "text": "Bold monitoring\nWe used a custom Balloon model:\n\nballoon_maith2021 = BoldModel(\n    parameters=\"\"\"\n            second    = 1000.0\n            phi       = 1.0\n            kappa     = 0.665\n            gamma     = 0.412\n            E_0       = 0.3424\n            tau       = 1.0368\n            alpha     = 0.3215\n            V_0       = 0.02\n            noise     = 0\n        \"\"\",\n    equations=\"\"\"\n            I_CBF          = sum(I_CBF) + noise                                         : init=0\n            ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second      : init=0\n            df_in/dt       = s / second                                                 : init=1, min=0.01\n\n            E              = 1 - (1 - E_0)**(1 / f_in)                                  : init=0.3424\n            dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)            : init=1, min=0.01\n            dv/dt          = (f_in - f_out)/(tau*second)                                : init=1, min=0.01\n            f_out          = v**(1 / alpha)                                             : init=1, min=0.01\n\n            k_1            = 7 * E_0\n            k_2            = 2\n            k_3            = 2 * E_0 - 0.2\n\n            BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v)) : init=0\n        \"\"\",\n    inputs=\"I_CBF\",\n    output=\"BOLD\"\n)\n\nand record BOLD signals in all BG areas:\n\nm_bold_GPe = BoldMonitor(\n    GPe,\n    balloon_maith2021,\n    mapping={\"I_CBF\": \"syn\"},\n    start=True,\n)\nm_bold_GPi = BoldMonitor(\n    GPi,\n    balloon_maith2021,\n    mapping={\"I_CBF\": \"syn\"},\n    start=True,\n)\nm_bold_STN = BoldMonitor(\n    STN,\n    balloon_maith2021,\n    mapping={\"I_CBF\": \"syn\"},\n    start=True,\n)\nm_bold_Strboth = BoldMonitor(\n    [SD1, SD2],\n    balloon_maith2021,\n    mapping={\"I_CBF\": \"syn\"},\n    scale_factor=[1, 1],\n    start=True,\n)\nm_bold_Thal = BoldMonitor(\n    Thalamus,\n    balloon_maith2021,\n    mapping={\"I_CBF\": \"syn\"},\n    start=True,\n)\nm_bold_Cortex = BoldMonitor(\n    [popV1, popInhibit],\n    balloon_maith2021,\n    mapping={\"I_CBF\": \"syn\"},\n    scale_factor=[1, 1],\n    start=True,\n)"
  },
  {
    "objectID": "notebooks/BoldParkinson.html#simulation",
    "href": "notebooks/BoldParkinson.html#simulation",
    "title": "BOLD monitoring using a basal ganglia model",
    "section": "Simulation",
    "text": "Simulation\n\ncompile()\n\nCompiling ...  OK \n\n\n\ndef simulateNoisePeriods(duration):\n    try:\n        if duration % noiseFrequency != 0:\n            raise ValueError(\n                \"duration (\"\n                + str(duration)\n                + \") has to be a multiple of noiseFrequency\"\n            )\n    except ValueError as ve:\n        print(ve)\n        exit(1)\n\n    for NoisePeriod in range(int(duration / noiseFrequency)):\n        popV1.SteadyStateNoise = rng.normal(\n            Cortex_noise_mean + rng.uniform(-Cortex_noise_delta, Cortex_noise_delta),\n            Cortex_noise_sd,\n            popV1.geometry,\n        )\n        SD1.SteadyStateNoise = rng.normal(\n            SD1_noise_mean + rng.uniform(-SD1_noise_delta, SD1_noise_delta),\n            SD1_noise_sd,\n            SD1.geometry,\n        )\n        SD2.SteadyStateNoise = rng.normal(\n            SD2_noise_mean + rng.uniform(-SD2_noise_delta, SD2_noise_delta),\n            SD2_noise_sd,\n            SD2.geometry,\n        )\n        GPi.SteadyStateNoise = rng.normal(\n            GPi_noise_mean + rng.uniform(-GPi_noise_delta, GPi_noise_delta),\n            GPi_noise_sd,\n            GPi.geometry,\n        )\n        GPe.SteadyStateNoise = rng.normal(\n            GPe_noise_mean + rng.uniform(-GPe_noise_delta, GPe_noise_delta),\n            GPe_noise_sd,\n            GPe.geometry,\n        )\n        STN.SteadyStateNoise = rng.normal(\n            STN_noise_mean + rng.uniform(-STN_noise_delta, STN_noise_delta),\n            STN_noise_sd,\n            STN.geometry,\n        )\n        Thalamus.SteadyStateNoise = rng.normal(\n            Thalamus_noise_mean\n            + rng.uniform(-Thalamus_noise_delta, Thalamus_noise_delta),\n            Thalamus_noise_sd,\n            Thalamus.geometry,\n        )\n\n        m_bold_GPi.noise = rng.uniform(0, 1, 1) * noiseSYAC\n        m_bold_GPe.noise = rng.uniform(0, 1, 1) * noiseSYAC\n        m_bold_STN.noise = rng.uniform(0, 1, 1) * noiseSYAC\n        m_bold_Strboth.noise = rng.uniform(0, 1, 1) * noiseSYAC\n        m_bold_Thal.noise = rng.uniform(0, 1, 1) * noiseSYAC\n        m_bold_Cortex.noise = rng.uniform(0, 1, 1) * noiseSYAC\n        \n        simulate(noiseFrequency)\n\n\n# Bring the network into a stable state\nsimulateNoisePeriods(initial_simulate)\n\n# Simulate the resting state\nsimulateNoisePeriods(simulationduration)\n\n\nbCortex = m_bold_Cortex.get(\"BOLD\")\nbStrboth = m_bold_Strboth.get(\"BOLD\")\nbSTN = m_bold_STN.get(\"BOLD\")\nbGPi = m_bold_GPi.get(\"BOLD\")\nbGPe = m_bold_GPe.get(\"BOLD\")\nbThal = m_bold_Thal.get(\"BOLD\")\n\nbold_gpe_arr = bGPe[int(initial_simulate / timestep) :, 0]\nbold_gpi_arr = bGPi[int(initial_simulate / timestep) :, 0]\nbold_stn_arr = bSTN[int(initial_simulate / timestep) :, 0]\nbold_cor_arr = bCortex[int(initial_simulate / timestep) :, 0]\nbold_str_arr = bStrboth[int(initial_simulate / timestep) :, 0]\nbold_tha_arr = bThal[int(initial_simulate / timestep) :, 0]"
  },
  {
    "objectID": "notebooks/BoldParkinson.html#analysis",
    "href": "notebooks/BoldParkinson.html#analysis",
    "title": "BOLD monitoring using a basal ganglia model",
    "section": "Analysis",
    "text": "Analysis\nLet’s visualize the BOLD signals over this quite long simulation (250 s)\n\nplt.figure(figsize=(15, 6))\nplt.subplot(321)\nplt.plot(bold_gpe_arr)\nplt.title(\"GPe\")\nplt.subplot(322)\nplt.plot(bold_gpi_arr)\nplt.title(\"GPi\")\nplt.subplot(323)\nplt.plot(bold_stn_arr)\nplt.title(\"STN\")\nplt.subplot(324)\nplt.plot(bold_cor_arr)\nplt.title(\"Cortex\")\nplt.subplot(325)\nplt.plot(bold_str_arr)\nplt.title(\"Striatum\")\nplt.subplot(326)\nplt.plot(bold_tha_arr)\nplt.title(\"Thalamus\")\n\nplt.show()\n\n\n\n\n\n\n\n\nWe compute the correlation between each pair of these signals:\n\nbold_arr_len = len(bold_gpe_arr)\ntime_step_list = list(\n    range(int(bold_arr_len / 1000), bold_arr_len, int(bold_arr_len / 1000))\n)\ncor_list = []\nfor idx in time_step_list:\n    cor_list.append(\n        np.corrcoef(\n            [\n                bold_gpe_arr[:idx],\n                bold_gpi_arr[:idx],\n                bold_stn_arr[:idx],\n                bold_cor_arr[:idx],\n                bold_str_arr[:idx],\n                bold_tha_arr[:idx],\n            ]\n        )\n    )\n\nWe can now compare the correlation matrix of the simulation with the data:\n\ncorr_mat_simulated = cor_list[-1]\n\nplt.figure(figsize=(15, 8))\nplt.subplot(121)\nplt.imshow(corr_mat_simulated, vmin=-1, vmax=1, cmap=\"bwr\")\nplt.title(\"Simulation\")\nplt.xticks([0, 1, 2, 3, 4, 5], labels=[\"GPe\", \"GPi\", \"STN\", \"Cortex\", \"Striatum\", \"Thalamus\"], rotation='vertical')\nplt.yticks([0, 1, 2, 3, 4, 5], labels=[\"GPe\", \"GPi\", \"STN\", \"Cortex\", \"Striatum\", \"Thalamus\"])\n\nplt.subplot(122)\nplt.imshow(corr_mat_experiment, vmin=-1, vmax=1, cmap=\"bwr\")\nplt.title(\"Data\")\nplt.xticks([0, 1, 2, 3, 4, 5], labels=[\"GPe\", \"GPi\", \"STN\", \"Cortex\", \"Striatum\", \"Thalamus\"], rotation='vertical')\nplt.yticks([0, 1, 2, 3, 4, 5], labels=[\"GPe\", \"GPi\", \"STN\", \"Cortex\", \"Striatum\", \"Thalamus\"])\nplt.show()\n\n\n\n\n\n\n\n\nThe loss (difference between simulated and recorded correlation matrix) depends on the simulation length. The simulations must be long enough to support the hypotheses.\n\nloss_list = []\nfor cor in cor_list:\n    loss_list.append(np.linalg.norm(cor - corr_mat_experiment))\n    \nplt.plot(time_step_list, loss_list)\nplt.xlabel(\"Duration\")\nplt.ylabel(\"Loss\")\nplt.show()"
  },
  {
    "objectID": "notebooks/COBA.html",
    "href": "notebooks/COBA.html",
    "title": "COBA network",
    "section": "",
    "text": "Download the Jupyter notebook: COBA.ipynb or run it directly on colab.\n#!pip install ANNarchy\nThis script reproduces the benchmark used in:\nbased on the balanced network proposed by:\nThe network is composed of 4000 neurons (3200 excitatory and 800 inhibitory), reciprocally connected with a probability of 0.02 (sparse connectivity).\nThe COBA model uses conductance-based IF neurons:\n\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) \\, (E_\\text{exc} - v(t)) + g_\\text{inh} (t) \\, (E_\\text{inh} - v(t)) + I(t)\nThe discretization step has to be set to 0.1 ms:\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom ANNarchy import * \nclear()\nsetup(dt=0.1) \n\nANNarchy 4.7 (4.7.2.5) on darwin (posix)."
  },
  {
    "objectID": "notebooks/COBA.html#neuron-definition",
    "href": "notebooks/COBA.html#neuron-definition",
    "title": "COBA network",
    "section": "Neuron definition",
    "text": "Neuron definition\n\nCOBA = Neuron(\n    parameters=\"\"\"\n        El = -60.0          : population\n        Vr = -60.0          : population\n        Erev_exc = 0.0      : population\n        Erev_inh = -80.0    : population\n        Vt = -50.0          : population\n        tau = 20.0          : population\n        tau_exc = 5.0       : population\n        tau_inh = 10.0      : population\n        I = 20.0            : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I\n\n        tau_exc * dg_exc/dt = - g_exc\n        tau_inh * dg_inh/dt = - g_inh\n    \"\"\",\n    spike = \"v &gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\nThe neuron defines exponentially-decreasing conductance g_exc and g_inh for the excitatory and inhibitory conductances, respectively. It also defines a refractory period of 5 ms."
  },
  {
    "objectID": "notebooks/COBA.html#population",
    "href": "notebooks/COBA.html#population",
    "title": "COBA network",
    "section": "Population",
    "text": "Population\n\nP = Population(geometry=4000, neuron=COBA)\nPe = P[:3200]\nPi = P[3200:]\n\nWe create a population of 4000 COBA neurons, and assign the 3200 first ones to the excitatory population and the 800 last ones to the inhibitory population.\nIt would have been equivalent to declare two separate populations as:\nPe = Population(geometry=3200, neuron=COBA)\nPi = Population(geometry= 800, neuron=COBA)\nbut splitting a global population allows to apply methods to all neurons, for example when recording all spikes with a single monitor, or when initializing populations parameters uniformly:\n\nP.v = Normal(-55.0, 5.0)\nP.g_exc = Normal(4.0, 1.5)\nP.g_inh = Normal(20.0, 12.0)"
  },
  {
    "objectID": "notebooks/COBA.html#connections",
    "href": "notebooks/COBA.html#connections",
    "title": "COBA network",
    "section": "Connections",
    "text": "Connections\nThe neurons are randomly connected with a probability of 0.02 (sparse connectivity).\nExcitatory neurons project on all other neurons with the target “exc” and a weight of 0.6, while the inhibitory neurons have the target “inh” and a weight of 6.7.\n\nWe= 0.6\nWi = 6.7\np = 0.02\n\nCe = Projection(pre=Pe, post=P, target='exc')\nCe.connect_fixed_probability(weights=We, probability=p)\n\nCi = Projection(pre=Pi, post=P, target='inh')\nCi.connect_fixed_probability(weights=Wi, probability=p)\n\n&lt;ANNarchy.core.Projection.Projection at 0x12f4da640&gt;\n\n\n\ncompile()\n\nAfter compilation, the weights are created and can be accessed like regular variables. This allows to visualize the connectivity matrices.\n\nexc_weights = Ce.w\nprint(exc_weights)\n\n0.6\n\n\nYou will note that network stores a single weight value, as it would be a waste of RAM to store it multiple times. If you want unique values, make the weight values slightly random:\nCe.connect_fixed_probability(weights=Normal(0.6, 0.01), probability=0.02)\nNevertheless, you can visualize the connectivity matrix with a built-in method:\n\nconnectivitymatrix = Ce.connectivity_matrix()\n\nplt.figure()\nplt.imshow(connectivitymatrix[:100, :100])\nplt.show()"
  },
  {
    "objectID": "notebooks/COBA.html#simulation",
    "href": "notebooks/COBA.html#simulation",
    "title": "COBA network",
    "section": "Simulation",
    "text": "Simulation\nWe first define a monitor to record the spikes emitted in the whole population:\n\nm = Monitor(P, ['spike'])\n\nWe can then simulate for 500 millisecond:\n\nsimulate(500.)\n\nWe retrieve the recorded spikes from the monitor:\n\ndata = m.get('spike')\n\nand compute a raster plot from the data:\n\nt, n = m.raster_plot(data)\n\nt and n are lists representing for each spike emitted during the simulation the time at which it was emitted and the index the neuron which fired. The length of this list represents the total number of spikes in the population, so we can compute the population mean firing rate:\n\nprint('Mean firing rate in the population: ' + str(len(t) / 4000.) + 'Hz')\n\nMean firing rate in the population: 9.11175Hz\n\n\nFinally, we can show the raster plot with pylab:\n\nplt.figure()\nplt.plot(t, n, '.', markersize=1.0)\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\nplt.show()\n\n\n\n\n\n\n\n\nWe can also plot the mean firing rate in the population over time:\n\nrate = m.population_rate(data, smooth=1.0)\n\nplt.figure()\nplt.plot(rate)\nplt.xlabel('Time')\nplt.ylabel('Population firing rate')\nplt.show()\n\n\n\n\n\n\n\n\nExperiments:\n\nExperiment with different values of the excitatory and inhibitory weights and conclude on the difficult of finding E/I balance, especially when synaptic plasticity is present."
  },
  {
    "objectID": "notebooks/AdEx.html",
    "href": "notebooks/AdEx.html",
    "title": "Adaptive Exponential IF neuron",
    "section": "",
    "text": "Download the Jupyter notebook: AdEx.ipynb or run it directly on colab.\nThis notebook explores how the AdEx neuron model can reproduce various spiking patterns observed in vivo.\nCode based on Naud et al. (2008):\n\nNaud, R., Marcille, N., Clopath, C., and Gerstner, W. (2008). Firing patterns in the adaptive exponential integrate-and-fire model. Biol Cybern 99, 335. doi:10.1007/s00422-008-0264-7.\n\nUncomment on colab:\n\n#!pip install ANNarchy\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom ANNarchy import *\n\nclear()\nsetup(dt=0.1)\n\nANNarchy 4.7 (4.7.2.5) on darwin (posix).\n\n\nThe AdEx neuron is defined by the following equations:\n\n    C \\, \\frac{dv}{dt} = -g_L \\ (v - E_L) + g_L \\, \\Delta_T \\, \\exp(\\frac{v - v_T}{\\Delta_T}) + I - w\n\n\n    \\tau_w \\, \\frac{dw}{dt} = a \\, (v - E_L) - w\n\nif v &gt; v_\\text{spike}:\n\nv = v_R\nw = w + b\n\n\nAdEx = Neuron(\n    parameters=\"\"\"\n        C = 200.\n        gL = 10. # not g_L! g_ is reserved for spike transmission\n        E_L = -70.\n        v_T = -50.\n        delta_T = 2.0\n        a = 2.0\n        tau_w = 30.\n        b = 0.\n        v_r = -58.\n        I = 500.\n        v_spike = 0.0 \n    \"\"\",\n    equations=\"\"\"\n        C * dv/dt = - gL * (v - E_L) +  gL * delta_T * exp((v-v_T)/delta_T) + I - w : init=-70.0     \n        \n        tau_w * dw/dt = a * (v - E_L) - w  : init=0.0\n    \"\"\",\n    spike=\"\"\"\n        v &gt;= v_spike\n    \"\"\",\n    reset=\"\"\"\n        v = v_r\n        w += b\n    \"\"\",\n    refractory = 2.0\n)\n\nWe create a population of 1 AdEx neurons which will get different parameter values.\n\npop = Population(1, AdEx)\n\n\ncompile()\n\nCompiling ...  OK \n\n\nWe add a monitor to track the membrane potential and the spike timings during the simulation.\n\nm = Monitor(pop, ['v', 'spike'])\n\nFollowing (Naud et al, 2008), we provide different parameter values corresponding to eight different firing patterns:\n\ntonic spiking\nadaptation,\ninitial burst,\nregular bursting,\ndelayed accelerating,\ndelayed regular bursting,\ntranscient spiking,\nirregular spiking.\n\n\n# a) tonic spiking b) adaptation, c) initial burst, d) regular bursting, e) delayed accelerating, f) delayed regular bursting, g) transcient spiking, h) irregular spiking\nC =       [200, 200, 130, 200, 200, 200, 100, 100]\ngL =      [ 10,  12,  18,  10,  12,  12,  10,  12]\nE_L =     [-70, -70, -58, -58, -70, -70, -65, -60]\nv_T =     [-50, -50, -50, -50, -50, -50, -50, -50]\ndelta_T = [  2,   2,   2,   2,   2,   2,   2,   2]\na =       [  2,   2,   4,   2,-10., -6.,-10.,-11.]\ntau_w =   [ 30, 300, 150, 120, 300, 300,  90, 130]\nb =       [  0,  60, 120, 100,   0,   0,  30,  30]\nv_r =     [-58, -58, -50, -46, -58, -58, -47, -48]\nI =       [500, 500, 400, 210, 300, 110, 350, 160]\n\nIn the trial() method, we simulate the network for 500 ms with a fixed input current, and remove that current for an additional 50 ms.\n\ndef trial(C, gL, E_L, v_T, delta_T, a, tau_w, b, v_r, I):\n\n    # Reset neuron\n    reset()\n\n    # Set parameters\n    pop.C = C; pop.gL = gL; pop.E_L = E_L; pop.v_T = v_T; pop.delta_T = delta_T\n    pop.a = a; pop.tau_w = tau_w; pop.b = b; pop.v_r = v_r; pop.I = I \n\n    # Simulate\n    simulate(500.)\n    pop.I = 0.0\n    simulate(50.)\n\n    # Recordings\n    data = m.get('v')\n    spikes = m.get('spike')\n    for n, t in spikes.items(): # Normalize the spikes\n        data[[x  for x in t], n] = 0.0\n\n    return data\n\nWe can now visualize the membrane potential for the different neuron types:\n\nidx = 0\ndata = trial(C[idx], gL[idx], E_L[idx], v_T[idx], delta_T[idx], a[idx], tau_w[idx], b[idx], v_r[idx], I[idx])\n\nplt.figure()\nplt.plot(data[:, 0])\nplt.ylim((-70., 5.))\nplt.show()\n\n\n\n\n\n\n\n\nExperiments\n\nVisualize the firing patterns for the 8 neuron types and compare them to the figure in Naud et al. (2008).\nFor the tonic firing neuron, vary the refractory period to observe its influence on the firing pattern.\nWhat is the minimum value of I allowing the tonic firing neuron to spike?\n\n\n\n\n\nReferences\n\nNaud, R., Marcille, N., Clopath, C., and Gerstner, W. (2008). Firing patterns in the adaptive exponential integrate-and-fire model. Biological Cybernetics 99, 335. doi:10.1007/s00422-008-0264-7."
  },
  {
    "objectID": "notebooks/RateCoded.html",
    "href": "notebooks/RateCoded.html",
    "title": "Rate-coded neuron",
    "section": "",
    "text": "Download the Jupyter notebook: RC.ipynb or run it directly on colab.\nTo install ANNarchy on colab, uncomment this line and run it:\n\n#!pip install ANNarchy\n\nWe start by importing numpy and matplotlib, as well as ANNarchy:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom ANNarchy import *\n\nANNarchy 4.7 (4.7.2.5) on darwin (posix).\n\n\nWhen the network has been already compiled and you want to modify its architecture, you should call the clear() method of ANNarchy. In a script, this never happens, but Jupyter notebooks are not linear. In case of doubt, always run this cell or even restart the notebook.\n\nclear()\n\nLet’s now define a simple leaky integrator with added noise:\n\n    \\tau \\, \\frac{d x(t)}{dt} + x(t) = I(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nThe input I(t) is neuron-specific and set externally. Such a neuron can be implemented as:\n\nLeaky = Neuron(\n    parameters = \"\"\"\n        I = 0.0                     # Input (neuron-specific)\n        tau = 30.0   : population   # Time constant \n        noise = 0.1 : population   # Noise level\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = I + noise * Uniform(-1, 1)  : init=0.0\n \n        r = tanh(x)\n    \"\"\"\n)\n\nWe can know implement a population of 10 such neurons:\n\npop = Population(10, Leaky)\n\nWe record both variables ‘x’ and ‘r’:\n\nm = Monitor(pop, ['x', 'r'])\n\nThese neurons will not be connected with each other, so we can already compile the “network”:\n\ncompile()\n\nCompiling ...  OK \n\n\nThe neural variable I is initially 0. We now define a simple stimulation protocol where each neuron receives a random but fixed input for 200 ms in the middle of a trial.\n\nsimulate(200.)\n\npop.I = np.random.uniform(-1.0, 1.0, 10)\n\nsimulate(200.)\n\npop.I = 0.0\n\nsimulate(200.)\n\n\nrecordings = m.get()\n\nplt.figure()\nplt.plot(recordings['x'][:, 0], label='x')\nplt.plot(recordings['r'][:, 0], label='r')\nplt.legend()\nplt.xlabel('Time (ms)')\nplt.title('Membrane potential and firing rate')\n\nplt.figure()\nfor i in range(10):\n    plt.plot(recordings['r'][:, i])\nplt.xlabel('Time (ms)')\nplt.title('Firing rates in the population')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments:\n\nChange the transfer function of the neurons in their definition, for example to sigmoid / logistic:\n\nr = 1.0 / (1.0 + exp(-x))\nor ReLU:\nr = x : min = 0.0\nand observe the consequence on the firing rates, especially for negative membrane potentials.\nDo not forget to run clear() after each change to the neuron definition and to recompile!"
  },
  {
    "objectID": "notebooks/BoldModel.html",
    "href": "notebooks/BoldModel.html",
    "title": "BOLD models",
    "section": "",
    "text": "Download the Jupyter notebook: BoldModel.ipynb or run it directly on colab.\n\n#!pip install ANNarchy\n\nLet’s now demonstrate how to define a custom BOLD model. The default Ballon model is defined by the following code:\nballoon_RN = BoldModel(\n    parameters = \"\"\"\n        phi       = 1.0         ;   kappa     = 1/1.54\n        gamma     = 1/2.46      ;   E_0       = 0.34\n        tau       = 0.98        ;   alpha     = 0.33\n        V_0       = 0.02        ;   v_0       = 40.3\n        TE        = 40/1000.    ;   epsilon   = 1.43\n        r_0       = 25.         ;   second    = 1000.0\n    \"\"\",\n    equations = \"\"\"\n        # CBF input\n        I_CBF          = sum(I_CBF)       \n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  \n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        # Balloon model\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear BOLD equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))\n    \"\"\",\n    inputs=['I_CBF']\n)\nIt is very similar to the interface of a Neuron model, with parameters and equations defined in two multi-line strings. The input signal I_CBF has to be explicitly defined in the inputs argument to help the BOLD monitor create the mapping.\nTo demonstrate how to create a custom BOLD model, let’s suppose we want a model that computes both the BOLD signal of the Balloon model and the one of the Davis model:\n\nDavis, T. L., Kwong, K. K., Weisskoff, R. M., and Rosen, B. R. (1998). Calibrated functional MRI: mapping the dynamics of oxidative metabolism. Proceedings of the National Academy of Sciences 95, 1834–1839\n\nWithout going into too many details, the Davis model computes the BOLD signal directly using f_in and E, without introducing a differential equation for the BOLD signal. Its implementation using the BOLD model would be:\nDavisModel = BoldModel(\n    parameters = \"\"\"\n        second = 1000.0\n        \n        phi    = 1.0    # Friston et al. (2000)\n        kappa  = 1/1.54\n        gamma  = 1/2.46\n        E_0    = 0.34\n        \n        M      = 0.149   # Griffeth & Buxton (2011)\n        alpha  = 0.14\n        beta   = 0.91\n    \"\"\",\n    equations = \"\"\"\n        # CBF-driving input as in Friston et al. (2000)\n        I_CBF    = sum(I_CBF)                                             : init=0\n        ds/dt    = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  : init=0\n        df_in/dt = s  / second                                            : init=1, min=0.01\n    ​\n        # Using part of the Balloon model to calculate r (normalized CMRO2) as in Buxton et al. (2004)\n        E        = 1 - (1 - E_0)**(1 / f_in)                              : init=0.34\n        r        = f_in * E / E_0\n        \n        # Davis model\n        BOLD     = M * (1 - f_in**alpha * (r / f_in)**beta)               : init=0\n    \"\"\",\n    inputs=['I_CBF']\n)\nNote that we could simply define two BOLD monitors using different models, but let’s create a complex model that does both for the sake of demonstration.\nLet’s first redefine the population of the previous section:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom ANNarchy import *\nfrom ANNarchy.extensions.bold import *\nclear()\n\n# One populations of 100 izhikevich neurons\npop = Population(100, neuron=Izhikevich)\n\n# Set noise to create some baseline activity\npop.noise = 5.0\n\n# Compute mean firing rate in Hz on 100ms window\npop.compute_firing_rate(window=100.0)\n\n# Create required monitor\nm_rate = Monitor(pop, [\"r\"], start=False)\n\nANNarchy 4.7 (4.7.2.5) on darwin (posix).\n\n\nWe can now create a hybrid model computing both the Balloon RN model of Stephan et al. (2007) and the Davis model:\n\nballoon_Davis = BoldModel(\n    parameters = \"\"\"\n        phi       = 1.0         ;   kappa     = 1/1.54\n        gamma     = 1/2.46      ;   E_0       = 0.34\n        tau       = 0.98        ;   alpha     = 0.33\n        V_0       = 0.02        ;   v_0       = 40.3\n        TE        = 40/1000.    ;   epsilon   = 1.43\n        r_0       = 25.         ;   second    = 1000.0\n        M         = 0.062       ;   alpha2    = 0.14\n        beta      = 0.91\n    \"\"\",\n    equations = \"\"\"\n        # CBF input\n        I_CBF          = sum(I_CBF)       \n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  \n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        # Balloon model\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Ballon RN model\n        BOLD    = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))\n        \n        # Davis model\n        r = f_in * E / E_0                                                         : init=1, min=0.01\n        BOLD_Davis =  M * (1 - f_in**alpha2 * (r / f_in)**beta) \n    \"\"\",\n    inputs=['I_CBF']\n)\n\nWe now only need to pass that new object to the BOLD monitor, and specify that we want to record both BOLD and BOLD_Davis:\n\nm_bold = BoldMonitor(\n    \n    populations = [pop],  \n    \n    bold_model = balloon_Davis,\n    \n    mapping={'I_CBF': 'r'},            \n    \n    normalize_input=2000, \n    \n    recorded_variables=[\"I_CBF\", \"BOLD\", \"BOLD_Davis\"]\n)\n\ncompile()\n\nCompiling ...  OK \n\n\nWe run the same simulation protocol and compare the two BOLD signals. Note that the value of M has been modified to give a similar amplitude to both signals:\n\n# Ramp up time\nsimulate(2000)\n\n# Start recording\nm_rate.start()\nm_bold.start()\n\n# Simulation\nsimulate(5000)      # 5s with low noise\npop.noise = 7.5\nsimulate(5000)      # 5s with higher noise (one population)\npop.noise = 5\nsimulate(20000)     # 20s with low noise\n\n# retrieve the recordings\nmean_fr = np.mean(m_rate.get(\"r\"), axis=1)\ninput_data = m_bold.get(\"I_CBF\")\nbold_data = m_bold.get(\"BOLD\")\ndavis_data = m_bold.get(\"BOLD_Davis\")\n\n\n# Firing rate\nplt.figure()\nplt.plot(mean_fr, label=\"pop\")\nplt.title(\"Average mean firing rate [Hz]\")\nplt.xlabel(\"time [s]\")\n\n# BOLD input signal\nplt.figure()\nplt.plot(input_data)\nplt.title(\"BOLD input I_CBF\")\nplt.xlabel(\"time [s]\")\n\n# BOLD output signal\nplt.figure()\nplt.plot(bold_data*100.0, label=\"Balloon\")\nplt.plot(davis_data*100.0, label=\"Davis\")\nplt.legend()\nplt.title(\"BOLD [%]\")\nplt.xlabel(\"time [s]\")\n\nplt.show()"
  },
  {
    "objectID": "notebooks/BoldMonitoring.html",
    "href": "notebooks/BoldMonitoring.html",
    "title": "BOLD monitoring",
    "section": "",
    "text": "Download the Jupyter notebook: BoldMonitoring.ipynb or run it directly on colab.\n\n#!pip install ANNarchy\n\nThis notebook demonstrates the usage of the BOLD monitoring extension. It has to be explicitly imported:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom ANNarchy import *\nfrom ANNarchy.extensions.bold import *\n\nclear()\n\nANNarchy 4.7 (4.7.2.5) on darwin (posix).\n\n\nThis script shows a simple example for the application of one of the default models (the balloon_RN model) on a simple network.\nThere are two important variables in that model: BOLD which is the output of the model and I_CBF which is the input signal, deonting the mean firing rate of the input populations.\nAs the BOLD model reflects the change of activity levels in the recorded area, we will implement the following experiment. We record from a population of 100 Izhikevich neurons. After a short period of time we raise the activity level of the population for a fixed time window. For simplicity, we do not use connections between the neurons but simply increase the noise term in the Izhikevich model. This should lead to a noticeable BOLD signal.\n\nPopulation\nWe first create a population of Izhikevich neurons:\n\npop = Population(100, neuron=Izhikevich)\n\nAs we will not have any connections between the neurons, we increase the noise to create some baseline activity:\n\npop.noise = 5.0\n\nThe mean firing rate of the neurons will be used as an input to the BOLD model. This need to be activated explicitly as the computation of this value is quite expensive.\nIn our example, the mean firing rate should be computed across a time window of 100 ms. The resulting value will be stored in the r attribute of the population and can be easily recorded.\n\n# Compute mean firing rate in Hz on 100ms window\npop.compute_firing_rate(window=100.0)\n\n# Record the mean firing rate\nm_spike = Monitor(pop, ['spike'])\nm_rate = Monitor(pop, ['r'], start=False)\n\n\n\nBOLD Monitor definition\nThe BOLD monitor expects a list of populations which we want to record (in our case only pop). A BOLD model should be specified, here we take balloon_RN which is the default. We then specify the mapping from the population variable that should influence the BOLD signal, in our case the mean firing rate r, to the input variable of the BOLD model I_CBF.\nThe mean firing rate is normalized over a time window of 2000 ms and we record both input and output signals:\n\nm_bold = BoldMonitor(\n    \n    populations = [pop], # recorded populations\n    \n    bold_model = balloon_RN(), # BOLD model to use (default is balloon_RN)\n    \n    mapping = {'I_CBF': 'r'}, # mapping from pop.r to I_CBF\n    \n    normalize_input = 2000,  # time window to compute baseline.\n    \n    recorded_variables = [\"I_CBF\", \"BOLD\"]  # variables to be recorded\n)\n\nNow we can compile and initialize the network:\n\ncompile()\n\nCompiling ...  OK \n\n\n\n\nSimulation\nWe first simulate 2 seconds biological time to ensure that the network reaches a stable firing rate:\n\n# Ramp up time\nsimulate(2000)\n\ndata = m_spike.get('spike')\nt, n = m_spike.raster_plot(data)\nm_spike.pause() # stop recording spikes\n\n# Raster plot\nplt.figure()\nplt.plot(t, n, '.', markersize=1.0)\nplt.title(\"Raster plot\")\nplt.xlabel(\"time [s]\")\nplt.show()\n\n\n\n\n\n\n\n\nWe then enable the recording of all monitors:\n\nm_rate.start()\nm_bold.start()\n\nWe increase the noise for 5 seconds in a 30s simulation:\n\n# We manipulate the noise for the half of the neurons\nsimulate(5000)      # 5s with low noise\npop.noise = 7.5\nsimulate(5000)      # 5s with higher noise\npop.noise = 5.0\nsimulate(20000)     # 20s with low noise\n\n# Retrieve the recordings\nmean_fr = np.mean(m_rate.get(\"r\"), axis=1)\ninput_data = m_bold.get(\"I_CBF\")\nbold_data = m_bold.get(\"BOLD\")\n\n\n\nEvaluation\nWe can now plot:\n\nthe mean firing rate in the input population.\nthe recorded activity I which serves as an input to the BOLD model.\nthe resulting BOLD signal.\n\n\n# Firing rate\nplt.figure()\nplt.plot(mean_fr)\nplt.title(\"Average mean firing rate [Hz]\")\nplt.xlabel(\"time [s]\")\n\n# BOLD input signal\nplt.figure()\nplt.plot(input_data)\nplt.title(\"BOLD input I_CBF\")\nplt.xlabel(\"time [s]\")\n\n# BOLD output signal\nplt.figure()\nplt.plot(bold_data*100.0)\nplt.title(\"BOLD [%]\")\nplt.xlabel(\"time [s]\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments:\n\nPlay around with the amplitude and duration of the stimulation to see the impact on the BOLD signal (do not hesitate to simulate for longer)."
  },
  {
    "objectID": "notebooks/SynapticTransmission.html",
    "href": "notebooks/SynapticTransmission.html",
    "title": "Synaptic transmission",
    "section": "",
    "text": "Download the Jupyter notebook: SynapticTransmission.ipynb or run it directly on colab.\nThis notebook simply demonstrates the three main type of synaptic transmission for spiking neurons:\n\nInstantaneous\nExponentially-decreasing\nAlpha-shaped\n\n\n#!pip install ANNarchy\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom ANNarchy import *\nclear()\n\nANNarchy 4.7 (4.7.2.5) on darwin (posix).\n\n\nWe use here a simple LIF neuron receving three types of projections (a, b, c). The conductance g_a uses instantaneous transmission, as it is reset to 0 after each step. g_b decreases exponentially with time following a first order ODE. g_c is integrated twice in alpha_c, leading to the alpha shape.\nAll methods use the exponential numerical method, as they are first order linear ODEs and can be solved exactly.\n\nLIF = Neuron(\n    parameters=\"\"\"\n        tau = 20.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        tau_b = 10.0\n        tau_c = 10.0\n    \"\"\",\n    equations=\"\"\"\n        # Membrane potential\n        tau * dv/dt = (E_L - v) + g_a + g_b + alpha_c : init=-70.\n        \n        # Exponentially decreasing\n        tau_b * dg_b/dt = -g_b : exponential\n        \n        # Alpha-shaped\n        tau_c * dg_c/dt = -g_c : exponential\n        tau_c * dalpha_c/dt = exp((tau_c - dt/2.0)/tau_c) * g_c - alpha_c  : exponential\n    \"\"\",\n    spike=\"v &gt;= v_T\",\n    reset=\"v = v_r\",\n    refractory = 2.0\n)\n\nThe LIF neuron will receive a single spike at t = 10 ms, using the SpikeSourceArray specific population.\n\ninp = SpikeSourceArray([10.])\npop = Population(1, LIF)\n\nWe implement three different projections between the same neurons, to highlight the three possible transmission mechanisms.\n\nproj = Projection(inp, pop, 'a').connect_all_to_all(weights=1.0)\nproj = Projection(inp, pop, 'b').connect_all_to_all(weights=1.0)\nproj = Projection(inp, pop, 'c').connect_all_to_all(weights=1.0)\n\n\ncompile()\n\nCompiling ...  OK \n\n\nWe monitor the three conductances:\n\nm = Monitor(pop, ['g_a', 'g_b', 'alpha_c'])\n\n\ninp.clear()\nsimulate(100.)\n\n\ndata = m.get()\n\n\nplt.figure()\n\nplt.subplot(311)\nplt.plot(data['g_a'][:, 0])\nplt.ylabel(\"Instantaneous\")\n\nplt.subplot(312)\nplt.plot(data['g_b'][:, 0])\nplt.ylabel(\"Exponential\")\n\nplt.subplot(313)\nplt.plot(data['alpha_c'][:, 0])\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"Alpha\")\n\nplt.show()\n\n\n\n\n\n\n\n\nExperiments:\n\nModify the SpikeSourceArray object so that the neurons receives several spikes in a row (e.g. after 10, 20, 30, 40 ms, etc). What is the impact of the type of synaptic transmission on the maximum conductance?"
  },
  {
    "objectID": "notebooks/STDP.html",
    "href": "notebooks/STDP.html",
    "title": "STDP",
    "section": "",
    "text": "Download the Jupyter notebook: STDP.ipynb or run it directly on colab.\nThis notebook demonstrates the online implementation of the spike time-dependent plasticity rule.\n#!pip install ANNarchy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nANNarchy 4.7 (4.7.2.5) on darwin (posix)."
  },
  {
    "objectID": "notebooks/STDP.html#model",
    "href": "notebooks/STDP.html#model",
    "title": "STDP",
    "section": "Model",
    "text": "Model\nThe STDP learning rule maintains exponentially-decaying traces for the pre-synaptic and post-synaptic spikes.\n\\tau^+ \\, \\dfrac{d x(t)}{dt} + x (t) = 0\n\\tau^- \\, \\dfrac{d y(t)}{dt} + y(t) = 0\nLTP and LTD occur at spike times depending on the corresponding traces.\n\nWhen a pre-synaptic spike occurs, x(t) is incremented and LTD is applied proportionally to y(t).\nWhen a post-synaptic spike occurs, y(t) is incremented and LTP is applied proportionally to x(t).\n\n\nSTDP = Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection \n        tau_minus = 20.0 : projection\n        \n        A_plus = 0.01 : projection \n        A_minus = 0.01 : projection\n        \n        w_min = 0.0 : projection\n        w_max = 2.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        # pre-synaptic trace\n        tau_plus * dx/dt = -x #: event-driven \n    \n        # post-synaptic trace\n        tau_minus * dy/dt = -y #: event-driven \n    \"\"\",\n    pre_spike=\"\"\"       \n        g_target += w\n        \n        x += A_plus * w_max\n        \n        w = clip(w - y, w_min , w_max) # LTD\n    \"\"\",\n    post_spike=\"\"\"  \n        y += A_minus * w_max\n        \n        w = clip(w + x, w_min , w_max) # LTP\n    \"\"\"\n)\n\nWe create two dummy populations with one neuron each, whose spike times we can control.\n\npre = SpikeSourceArray([[50.]])\npost = SpikeSourceArray([[50.]])\n\nWe connect the population using a STDP synapse.\n\nproj = Projection(pre, post, 'exc', STDP)\nproj.connect_all_to_all(weights=1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x10ed2e220&gt;\n\n\n\ncompile()\n\nCompiling ...  OK"
  },
  {
    "objectID": "notebooks/STDP.html#stdp-mechanism",
    "href": "notebooks/STDP.html#stdp-mechanism",
    "title": "STDP",
    "section": "STDP mechanism",
    "text": "STDP mechanism\n\nm = Monitor(proj, ['x', 'y', 'w'])\n\nWARNING: Monitor(): it is a bad idea to record synaptic variables of a projection at each time step! \n\n\n\n# Reset the populations\nreset()\n\n# Set the spike times\npre.spike_times = [[0.0]]\npost.spike_times = [[50.0]]\n\n# Simulate\nm.resume()\nsimulate(100.)\ndata = m.get()\n\n# Plot\nplt.figure()\nplt.subplot(311)\nplt.plot(data['x'][:, 0, 0])\nplt.title(\"Pre-synaptic trace\")\nplt.subplot(312)\nplt.plot(data['y'][:, 0, 0])\nplt.title(\"Post-synaptic trace\")\nplt.subplot(313)\nplt.plot(data['w'][:, 0, 0])\nplt.title(\"Weight\")\nplt.xlabel(\"Time (ms)\")\nplt.show()\n\n\n\n\n\n\n\n\nExperiments:\n\nVary the pre-synaptic spike time and observe how it impacts x, y and the weight change."
  },
  {
    "objectID": "notebooks/STDP.html#stdp-figure",
    "href": "notebooks/STDP.html#stdp-figure",
    "title": "STDP",
    "section": "STDP figure",
    "text": "STDP figure\nLet’s now try to reproduce the (Bi and Poo, 2001) figure to check the validity of the online model.\nWe first reset our “network”:\n\nreset()\nm.pause() # Stop recording\n\nThe presynaptic neuron will fire at various times between 0 and 100 ms, while the postsynaptic neuron keeps firing at 50 ms.\n\npre_times = np.linspace(100.0, 0.0, 101)\nt_post = 50.0\n\n\nweight_changes = []\nfor t_pre in pre_times:\n    \n    # Reset the populations\n    pre.clear()\n    post.clear()\n    pre.spike_times = [[t_pre]]\n    post.spike_times = [[t_post]]\n    \n    # Reset the traces\n    proj.x = 0.0\n    proj.y = 0.0\n    \n    # Weight before the simulation (should be 1)\n    w_before = proj[0].w[0]\n    \n    # Simulate long enough\n    simulate(105.0)\n    \n    # Record weight change\n    delta_w = 100*(proj[0].w[0] - w_before)\n    weight_changes.append(delta_w)\n\nWe can now plot the classical STDP figure:\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nplt.plot(50. - pre_times, weight_changes, \"*\")\nplt.xlabel(\"t_post - t_pre (ms)\", loc='right')\nplt.title(\"weight change (%)\")\n\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nplt.show()"
  },
  {
    "objectID": "notebooks/STP.html",
    "href": "notebooks/STP.html",
    "title": "Short-term Plasticity",
    "section": "",
    "text": "Download the Jupyter notebook: STP.ipynb or run it directly on colab.\n\n#!pip install ANNarchy\n\nThe STP model is based on:\n\nTsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses, The Journal of Neuroscience, 20(50).\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom ANNarchy import *\nclear()\nsetup(dt=0.1)\n\nANNarchy 4.7 (4.7.2.5) on darwin (posix).\n\n\nWe will investigate a simple setup with one input neuron firing at a constant rate and one output neuron receiving spikes through a STP synapse.\nThe receiving neuron is a simple leaky integrate-and-fire:\n\nLIF = Neuron(\n    parameters = \"\"\"\n        tau = 10.0 : population\n    \"\"\",\n    equations = \"\"\"\n        tau * dv/dt = -v + g_exc : init=0.0\n    \"\"\",\n    spike = \"v &gt; 15.0\",\n    reset = \"v = 0.0\",\n    refractory = 3.0\n)\n\nShort-term plasticity is defined by dynamical changes of synaptic efficiency, based on pre- or post-synaptic activity.\nWe define a STP synapse, whose post-pynaptic potential (psp, define by g_target) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u.\nThe synaptic variables x and u generally follow linear ODEs:\n\\tau_\\text{rec} \\, \\dfrac{d x(t)}{dt} + x(t) = 1 \\tau_\\text{facil} \\, \\dfrac{d u(t)}{dt} + u(t) = 0.1\nWhen a pre-synaptic spike arrives at the synapse, the following updates are applied asynchronously:\nx(t) \\leftarrow (1 - u(t)) \\times x(t) u(t) \\leftarrow u(t) + 0.1 \\, (1 - u(t)) \nand the post-synaptic conductance is increased from:\ng(t) \\leftarrow g(t) + w \\, u(t) \\, x(t)\n\nSTP = Synapse(\n    parameters = \"\"\"\n        tau_rec = 10.0\n        tau_facil = 10.0\n        U = 0.1\n    \"\"\",\n    equations = \"\"\"\n        dx/dt = (1 - x)/tau_rec : init = 1.0, exponential\n        du/dt = (U - u)/tau_facil : init = 0.1, exponential\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w * u * x\n        \n        x *= (1 - u)\n        u += U * (1 - u)\n    \"\"\"\n)\n\nThe input population has one neuron that spikes regulary at 20 Hz (i.e. every 50 ms) for one second.\nThe projection uses the STP synapse.\n\n# Input\ninp = SpikeSourceArray(spike_times=[[50.*i for i in range(20)]])\n\n# Output\npop = Population(geometry=1, neuron=LIF)\n\n# Create projections\nproj = Projection(pre=inp, post=pop, target='exc', synapse=STP)\nproj.connect_all_to_all(weights=10.0) \n\n&lt;ANNarchy.core.Projection.Projection at 0x11f5106a0&gt;\n\n\nWe compile and simulate for 10 seconds:\n\ncompile()\n\nCompiling ...  OK \n\n\nWe can record the postsynaptic membrane potential and the synaptic variables u and v:\n\nm = Monitor(pop, ['v'])\nn = Monitor(proj, ['x', 'u'])\n\nWARNING: Monitor(): it is a bad idea to record synaptic variables of a projection at each time step! \n\n\nWe retrieve the recordings and plot them:\n\nsimulate(1100.0)\n\ndata_neuron = m.get()\ndata_synapse = n.get()\n\n\nplt.figure()\nplt.plot(data_neuron['v'])\nplt.xlabel('Time')\nplt.title('Post-synaptic membrane potential')\n\nplt.figure()\nplt.plot(data_synapse['x'][:, 0, 0], label='x')\nplt.plot(data_synapse['u'][:, 0, 0], label='u')\nplt.legend()\nplt.xlabel('Time')\nplt.title('Post-synaptic membrane potential')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments:\n\nWith the default values of \\tau_\\text{rec} and \\tau_\\text{facil}, there is no visible facilitating nor depressing effects in synaptic transmission. Increase their value (e.g. 100 ms or 1000 ms) and/or make them asymmetric to observe such effects.\nUse the ‘event-driven’ numerical for ‘x’ and ‘u’. What does it change to the recorded traces?"
  },
  {
    "objectID": "notebooks/BoldSearch.html",
    "href": "notebooks/BoldSearch.html",
    "title": "Fitting BOLD signals",
    "section": "",
    "text": "Download the Jupyter notebook: BoldSearch.ipynb or run it directly on colab.\n\n#!pip install ANNarchy\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom ANNarchy import *\nfrom ANNarchy.extensions.bold import *\nclear()\n\nANNarchy 4.7 (4.7.2.5) on darwin (posix).\n\n\nWe will now use the BOLD monitor to find the hyperparameters of a model that explains the best some recorded BOLD signals.\nThese hyperparameters can be anything: time constants, synaptic strengths, number of neurons, etc.\nWith a fixed set of values for the hyperparameters, one can create a network, run a simulation comparable to the experiment, monitor the BOLD signal and “match” it with the recorded BOLD data.\nUsing some optimization algorithm (grid/random search, genetic algorithms, Bayesian surrogate optimization), one can then find which set of hyperparameters matches the most with the data.\nIn this notebook, we use Bayesian surrogate optimization with the hyperopt library, but any other gradient-free method could be used (optuna is a good alternative to hyperopt):\nhttps://hyperopt.github.io\n\n#!pip install hyperopt\n\n\nimport hyperopt as ho\n\nIn order to keep the simulation short, we demonstrate here the method using a dummy problem, i.e. our 100 Izhikevich neurons from the last notebooks, and search for which noise level do the min and max values of the BOLD signal get a desired value.\nTo do so, we need to implement a method that runs a simulation for a given value of the parameters, records the BOLD signal and return how well it matches with the desired profile. We use here a simple Euclidian loss:\n\ndef trial(params):\n\n    clear()\n\n    # One populations of 100 izhikevich neurons\n    pop = Population(100, neuron=Izhikevich)\n\n    # Set noise to create some baseline activity\n    pop.noise = 5.0\n\n    # Compute mean firing rate in Hz on 100ms window\n    pop.compute_firing_rate(window=100.0)\n\n    m_bold = BoldMonitor(\n        \n        populations = pop, # recorded population\n        \n        bold_model = balloon_RN(), # BOLD model to use (default is balloon_RN)\n        \n        mapping = {'I_CBF': 'r'}, # mapping from pop.r to I_CBF\n        \n        normalize_input = 2000,  # time window to compute baseline.\n        \n        recorded_variables = [\"I_CBF\", \"BOLD\"]  # variables to be recorded in the BOLD model\n    )\n\n    compile()\n\n    # Warm-up\n    simulate(2000)\n    m_bold.start()\n\n    # We manipulate the noise for the half of the neurons\n    simulate(5000)      # 5s with low noise\n    pop.noise = params[0]\n    simulate(5000)      # 500ms with higher noise (one population)\n    pop.noise = 5.0\n    simulate(20000)     # 20s with low noise\n\n    # Retrieve the recordings\n    input_data = m_bold.get(\"I_CBF\")\n    bold_data = m_bold.get(\"BOLD\")\n\n    # Compute the loss\n    loss = (bold_data.max() - 0.05)**2 + (bold_data.min() + 0.02)**2\n\n    return {\n        'loss': loss,\n        'status': ho.STATUS_OK,\n        # -- store other results like this\n        'input_data': input_data,\n        'bold_data': bold_data,\n        }\n\nIf we run this method with a noise level of 7.5, we get the same results:\n\ndata = trial([7.5])\n\n# BOLD input signal\nplt.figure()\nplt.plot(data['input_data'])\nplt.title(\"BOLD input I_CBF\")\nplt.xlabel(\"time [s]\")\n\n# BOLD output signal\nplt.figure()\nplt.plot(data['bold_data']*100.0)\nplt.title(\"BOLD [%]\")\nplt.xlabel(\"time [s]\")\n\nplt.show()\n\nCompiling ...  OK \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s now run the hyperparameter optimization to minimize the loss function, defined as the Euclidian distance between the minimum and maximum values of the BOLD signal, and (5 %, 2 %)…\nloss = (bold_data.max() - 0.05)**2 + (bold_data.min() + 0.02)**2\nhyperopt.fmin will run 200 trials using different values of the noise between 5 and 15, computing the loss everytime and finding wich value minimizes the loss.\n\nbest = ho.fmin(\n    fn=trial,\n    space=[\n        ho.hp.uniform('noise', 5.0, 15.0), \n    ],\n    algo=ho.tpe.suggest,\n    max_evals=200\n)\n\nprint(best)\n\n  0%|          | 0/200 [00:00&lt;?, ?trial/s, best loss=?]100%|██████████| 200/200 [01:14&lt;00:00,  2.67trial/s, best loss: 3.165198988002009e-07]\n{'noise': 9.641243776203385}\n\n\nWe can now check that the obtained hyperparameters indeed do what we want.\n\ndata = trial(list(best.values()))\n\n# BOLD input signal\nplt.figure()\nplt.plot(data['input_data'])\nplt.title(\"BOLD input I_CBF\")\nplt.xlabel(\"time [s]\")\n\n# BOLD output signal\nplt.figure()\nplt.plot(data['bold_data']*100.0)\nplt.title(\"BOLD [%]\")\nplt.xlabel(\"time [s]\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments:\n\nDefine more complex criteria (min/max, temporal gap between min and max, fit to data, etc) and rerun the hyperoptimization.\nCreate a more complex network and run the optimization on more hyperparameters."
  },
  {
    "objectID": "2-ANNarchy.html",
    "href": "2-ANNarchy.html",
    "title": "ANNarchy",
    "section": "",
    "text": "White paper:\nVitay et al. (2015)\nANNarchy: a code generation approach to neural simulations on parallel hardware.\nFrontiers in Neuroinformatics 9. doi:10.3389/fninf.2015.00019\nSource code:\nhttps://github.com/ANNarchy/ANNarchy\nDocumentation:\nhttps://annarchy.github.io\nForum:\nhttps://groups.google.com/forum/#!forum/annarchy\n\n\n\nInstallation guide: https://annarchy.github.io/Installation/\nUsing pip:\npip install ANNarchy\nFrom source:\ngit clone https://github.com/ANNarchy/ANNarchy.git\ncd annarchy\npip install -e .\nRequirements (Linux and MacOS):\n\ng++/clang++\npython &gt;= 3.6\nnumpy\ncython\nsympy\n\nANNarchy also works out-of-the-box on colab.research.google.com, you just need to pip install it at the beginning of the notebook:\n!pip install ANNarchy\n\n\n\nA neuro-computational model in ANNarchy is composed of:\n\nSeveral populations implementing different neuron models.\nSeveral projections between the populations, that can implement specific synapse models.\nMonitors to record what is happening during a simulation.\n\n\n\n\n\n\nThe following script provides the basic structure of a model. First, the neuron and synapse models have to be defined using the equation-oriented interface. Populations are then created and connected with each other using projections. The network can then be generated and compiled, before the simulation can start.\nfrom ANNarchy import *\n\n# Create neuron types\nneuron = Neuron(...) \n\n# Create synapse types for transmission and/or plasticity\nstdp = Synapse(...) \n\n# Create populations of neurons\npop = Population(1000, neuron) \n\n# Connect the populations through projections\nproj = Projection(pop, pop, 'exc', stdp) \nproj.connect_fixed_probability(weights=Uniform(0.0, 1.0), probability=0.1)\n\n# Generate and compile the code\ncompile() \n\n# Record spiking activity\nm = Monitor(pop, ['spike']) \n\n# Simulate for 1 second\nsimulate(1000.)\nThe rest of this tutorial explains step by step how to implement those different mechanisms."
  },
  {
    "objectID": "2-ANNarchy.html#resources",
    "href": "2-ANNarchy.html#resources",
    "title": "ANNarchy",
    "section": "",
    "text": "White paper:\nVitay et al. (2015)\nANNarchy: a code generation approach to neural simulations on parallel hardware.\nFrontiers in Neuroinformatics 9. doi:10.3389/fninf.2015.00019\nSource code:\nhttps://github.com/ANNarchy/ANNarchy\nDocumentation:\nhttps://annarchy.github.io\nForum:\nhttps://groups.google.com/forum/#!forum/annarchy"
  },
  {
    "objectID": "2-ANNarchy.html#installation",
    "href": "2-ANNarchy.html#installation",
    "title": "ANNarchy",
    "section": "",
    "text": "Installation guide: https://annarchy.github.io/Installation/\nUsing pip:\npip install ANNarchy\nFrom source:\ngit clone https://github.com/ANNarchy/ANNarchy.git\ncd annarchy\npip install -e .\nRequirements (Linux and MacOS):\n\ng++/clang++\npython &gt;= 3.6\nnumpy\ncython\nsympy\n\nANNarchy also works out-of-the-box on colab.research.google.com, you just need to pip install it at the beginning of the notebook:\n!pip install ANNarchy"
  },
  {
    "objectID": "2-ANNarchy.html#structure-of-a-script",
    "href": "2-ANNarchy.html#structure-of-a-script",
    "title": "ANNarchy",
    "section": "",
    "text": "A neuro-computational model in ANNarchy is composed of:\n\nSeveral populations implementing different neuron models.\nSeveral projections between the populations, that can implement specific synapse models.\nMonitors to record what is happening during a simulation.\n\n\n\n\n\n\nThe following script provides the basic structure of a model. First, the neuron and synapse models have to be defined using the equation-oriented interface. Populations are then created and connected with each other using projections. The network can then be generated and compiled, before the simulation can start.\nfrom ANNarchy import *\n\n# Create neuron types\nneuron = Neuron(...) \n\n# Create synapse types for transmission and/or plasticity\nstdp = Synapse(...) \n\n# Create populations of neurons\npop = Population(1000, neuron) \n\n# Connect the populations through projections\nproj = Projection(pop, pop, 'exc', stdp) \nproj.connect_fixed_probability(weights=Uniform(0.0, 1.0), probability=0.1)\n\n# Generate and compile the code\ncompile() \n\n# Record spiking activity\nm = Monitor(pop, ['spike']) \n\n# Simulate for 1 second\nsimulate(1000.)\nThe rest of this tutorial explains step by step how to implement those different mechanisms."
  }
]