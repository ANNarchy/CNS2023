[
  {
    "objectID": "Spiking.html",
    "href": "Spiking.html",
    "title": "Spiking networks",
    "section": "",
    "text": "Spiking neurons are defined with the same interface as rate-coded ones, but they must also define two additional fields:\n\nspike: condition for emitting a spike (typically when the membrane potential exceeds a threshold).\nreset: what happens after a spike is emitted (at the start of the refractory period).\n\nA refractory period in ms can also be specified.\n\n\n\n\n\nExample of the Leaky Integrate-and-Fire:\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n\n\n    \\text{if} \\; v(t) &gt; V_T \\; \\text{emit a spike and reset.}\n\nLIF = Neuron(\n    parameters = \"\"\"\n        C = 200.\n        g_L = 10.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        I = 0.25\n    \"\"\",\n    equations = \"\"\"\n        C * dv/dt = g_L * (E_L - v) + I : init=E_L     \n    \"\"\",\n    spike = \"v &gt;= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\n\n\nNotebook: AdEx neuron - Adaptive exponential Integrate-and-fire\n\n\n\nDownload the Jupyter notebook: AdEx.ipynb\nRun it directly on colab: AdEx.ipynb"
  },
  {
    "objectID": "Spiking.html#spiking-neurons",
    "href": "Spiking.html#spiking-neurons",
    "title": "Spiking networks",
    "section": "",
    "text": "Spiking neurons are defined with the same interface as rate-coded ones, but they must also define two additional fields:\n\nspike: condition for emitting a spike (typically when the membrane potential exceeds a threshold).\nreset: what happens after a spike is emitted (at the start of the refractory period).\n\nA refractory period in ms can also be specified.\n\n\n\n\n\nExample of the Leaky Integrate-and-Fire:\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n\n\n    \\text{if} \\; v(t) &gt; V_T \\; \\text{emit a spike and reset.}\n\nLIF = Neuron(\n    parameters = \"\"\"\n        C = 200.\n        g_L = 10.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        I = 0.25\n    \"\"\",\n    equations = \"\"\"\n        C * dv/dt = g_L * (E_L - v) + I : init=E_L     \n    \"\"\",\n    spike = \"v &gt;= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\n\n\nNotebook: AdEx neuron - Adaptive exponential Integrate-and-fire\n\n\n\nDownload the Jupyter notebook: AdEx.ipynb\nRun it directly on colab: AdEx.ipynb"
  },
  {
    "objectID": "Spiking.html#conductances-currents",
    "href": "Spiking.html#conductances-currents",
    "title": "Spiking networks",
    "section": "Conductances / currents",
    "text": "Conductances / currents\nA pre-synaptic spike arriving to a spiking neuron increases the conductance/current g_target (e.g. g_exc or g_inh, depending on the projection).\nLIF = Neuron(\n    parameters = \"...\",\n    equations = \"\"\"\n        C*dv/dt = g_L*(E_L - v) + g_exc : init=E_L    \n    \"\"\",\n    spike = \"v &gt;= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\nEach spike increments instantaneously g_target from the synaptic efficiency w of the corresponding synapse.\ng_target += w\n\n\n\n\n\nFor exponentially-decreasing or alpha-shaped synapses, ODEs have to be introduced for the conductance/current.\nThe exponential numerical method should be preferred, as integration is exact.\nLIF = Neuron(\n    parameters = \"...\",\n    equations = \"\"\"\n        C*dv/dt = g_L*(E_L - v) + g_exc : init=E_L   \n\n        tau_exc * dg_exc/dt = - g_exc : exponential\n    \"\"\",\n    spike = \"v &gt;= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\n\n\nNotebook: Synaptic transmission\n\n\n\nDownload the Jupyter notebook: SynapticTransmission.ipynb\nRun it directly on colab: SynapticTransmission.ipynb\n\n\n\n\n\n\n\n\nNotebook: COBA - Conductance-based E/I network\n\n\n\nDownload the Jupyter notebook: COBA.ipynb\nRun it directly on colab: COBA.ipynb"
  },
  {
    "objectID": "Spiking.html#spike-timing-dependent-plasticity-stdp",
    "href": "Spiking.html#spike-timing-dependent-plasticity-stdp",
    "title": "Spiking networks",
    "section": "Spike-Timing Dependent plasticity (STDP)",
    "text": "Spike-Timing Dependent plasticity (STDP)\npost_spike similarly defines what happens when a post-synaptic spike is emitted. This can be used to implement the event-driven version of STDP using traces.\nSTDP = Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection ; tau_minus = 20.0 : projection\n        A_plus = 0.01 : projection   ; A_minus = 0.01 : projection\n        w_min = 0.0 : projection     ; w_max = 1.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau_plus  * dx/dt = -x : event-driven # pre-synaptic trace\n        tau_minus * dy/dt = -y : event-driven # post-synaptic trace\n    \"\"\",\n    pre_spike=\"\"\"\n        g_target += w\n        x += A_plus * w_max\n        w = clip(w + y, w_min , w_max)\n    \"\"\",\n    post_spike=\"\"\"\n        y -= A_minus * w_max\n        w = clip(w + x, w_min , w_max)\n    \"\"\")\n\n\n\n\n\n\nNotebook: STDP\n\n\n\nDownload the Jupyter notebook: STDP.ipynb\nRun it directly on colab: STDP.ipynb"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simulation of spiking and BOLD signals using the ANNarchy simulator",
    "section": "",
    "text": "ANNarchy (Artificial Neural Networks architect) is a neuro-simulator for rate-coded and spiking neural networks in Python.\nThe source code is available at https://github.com/ANNarchy/ANNarchy, while the full documentation is at: https://annarchy.github.io.\n\nProgram\n\n14:00 - 15:30 : Introduction to ANNarchy and spiking neural networks\n15:30 - 16:00 : Coffee break\n16:00 - 17:20 : BOLD monitoring and fitting"
  },
  {
    "objectID": "1-Rationale.html",
    "href": "1-Rationale.html",
    "title": "Why ANNarchy?",
    "section": "",
    "text": "There already exist several neuro-simulators for computational neuroscience, each focusing on different levels of description (multi-compartmental, spiking, rate-coded, etc.) or supported hardware (CPU, GPU).\nSome simulators provide fixed libraries of neural and synaptic models:\n\nNEURON https://neuron.yale.edu/neuron/\n\nMulti-compartmental models, spiking neurons (CPU)\n\nGENESIS http://genesis-sim.org/\n\nMulti-compartmental models, spiking neurons (CPU)\n\nNEST https://nest-initiative.org/\n\nSpiking neurons (CPU)\n\nGeNN https://genn-team.github.io/genn/\n\nSpiking neurons (GPU)\n\nAuryn https://fzenke.net/auryn/doku.php\n\nSpiking neurons (CPU)\n\n\nSome, such as ANNarchy, rely instead on code generation, allowing virtual any model to be implemented:\n\nBrian https://briansimulator.org/\n\nSpiking neurons (CPU)\n\nBrian2CUDA https://github.com/brian-team/brian2cuda\n\nSpiking neurons (GPU)\n\nANNarchy https://github.com/ANNarchy/ANNarchy\n\nRate-coded and spiking neurons (CPU, GPU)"
  },
  {
    "objectID": "1-Rationale.html#neuro-simulators",
    "href": "1-Rationale.html#neuro-simulators",
    "title": "Why ANNarchy?",
    "section": "",
    "text": "There already exist several neuro-simulators for computational neuroscience, each focusing on different levels of description (multi-compartmental, spiking, rate-coded, etc.) or supported hardware (CPU, GPU).\nSome simulators provide fixed libraries of neural and synaptic models:\n\nNEURON https://neuron.yale.edu/neuron/\n\nMulti-compartmental models, spiking neurons (CPU)\n\nGENESIS http://genesis-sim.org/\n\nMulti-compartmental models, spiking neurons (CPU)\n\nNEST https://nest-initiative.org/\n\nSpiking neurons (CPU)\n\nGeNN https://genn-team.github.io/genn/\n\nSpiking neurons (GPU)\n\nAuryn https://fzenke.net/auryn/doku.php\n\nSpiking neurons (CPU)\n\n\nSome, such as ANNarchy, rely instead on code generation, allowing virtual any model to be implemented:\n\nBrian https://briansimulator.org/\n\nSpiking neurons (CPU)\n\nBrian2CUDA https://github.com/brian-team/brian2cuda\n\nSpiking neurons (GPU)\n\nANNarchy https://github.com/ANNarchy/ANNarchy\n\nRate-coded and spiking neurons (CPU, GPU)"
  },
  {
    "objectID": "1-Rationale.html#key-features-of-annarchy",
    "href": "1-Rationale.html#key-features-of-annarchy",
    "title": "Why ANNarchy?",
    "section": "Key features of ANNarchy",
    "text": "Key features of ANNarchy\n\nSimulation of both rate-coded and spiking neural networks.\nOnly local biologically realistic mechanisms are possible (no backpropagation).\nEquation-oriented description of neural/synaptic dynamics (à la Brian).\nCode generation in C++, parallelized using OpenMP on CPU and CUDA on GPU (MPI is coming).\nSynaptic, intrinsic and structural plasticity mechanisms.\nBOLD monitoring from spiking neural networks.\n\nMaith et al. (2022)\nBOLD Monitoring in the Neural Simulator ANNarchy.\nFrontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966\nMaith et al. (2020)\nA computational model-based analysis of basal ganglia pathway changes in Parkinson’s disease inferred from resting-state fMRI.\nEuropean Journal of Neuroscience. doi:10.1111/ejn.14868"
  },
  {
    "objectID": "1-Rationale.html#parallel-performance",
    "href": "1-Rationale.html#parallel-performance",
    "title": "Why ANNarchy?",
    "section": "Parallel performance",
    "text": "Parallel performance\nMost importantly, ANNarchy is fast on multicore systems and GPUs. Depending on the model, it is comparable to Brian2 and faster than NEST on standard consumer PCs.\n\n\n\nCOBA Benchmark with 4000 neurons using an AMD Ryzen7 2700X / NVIDIA RTX 2060. See Dinkelbach et al. (2019).\n\n\n\n\n\nCOBA Benchmark with varying neurons using an AMD Ryzen7 2700X / NVIDIA RTX 2060. See Dinkelbach et al. (2019).\n\n\n\n\n\nCOBA Benchmark with varying neurons using an AMD Ryzen7 2700X / NVIDIA RTX 2060. See Dinkelbach et al. (2019)."
  },
  {
    "objectID": "RateCoded.html",
    "href": "RateCoded.html",
    "title": "Rate-coded networks",
    "section": "",
    "text": "In this section, we will take as an example an Echo-State Network (ESN), the rate-coded variant of reservoir computing introduced by Jaeger (2001). ESNs are recurrent neural networks with fixed recurrent weights. A linear readout is usually able to approximate any target signal based on the dynamic recurrent activity.\n\n\n\n\n\nESN rate-coded neurons follow first-order ODEs:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum w^\\text{in} \\, r^\\text{in}(t) + g \\, \\sum w^\\text{rec} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nIn ANNarchy, neural dynamics are described by the equation-oriented interface:\nfrom ANNarchy import *\n\nESN_Neuron = Neuron(\n    parameters = \"\"\"\n        tau = 30.0   : population   # Time constant\n        g = 1.0      : population   # Scaling\n        noise = 0.01 : population   # Noise level\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1)  : init=0.0\n \n        r = tanh(x)\n    \"\"\"\n)\n\n\nAll parameters used in the equations must be declared in the Neuron definition.\nparameters = \"\"\"\n    tau = 30.0   : population   # Time constant\n    g = 1.0      : population   # Scaling\n    noise = 0.01 : population   # Noise level\n\"\"\"\nParameters can have one value per neuron in the population (default) or be common to all neurons (flag population or projection).\nParameters and variables are double floats by default, but the type can be specified (int, bool).\n\n\n\nVariables are evaluated at each time step in the order of their declaration, except for coupled ODEs.\nequations=\"\"\"\n    tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1) : init=0.0\n\n    r = tanh(x)\n\"\"\"\nThe output variable of a rate-coded neuron must be named r.\nVariables can be updated with assignments (=, +=, etc) or by defining first order ODEs. The math C library symbols can be used (tanh, cos, exp, etc).\nInitial values at the start of the simulation can be specified with init (default: 0.0).\nLower/higher bounds on the values of the variables can be set with the min/max flags:\nr = x : min=0.0 # ReLU\nAdditive noise can be drawn from several distributions, including Uniform, Normal, LogNormal, Exponential, Gamma…\n\n\n\nFirst-order ODEs are parsed and manipulated using sympy:\n    # All equivalent:\n    tau * dx/dt + x = I\n    tau * dx/dt = I - x\n    dx/dt = (I - x)/tau\nThe generated C++ code applies a numerical method (fixed step size dt) for all neurons:\n#pragma omp simd\nfor(unsigned int i = 0; i &lt; size; i++){\n    double _x = (I[i] - x[i])/tau;\n    x[i] += dt*_x ;\n    r[i] = tanh(x[i]);\n}\nSeveral numerical methods are available:\n\nExplicit (forward) Euler (default):\n\ntau * dx/dt + x = I : explicit\n\nImplicit (backward) Euler:\n\ntau * dx/dt + x = I : implicit\n\nExponential Euler (exact for linear ODE):\n\ntau * dx/dt + x = I : exponential\n\nMidpoint (RK2):\n\ntau * dx/dt + x = I : midpoint\n\nRunge-Kutta (RK4):\n\ntau * dx/dt + x = I : rk4\n\nEvent-driven (spiking synapses):\n\ntau * dx/dt + x = I : event-driven\nSee https://annarchy.github.io/manual/NumericalMethods/ for more explanations.\n\n\n\nPopulations are creating by specifying a number of neurons and a neuron type:\npop = Population(1000, ESN_Neuron)\nFor visualization purposes or when using convolutional layers, a tuple geometry can be passed instead of the size:\npop = Population((100, 100), ESN_Neuron)\nAll parameters and variables become attributes of the population (read and write) as numpy arrays:\npop.tau = np.linspace(20.0, 40.0, 1000)\npop.r = np.tanh(pop.v)\nSlices of populations are called PopulationView and can be addressed separately:\npop = Population(1000, ESN_Neuron)\nE = pop[:800]\nI = pop[800:]\n\n\n\nProjections connect two populations (or views) in a uni-directional way.\nproj_exc = Projection(E, pop, 'exc')\nproj_inh = Projection(I, pop, 'inh')\nEach target ('exc', 'inh', 'AMPA', 'NMDA', 'GABA') can be defined as needed and will be treated differently by the post-synaptic neurons.\nThe weighted sum of inputs for a specific target is accessed in the equations by sum(target):\n    equations=\"\"\"\n        tau * dx/dt + x = sum(exc) - sum(inh)\n\n        r = tanh(x)\n    \"\"\"\nIt is therefore possible to model modulatory effects, divisive inhibition, etc.\n\n\n\nProjections must be populated with a connectivity matrix (who is connected to who), a weight w and optionally a delay d (uniform or variable).\nSeveral patterns are predefined:\nproj.connect_all_to_all(weights=Normal(0.0, 1.0), delays=2.0, allow_self_connections=False)\n\nproj.connect_one_to_one(weights=1.0, delays=Uniform(1.0, 10.0))\n\nproj.connect_fixed_number_pre(number=20, weights=1.0)\n\nproj.connect_fixed_number_post(number=20, weights=1.0)\n\nproj.connect_fixed_probability(probability=0.2, weights=1.0)\n\nproj.connect_gaussian(amp=1.0, sigma=0.2, limit=0.001)\n\nproj.connect_dog(amp_pos=1.0, sigma_pos=0.2, amp_neg=0.3, sigma_neg=0.7, limit=0.001)\nBut you can also load Numpy arrays or Scipy sparse matrices. Example for synfire chains:\nw = np.array([[None]*pre.size]*post.size)\n\nfor i in range(post.size):\n    w[i, (i-1)%pre.size] = 1.0\n\nproj.connect_from_matrix(w)\nSparse variant:\nw = lil_matrix((pre.size, post.size))\n\nfor i in range(pre.size):\n    w[pre.size, (i+1)%post.size] = 1.0\n\nproj.connect_from_sparse(w)\n\n\n\nOnce all populations and projections are created, you have to generate to the C++ code and compile it:\ncompile()\nYou can now manipulate all parameters/variables from Python thanks to the Cython bindings.\nA simulation is simply run for a fixed duration in milliseconds with:\nsimulate(1000.) # 1 second\nYou can also run a simulation until a criteria is filled, check https://annarchy.github.io/manual/Simulation/#early-stopping\n\n\n\nBy default, a simulation is run in C++ without interaction with Python. You may want to record some variables (neural or synaptic) during the simulation with a Monitor:\nm = Monitor(pop, ['v', 'r'])\n\nn = Monitor(proj, ['w'])\nAfter the simulation, you can retrieve the recordings with:\nrecorded_v = m.get('v')\n\nrecorded_r = m.get('r')\n\nrecorded_w = n.get('w')\n\n\n\n\n\n\nWarning\n\n\n\n\nCalling get() flushes the underlying arrays.\nRecording projections can quickly fill up the RAM…\n\n\n\n\n\n\n\n\n\nNotebook: Echo-State Network\n\n\n\nDownload the Jupyter notebook: RC.ipynb\nRun it directly on colab: RC.ipynb"
  },
  {
    "objectID": "RateCoded.html#rate-coded-networks",
    "href": "RateCoded.html#rate-coded-networks",
    "title": "Rate-coded networks",
    "section": "",
    "text": "In this section, we will take as an example an Echo-State Network (ESN), the rate-coded variant of reservoir computing introduced by Jaeger (2001). ESNs are recurrent neural networks with fixed recurrent weights. A linear readout is usually able to approximate any target signal based on the dynamic recurrent activity.\n\n\n\n\n\nESN rate-coded neurons follow first-order ODEs:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum w^\\text{in} \\, r^\\text{in}(t) + g \\, \\sum w^\\text{rec} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nIn ANNarchy, neural dynamics are described by the equation-oriented interface:\nfrom ANNarchy import *\n\nESN_Neuron = Neuron(\n    parameters = \"\"\"\n        tau = 30.0   : population   # Time constant\n        g = 1.0      : population   # Scaling\n        noise = 0.01 : population   # Noise level\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1)  : init=0.0\n \n        r = tanh(x)\n    \"\"\"\n)\n\n\nAll parameters used in the equations must be declared in the Neuron definition.\nparameters = \"\"\"\n    tau = 30.0   : population   # Time constant\n    g = 1.0      : population   # Scaling\n    noise = 0.01 : population   # Noise level\n\"\"\"\nParameters can have one value per neuron in the population (default) or be common to all neurons (flag population or projection).\nParameters and variables are double floats by default, but the type can be specified (int, bool).\n\n\n\nVariables are evaluated at each time step in the order of their declaration, except for coupled ODEs.\nequations=\"\"\"\n    tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1) : init=0.0\n\n    r = tanh(x)\n\"\"\"\nThe output variable of a rate-coded neuron must be named r.\nVariables can be updated with assignments (=, +=, etc) or by defining first order ODEs. The math C library symbols can be used (tanh, cos, exp, etc).\nInitial values at the start of the simulation can be specified with init (default: 0.0).\nLower/higher bounds on the values of the variables can be set with the min/max flags:\nr = x : min=0.0 # ReLU\nAdditive noise can be drawn from several distributions, including Uniform, Normal, LogNormal, Exponential, Gamma…\n\n\n\nFirst-order ODEs are parsed and manipulated using sympy:\n    # All equivalent:\n    tau * dx/dt + x = I\n    tau * dx/dt = I - x\n    dx/dt = (I - x)/tau\nThe generated C++ code applies a numerical method (fixed step size dt) for all neurons:\n#pragma omp simd\nfor(unsigned int i = 0; i &lt; size; i++){\n    double _x = (I[i] - x[i])/tau;\n    x[i] += dt*_x ;\n    r[i] = tanh(x[i]);\n}\nSeveral numerical methods are available:\n\nExplicit (forward) Euler (default):\n\ntau * dx/dt + x = I : explicit\n\nImplicit (backward) Euler:\n\ntau * dx/dt + x = I : implicit\n\nExponential Euler (exact for linear ODE):\n\ntau * dx/dt + x = I : exponential\n\nMidpoint (RK2):\n\ntau * dx/dt + x = I : midpoint\n\nRunge-Kutta (RK4):\n\ntau * dx/dt + x = I : rk4\n\nEvent-driven (spiking synapses):\n\ntau * dx/dt + x = I : event-driven\nSee https://annarchy.github.io/manual/NumericalMethods/ for more explanations.\n\n\n\nPopulations are creating by specifying a number of neurons and a neuron type:\npop = Population(1000, ESN_Neuron)\nFor visualization purposes or when using convolutional layers, a tuple geometry can be passed instead of the size:\npop = Population((100, 100), ESN_Neuron)\nAll parameters and variables become attributes of the population (read and write) as numpy arrays:\npop.tau = np.linspace(20.0, 40.0, 1000)\npop.r = np.tanh(pop.v)\nSlices of populations are called PopulationView and can be addressed separately:\npop = Population(1000, ESN_Neuron)\nE = pop[:800]\nI = pop[800:]\n\n\n\nProjections connect two populations (or views) in a uni-directional way.\nproj_exc = Projection(E, pop, 'exc')\nproj_inh = Projection(I, pop, 'inh')\nEach target ('exc', 'inh', 'AMPA', 'NMDA', 'GABA') can be defined as needed and will be treated differently by the post-synaptic neurons.\nThe weighted sum of inputs for a specific target is accessed in the equations by sum(target):\n    equations=\"\"\"\n        tau * dx/dt + x = sum(exc) - sum(inh)\n\n        r = tanh(x)\n    \"\"\"\nIt is therefore possible to model modulatory effects, divisive inhibition, etc.\n\n\n\nProjections must be populated with a connectivity matrix (who is connected to who), a weight w and optionally a delay d (uniform or variable).\nSeveral patterns are predefined:\nproj.connect_all_to_all(weights=Normal(0.0, 1.0), delays=2.0, allow_self_connections=False)\n\nproj.connect_one_to_one(weights=1.0, delays=Uniform(1.0, 10.0))\n\nproj.connect_fixed_number_pre(number=20, weights=1.0)\n\nproj.connect_fixed_number_post(number=20, weights=1.0)\n\nproj.connect_fixed_probability(probability=0.2, weights=1.0)\n\nproj.connect_gaussian(amp=1.0, sigma=0.2, limit=0.001)\n\nproj.connect_dog(amp_pos=1.0, sigma_pos=0.2, amp_neg=0.3, sigma_neg=0.7, limit=0.001)\nBut you can also load Numpy arrays or Scipy sparse matrices. Example for synfire chains:\nw = np.array([[None]*pre.size]*post.size)\n\nfor i in range(post.size):\n    w[i, (i-1)%pre.size] = 1.0\n\nproj.connect_from_matrix(w)\nSparse variant:\nw = lil_matrix((pre.size, post.size))\n\nfor i in range(pre.size):\n    w[pre.size, (i+1)%post.size] = 1.0\n\nproj.connect_from_sparse(w)\n\n\n\nOnce all populations and projections are created, you have to generate to the C++ code and compile it:\ncompile()\nYou can now manipulate all parameters/variables from Python thanks to the Cython bindings.\nA simulation is simply run for a fixed duration in milliseconds with:\nsimulate(1000.) # 1 second\nYou can also run a simulation until a criteria is filled, check https://annarchy.github.io/manual/Simulation/#early-stopping\n\n\n\nBy default, a simulation is run in C++ without interaction with Python. You may want to record some variables (neural or synaptic) during the simulation with a Monitor:\nm = Monitor(pop, ['v', 'r'])\n\nn = Monitor(proj, ['w'])\nAfter the simulation, you can retrieve the recordings with:\nrecorded_v = m.get('v')\n\nrecorded_r = m.get('r')\n\nrecorded_w = n.get('w')\n\n\n\n\n\n\nWarning\n\n\n\n\nCalling get() flushes the underlying arrays.\nRecording projections can quickly fill up the RAM…\n\n\n\n\n\n\n\n\n\nNotebook: Echo-State Network\n\n\n\nDownload the Jupyter notebook: RC.ipynb\nRun it directly on colab: RC.ipynb"
  },
  {
    "objectID": "RateCoded.html#plasticity-in-rate-coded-networks",
    "href": "RateCoded.html#plasticity-in-rate-coded-networks",
    "title": "Rate-coded networks",
    "section": "Plasticity in rate-coded networks",
    "text": "Plasticity in rate-coded networks\nSynapses can also implement plasticity rules that will be evaluated after each neural update.\nExample of the Intrator & Cooper BCM learning rule:\n\\Delta w = \\eta \\, r^\\text{pre} \\, r^\\text{post}  \\,  (r^\\text{post} - \\mathbb{E}[(r^\\text{post})^2])\nIBCM = Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 2000.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = post.r^2 : postsynaptic, exponential\n\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit\n    \"\"\",\n    psp = \"w * pre.r\"\n)\nEach synapse can access pre- and post-synaptic variables with pre. and post.. The postsynaptic flag allows to do computations only once per post-synaptic neurons.\npsp optionally defines what will be summed by the post-synaptic neuron (e.g. psp = \"w * log(pre.r)\").\nThe synapse type just has to be passed to the Projection:\nproj = Projection(inp, pop, 'exc', synapse=IBCM)\nSynaptic variables can be accessed as lists of lists for the whole projection:\nproj.w\nproj.theta\nor for a single post-synaptic neuron:\nproj[10].w\n\n\n\n\n\n\nNotebook: IBCM learning rule (Intrator and Cooper, 1992)\n\n\n\nDownload the Jupyter notebook: BCM.ipynb\nRun it directly on colab: BCM.ipynb\n\n\n\n\n\n\n\n\nNotebook: Reward-modulated RC network of Miconi (2017)\n\n\n\nDownload the Jupyter notebook: Miconi.ipynb\nRun it directly on colab: Miconi.ipynb"
  },
  {
    "objectID": "3-Neurons.html",
    "href": "3-Neurons.html",
    "title": "Neuron models",
    "section": "",
    "text": "The two types of neuron models supported by ANNarchy are the rate-coded neuron and the (point) spiking neuron, based on the integrate-and-fire model. More complex models such as the Hodgkin-Huxley model are possible, but less practical.\n\n\nRate-coded neuron\n\n\n\n\n\n\n    \\tau \\, \\frac{d x(t)}{dt} + x(t) = \\sum_{i=1}^d w_{i, j} \\, r_i(t) + b\n \n    r(t) = f(v(t))\n\n\nLeaky integrate-and-fire (LIF) neuron.\n\n\n\n\n\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n \n    \\text{if} \\; v(t) &gt; V_T \\; \\text{emit a spike and reset.}\n\n\n\nBoth neuron models have an internal variable for each neuron (membrane potential) which follows an ordinary differential equation (ODE).\nThe main difference is how they transmit information: rate-coded neurons transmit continuously their instantaneous firing rate, while spiking neurons implement a binary spike transmission mechanism (membrane potential over a threshold) followed by a reset.\n\n\n\nLet’s use a simple noisy leaky integrator with the tanh transfer function:\n\n    \\tau \\, \\frac{d x(t)}{dt} + x(t) = I(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nIn ANNarchy, neural dynamics are described by the equation-oriented interface:\nfrom ANNarchy import *\n\nLeaky = Neuron(\n    parameters = \"\"\"\n        I = 0.0                     # Input (neuron-specific)\n        tau = 30.0   : population   # Time constant \n        noise = 0.01 : population   # Noise level\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = I + noise * Uniform(-1, 1)  : init=0.0\n \n        r = tanh(x)\n    \"\"\"\n)\n\n\nAll parameters used in the equations must be declared in the Neuron definition. Parameters can have one value per neuron in the population (default) or be common to all neurons (flag population or projection). Parameters and variables are double floats by default, but the type can be specified (int, bool).\n\n\n\nVariables are evaluated at each time step in the order of their declaration, except for coupled ODEs. The output variable of a rate-coded neuron must be named r.\nVariables can be updated with assignments (=, +=, etc) or by defining first order ODEs. The math C library symbols can be used (tanh, cos, exp, etc).\nInitial values at the start of the simulation can be specified with init (default: 0.0). Lower/higher bounds on the values of the variables can be set with the min/max flags:\nr = x : min=0.0 # ReLU\nAdditive noise can be drawn from several distributions, including Uniform, Normal, LogNormal, Exponential, Gamma…\n\n\n\nequations=\"\"\"\n    tau * dx/dt + x = I  : explicit\n    r = tanh(x)\n\"\"\"\nFirst-order ODEs are parsed and manipulated using sympy to obtain the derivative:\n# All equivalent:\ntau * dx/dt + x = I\ntau * dx/dt = I - x\ndx/dt = (I - x)/tau\nThe generated C++ code applies a numerical method (fixed step size dt) for all neural and synaptic variables. The step size dt is 1 ms by default, but this can changed in the setup() function:\nsetup(dt=0.1)\nSeveral numerical methods are available:\n\nexplicit: Explicit (forward) Euler (default).\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _x = (I[i] - x[i])/tau;\n    // tau * dx/dt + x = I\n    x[i] += dt*_x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nimplicit: Implicit (backward) Euler.\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _x = (I[i]*dt + tau*x[i])/(dt + tau);\n    // tau * dx/dt + x = I\n    x[i] = _x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nexponential: Exponential Euler (exact for linear ODE).\n\ndouble __stepsize_x = 1.0 - exp( -dt/tau);\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _x = __stepsize_x*(I[i] - x[i]);\n    // tau * dx/dt + x = I\n    x[i] = _x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nmidpoint: Midpoint.\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _k_x = dt*((I[i] - x[i])/tau);\n    double _x = (-(x[i] + 0.5*_k_x ) + I[i])/tau;\n    // tau * dx/dt + x = I\n    x[i] += dt*_x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nrk4: Runge-Kutta fourth-order (RK4).\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _k1_x = ((I[i] - x[i])/tau);\n    double _k2_x = ((-(x[i] + 0.5 * dt * _k1_x ) + I[i])/tau);\n    double _k3_x = ((-(x[i] + 0.5 * dt * _k2_x ) + I[i])/tau);\n    double _k4_x = ((-(x[i] + dt * _k3_x ) + I[i])/tau);\n    // tau * dx/dt + x = I\n    x[i] += dt/6.0 * ( _k1_x + (_k2_x+_k2_x) + (_k3_x+_k3_x) + _k4_x);\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nevent-driven: Event-driven (only for spiking synapses):\n\nfor(int i = 0; i &lt; size_pre; i++){\n    for(int j = 0; i &lt; size_post; i++){\n        // tau_plus  * dx/dt = -x\n        x[i][j] *= exp(dt*(_last_event[i][j] - (t-1))/(tau_plus));\n        // tau_minus * dy/dt = -y\n        y[i][j] *= exp(dt*(_last_event[i][j] - (t-1))/(tau_minus));\n    }\n}\nSee https://annarchy.github.io/manual/NumericalMethods for more explanations.\n\n\n\nPopulations are creating by specifying a number of neurons and a neuron type:\npop = Population(1000, Leaky)\nFor visualization purposes or when using convolutional layers, a tuple geometry can be passed instead of the size:\npop = Population((100, 100), Leaky)\nAll parameters and variables become attributes of the population (read and write) as numpy arrays:\npop.tau = np.linspace(20.0, 40.0, 1000)\npop.r = np.tanh(pop.v)\nSlices of populations are called PopulationView and can be addressed separately:\npop = Population(1000, Leaky)\nE = pop[:800]\nI = pop[800:]\n\n\n\nOnce all populations (and projections) are created, you have to generate to the C++ code and compile it with:\ncompile()\nThis creates a subfolder annarchy/ where the code is generated, compiled, linked and instantiated. You can then manipulate all parameters/variables from Python thanks to the Cython bindings.\nA simulation is simply run for a fixed duration in milliseconds with:\nsimulate(1000.) # 1 second\nYou can also run a simulation until a criteria is filled, check https://annarchy.github.io/manual/Simulation/#early-stopping\n\n\n\nBy default, a simulation is run in C++ without interaction with Python. You may want to record some variables (neural or synaptic) during the simulation with a Monitor:\nm = Monitor(pop, ['x', 'r'])\n\nn = Monitor(proj, ['w'])\nAfter the simulation, you can retrieve the recordings with:\nrecorded_v = m.get('v')\n\nrecorded_r = m.get('r')\n\nrecorded_w = n.get('w')\n\n\n\n\n\n\nWarning\n\n\n\n\nCalling get() flushes the underlying arrays.\nRecording projections at each time stepcan quickly fill up the RAM…\n\n\n\n\n\n\n\n\n\nNotebook: Rate-coded neuron\n\n\n\nDownload the Jupyter notebook: RC.ipynb or run it directly on colab.\n\n\n\n\n\n\nSpiking neurons are defined with the same interface as rate-coded ones, but they must also define two additional fields:\n\nspike: condition for emitting a spike (typically when the membrane potential exceeds a threshold).\nreset: what happens after a spike is emitted (at the start of the refractory period).\n\nA refractory period in ms can also be specified.\n\n\n\n\n\nExample of the Leaky Integrate-and-Fire:\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n\n\n    \\text{if} \\; v(t) &gt; V_T \\; \\text{emit a spike and reset.}\n\nLIF = Neuron(\n    parameters = \"\"\"\n        C = 200.\n        g_L = 10.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        I = 0.25\n    \"\"\",\n    equations = \"\"\"\n        C * dv/dt = g_L * (E_L - v) + I : init=E_L     \n    \"\"\",\n    spike = \"v &gt;= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\n\n\nNotebook: AdEx neuron - Adaptive exponential Integrate-and-fire\n\n\n\nDownload the Jupyter notebook: AdEx.ipynb or run it directly on colab."
  },
  {
    "objectID": "3-Neurons.html#rate-coded-vs.-spiking-neurons",
    "href": "3-Neurons.html#rate-coded-vs.-spiking-neurons",
    "title": "Neuron models",
    "section": "",
    "text": "The two types of neuron models supported by ANNarchy are the rate-coded neuron and the (point) spiking neuron, based on the integrate-and-fire model. More complex models such as the Hodgkin-Huxley model are possible, but less practical.\n\n\nRate-coded neuron\n\n\n\n\n\n\n    \\tau \\, \\frac{d x(t)}{dt} + x(t) = \\sum_{i=1}^d w_{i, j} \\, r_i(t) + b\n \n    r(t) = f(v(t))\n\n\nLeaky integrate-and-fire (LIF) neuron.\n\n\n\n\n\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n \n    \\text{if} \\; v(t) &gt; V_T \\; \\text{emit a spike and reset.}\n\n\n\nBoth neuron models have an internal variable for each neuron (membrane potential) which follows an ordinary differential equation (ODE).\nThe main difference is how they transmit information: rate-coded neurons transmit continuously their instantaneous firing rate, while spiking neurons implement a binary spike transmission mechanism (membrane potential over a threshold) followed by a reset."
  },
  {
    "objectID": "3-Neurons.html#rate-coded-neurons",
    "href": "3-Neurons.html#rate-coded-neurons",
    "title": "Neuron models",
    "section": "",
    "text": "Let’s use a simple noisy leaky integrator with the tanh transfer function:\n\n    \\tau \\, \\frac{d x(t)}{dt} + x(t) = I(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nIn ANNarchy, neural dynamics are described by the equation-oriented interface:\nfrom ANNarchy import *\n\nLeaky = Neuron(\n    parameters = \"\"\"\n        I = 0.0                     # Input (neuron-specific)\n        tau = 30.0   : population   # Time constant \n        noise = 0.01 : population   # Noise level\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = I + noise * Uniform(-1, 1)  : init=0.0\n \n        r = tanh(x)\n    \"\"\"\n)\n\n\nAll parameters used in the equations must be declared in the Neuron definition. Parameters can have one value per neuron in the population (default) or be common to all neurons (flag population or projection). Parameters and variables are double floats by default, but the type can be specified (int, bool).\n\n\n\nVariables are evaluated at each time step in the order of their declaration, except for coupled ODEs. The output variable of a rate-coded neuron must be named r.\nVariables can be updated with assignments (=, +=, etc) or by defining first order ODEs. The math C library symbols can be used (tanh, cos, exp, etc).\nInitial values at the start of the simulation can be specified with init (default: 0.0). Lower/higher bounds on the values of the variables can be set with the min/max flags:\nr = x : min=0.0 # ReLU\nAdditive noise can be drawn from several distributions, including Uniform, Normal, LogNormal, Exponential, Gamma…\n\n\n\nequations=\"\"\"\n    tau * dx/dt + x = I  : explicit\n    r = tanh(x)\n\"\"\"\nFirst-order ODEs are parsed and manipulated using sympy to obtain the derivative:\n# All equivalent:\ntau * dx/dt + x = I\ntau * dx/dt = I - x\ndx/dt = (I - x)/tau\nThe generated C++ code applies a numerical method (fixed step size dt) for all neural and synaptic variables. The step size dt is 1 ms by default, but this can changed in the setup() function:\nsetup(dt=0.1)\nSeveral numerical methods are available:\n\nexplicit: Explicit (forward) Euler (default).\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _x = (I[i] - x[i])/tau;\n    // tau * dx/dt + x = I\n    x[i] += dt*_x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nimplicit: Implicit (backward) Euler.\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _x = (I[i]*dt + tau*x[i])/(dt + tau);\n    // tau * dx/dt + x = I\n    x[i] = _x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nexponential: Exponential Euler (exact for linear ODE).\n\ndouble __stepsize_x = 1.0 - exp( -dt/tau);\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _x = __stepsize_x*(I[i] - x[i]);\n    // tau * dx/dt + x = I\n    x[i] = _x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nmidpoint: Midpoint.\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _k_x = dt*((I[i] - x[i])/tau);\n    double _x = (-(x[i] + 0.5*_k_x ) + I[i])/tau;\n    // tau * dx/dt + x = I\n    x[i] += dt*_x ;\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nrk4: Runge-Kutta fourth-order (RK4).\n\n#pragma omp simd\nfor(int i = 0; i &lt; size; i++){\n    // tau * dx/dt + x = I\n    double _k1_x = ((I[i] - x[i])/tau);\n    double _k2_x = ((-(x[i] + 0.5 * dt * _k1_x ) + I[i])/tau);\n    double _k3_x = ((-(x[i] + 0.5 * dt * _k2_x ) + I[i])/tau);\n    double _k4_x = ((-(x[i] + dt * _k3_x ) + I[i])/tau);\n    // tau * dx/dt + x = I\n    x[i] += dt/6.0 * ( _k1_x + (_k2_x+_k2_x) + (_k3_x+_k3_x) + _k4_x);\n    // r = tanh(x)\n    r[i] = tanh(x[i]);\n}\n\nevent-driven: Event-driven (only for spiking synapses):\n\nfor(int i = 0; i &lt; size_pre; i++){\n    for(int j = 0; i &lt; size_post; i++){\n        // tau_plus  * dx/dt = -x\n        x[i][j] *= exp(dt*(_last_event[i][j] - (t-1))/(tau_plus));\n        // tau_minus * dy/dt = -y\n        y[i][j] *= exp(dt*(_last_event[i][j] - (t-1))/(tau_minus));\n    }\n}\nSee https://annarchy.github.io/manual/NumericalMethods for more explanations.\n\n\n\nPopulations are creating by specifying a number of neurons and a neuron type:\npop = Population(1000, Leaky)\nFor visualization purposes or when using convolutional layers, a tuple geometry can be passed instead of the size:\npop = Population((100, 100), Leaky)\nAll parameters and variables become attributes of the population (read and write) as numpy arrays:\npop.tau = np.linspace(20.0, 40.0, 1000)\npop.r = np.tanh(pop.v)\nSlices of populations are called PopulationView and can be addressed separately:\npop = Population(1000, Leaky)\nE = pop[:800]\nI = pop[800:]\n\n\n\nOnce all populations (and projections) are created, you have to generate to the C++ code and compile it with:\ncompile()\nThis creates a subfolder annarchy/ where the code is generated, compiled, linked and instantiated. You can then manipulate all parameters/variables from Python thanks to the Cython bindings.\nA simulation is simply run for a fixed duration in milliseconds with:\nsimulate(1000.) # 1 second\nYou can also run a simulation until a criteria is filled, check https://annarchy.github.io/manual/Simulation/#early-stopping\n\n\n\nBy default, a simulation is run in C++ without interaction with Python. You may want to record some variables (neural or synaptic) during the simulation with a Monitor:\nm = Monitor(pop, ['x', 'r'])\n\nn = Monitor(proj, ['w'])\nAfter the simulation, you can retrieve the recordings with:\nrecorded_v = m.get('v')\n\nrecorded_r = m.get('r')\n\nrecorded_w = n.get('w')\n\n\n\n\n\n\nWarning\n\n\n\n\nCalling get() flushes the underlying arrays.\nRecording projections at each time stepcan quickly fill up the RAM…\n\n\n\n\n\n\n\n\n\nNotebook: Rate-coded neuron\n\n\n\nDownload the Jupyter notebook: RC.ipynb or run it directly on colab."
  },
  {
    "objectID": "3-Neurons.html#spiking-neurons",
    "href": "3-Neurons.html#spiking-neurons",
    "title": "Neuron models",
    "section": "",
    "text": "Spiking neurons are defined with the same interface as rate-coded ones, but they must also define two additional fields:\n\nspike: condition for emitting a spike (typically when the membrane potential exceeds a threshold).\nreset: what happens after a spike is emitted (at the start of the refractory period).\n\nA refractory period in ms can also be specified.\n\n\n\n\n\nExample of the Leaky Integrate-and-Fire:\n\n    C \\, \\frac{d v(t)}{dt} = - g_L \\, (v(t) - V_L) + I(t)\n\n\n    \\text{if} \\; v(t) &gt; V_T \\; \\text{emit a spike and reset.}\n\nLIF = Neuron(\n    parameters = \"\"\"\n        C = 200.\n        g_L = 10.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        I = 0.25\n    \"\"\",\n    equations = \"\"\"\n        C * dv/dt = g_L * (E_L - v) + I : init=E_L     \n    \"\"\",\n    spike = \"v &gt;= v_T\",\n    reset = \"v = v_r\",\n    refractory = 2.0\n)\n\n\n\n\n\n\nNotebook: AdEx neuron - Adaptive exponential Integrate-and-fire\n\n\n\nDownload the Jupyter notebook: AdEx.ipynb or run it directly on colab."
  },
  {
    "objectID": "notebooks/RC.html",
    "href": "notebooks/RC.html",
    "title": "Echo state networks",
    "section": "",
    "text": "Download the Jupyter notebook: RC.ipynb\nRun it directly on colab: RC.ipynb\nIf you run this notebook in colab, first run this cell to install ANNarchy:\n\n!pip install ANNarchy\n\nLet’s start by importing ANNarchy.\nThe clear() command is necessary in notebooks when recreating a network. If you re-run the cells creating a network without calling clear() first, populations will add up, and the results may not be what you expect.\nsetup() sets various parameters, such as the step size dt in milliseconds. By default, dt is 1.0, so the call is not necessary here.\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nEach neuron in the reservoir follows the following equations:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + g \\,  \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nwhere \\xi(t) is some uniform noise.\n\nESN_Neuron = Neuron(\n    parameters = \"\"\"\n        tau = 30.0 : population\n        g = 1.0 : population\n        noise = 0.01\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = sum(in) + g * sum(exc) + noise * Uniform(-1, 1)\n\n        r = tanh(x)\n    \"\"\"\n)\n\nWe take one input neuron and a RC of 400 units.\n\n# Input population\ninp = Population(1, Neuron(parameters=\"r=0.0\"))\n\n# Recurrent population\nN = 400\npop = Population(N, ESN_Neuron)\n\n\npop.tau = 30.0\npop.g = 1.4\npop.noise = 0.01\n\nInput weights are classically uniformly distributed between -1 and 1.\nRecurrent weights are sampled from the normal distribution with mean 0 and variance g^2 / N. Here, we put the synaptic scaling g inside the neuron.\n\n# Input weights\nWi = Projection(inp, pop, 'in')\nWi.connect_all_to_all(weights=Uniform(-1.0, 1.0))\n\n# Recurrent weights\nWrec = Projection(pop, pop, 'exc')\nWrec.connect_all_to_all(weights=Normal(0., 1/np.sqrt(N)))\n\n&lt;ANNarchy.core.Projection.Projection at 0x1482198e0&gt;\n\n\n\ncompile()\n\n\nm = Monitor(pop, 'r')\n\nA single trial lasts 3s, with a step input between 100 and 200 ms.\n\ndef trial():\n    \"Runs two trials for a given spectral radius.\"\n\n    # Reset firing rates\n    inp.r = 0.0\n    pop.x = 0.0\n    pop.r = 0.0\n    \n    # Run the trial\n    simulate(100.)\n    inp[0].r = 1.0\n    simulate(100.0) # initial stimulation\n    inp[0].r = 0.0\n    simulate(2800.)\n    \n    data = m.get('r')\n    \n    return data\n\nWe run two trials successively to look at the chaoticity depending on g.\n\npop.g = 1.4\ndata1 = trial()\ndata2 = trial()\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(20, 8))\nplt.subplot(131)\nplt.title(\"First trial\")\nfor i in range(5):\n    plt.plot(data1[:, i], lw=2)\nplt.subplot(132)\nplt.title(\"Second trial\")\nfor i in range(5):\n    plt.plot(data2[:, i], lw=2)\nplt.subplot(133)\nplt.title(\"Difference\")\nfor i in range(5):\n    plt.plot(data1[:, i] - data2[:, i], lw=2)\n\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can now train the readout neurons to reproduce a step signal after 2 seconds.\nFor simplicity, we just train a L1-regularized linear regression (LASSO) on the reservoir activity.\n\ntarget = np.zeros(3000)\ntarget[2000:2500] = 1.0\n\n\nfrom sklearn import linear_model\nreg = linear_model.Lasso(alpha=0.001, max_iter=10000)\nreg.fit(data1, target)\npred = reg.predict(data2)\n\n\nplt.figure(figsize=(20, 10))\nplt.plot(pred, lw=3)\nplt.plot(target, lw=3)\n\nsns.despine()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/COBA.html",
    "href": "notebooks/COBA.html",
    "title": "COBA network",
    "section": "",
    "text": "Download the Jupyter notebook: COBA.ipynb\nRun it directly on colab: COBA.ipynb\n!pip install ANNarchy\nThis script reproduces the benchmark used in:\nbased on the balanced network proposed by:\nThe network is composed of 4000 neurons (3200 excitatory and 800 inhibitory), reciprocally connected with a probability of 0.02 (sparse connectivity).\nThe COBA model uses conductance-based IF neurons:\n\\tau \\cdot \\frac{dv (t)}{dt} = E_l - v(t) + g_\\text{exc} (t) \\, (E_\\text{exc} - v(t)) + g_\\text{inh} (t) \\, (E_\\text{inh} - v(t)) + I(t)\nThe discretization step has to be set to 0.1 ms:\nfrom ANNarchy import * \nsetup(dt=0.1) \n\nANNarchy 4.7 (4.7.1.5) on darwin (posix)."
  },
  {
    "objectID": "notebooks/COBA.html#neuron-definition",
    "href": "notebooks/COBA.html#neuron-definition",
    "title": "COBA network",
    "section": "Neuron definition",
    "text": "Neuron definition\n\nCOBA = Neuron(\n    parameters=\"\"\"\n        El = -60.0          : population\n        Vr = -60.0          : population\n        Erev_exc = 0.0      : population\n        Erev_inh = -80.0    : population\n        Vt = -50.0          : population\n        tau = 20.0          : population\n        tau_exc = 5.0       : population\n        tau_inh = 10.0      : population\n        I = 20.0            : population\n    \"\"\",\n    equations=\"\"\"\n        tau * dv/dt = (El - v) + g_exc * (Erev_exc - v) + g_inh * (Erev_inh - v ) + I\n\n        tau_exc * dg_exc/dt = - g_exc\n        tau_inh * dg_inh/dt = - g_inh\n    \"\"\",\n    spike = \"v &gt; Vt\",\n    reset = \"v = Vr\",\n    refractory = 5.0\n)\n\nThe neuron defines exponentially-decreasing conductance g_exc and g_inh for the excitatory and inhibitory conductances, respectively.\nIt also defines a refractory period of 5 ms."
  },
  {
    "objectID": "notebooks/COBA.html#population",
    "href": "notebooks/COBA.html#population",
    "title": "COBA network",
    "section": "Population",
    "text": "Population\n\nP = Population(geometry=4000, neuron=COBA)\nPe = P[:3200]\nPi = P[3200:]\n\nWe create a population of 4000 COBA neurons, and assign the 3200 first ones to the excitatory population and the 800 last ones to the inhibitory population.\nIt would have been equivalent to declare two separate populations as:\nPe = Population(geometry=3200, neuron=COBA)\nPi = Population(geometry= 800, neuron=COBA)\nbut splitting a global population allows to apply methods to all neurons, for example when recording all spikes with a single monitor, or when initializing populations parameters uniformly:\n\nP.v = Normal(-55.0, 5.0)\nP.g_exc = Normal(4.0, 1.5)\nP.g_inh = Normal(20.0, 12.0)"
  },
  {
    "objectID": "notebooks/COBA.html#connections",
    "href": "notebooks/COBA.html#connections",
    "title": "COBA network",
    "section": "Connections",
    "text": "Connections\nThe neurons are randomly connected with a probability of 0.02. Excitatory neurons project on all other neurons with the target “exc” and a weight of 0.6, while the inhibitory neurons have the target “inh” and a weight of 6.7.\n\nCe = Projection(pre=Pe, post=P, target='exc')\nCe.connect_fixed_probability(weights=0.6, probability=0.02)\n\nCi = Projection(pre=Pi, post=P, target='inh')\nCi.connect_fixed_probability(weights=6.7, probability=0.02)\n\n&lt;ANNarchy.core.Projection.Projection at 0x117af1e20&gt;\n\n\n\ncompile()"
  },
  {
    "objectID": "notebooks/COBA.html#simulation",
    "href": "notebooks/COBA.html#simulation",
    "title": "COBA network",
    "section": "Simulation",
    "text": "Simulation\nWe first define a monitor to record the spikes emitted in the whole population:\n\nm = Monitor(P, ['spike'])\n\nWe can then simulate for 100 millisecond:\n\nsimulate(100.)\n\nWe retrieve the recorded spikes from the monitor:\n\ndata = m.get('spike')\n\nand compute a raster plot from the data:\n\nt, n = m.raster_plot(data)\n\nt and n are lists representing for each spike emitted during the simulation the time at which it was emitted and the index the neuron which fired. The length of this list represents the total number of spikes in the population, so we can compute the population mean firing rate:\n\nprint('Mean firing rate in the population: ' + str(len(t) / 4000.) + 'Hz')\n\nMean firing rate in the population: 1.9125Hz\n\n\nFinally, we can show the raster plot with pylab:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(10, 8))\nplt.plot(t, n, '.')\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\n\n\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can also plot the mean firing rate in the population over time:\n\nrate = m.population_rate(data)\n\nplt.figure(figsize=(10, 6))\nplt.plot(np.linspace(0, 100, 1001), rate)\nplt.xlabel('Time (ms)')\nplt.ylabel('Population firing rate')\n\nsns.despine()\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/Miconi.html",
    "href": "notebooks/Miconi.html",
    "title": "Miconi network",
    "section": "",
    "text": "Download the Jupyter notebook: Miconi.ipynb\nRun it directly on colab: Miconi.ipynb\nReward-modulated recurrent network based on:\n\nMiconi T. (2017). Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks. eLife 6:e20899. doi:10.7554/eLife.20899\n\n\n!pip install ANNarchy\n\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nEach neuron in the reservoir follows the following equations:\n\n    \\tau \\frac{dx(t)}{dt} + x(t) = \\sum_\\text{input} W^\\text{IN} \\, r^\\text{IN}(t) + \\sum_\\text{rec} W^\\text{REC} \\, r(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nwhere \\xi(t) is a random perturbation at 3 Hz, with an amplitude randomly sampled between -A and +A.\nWe additionally keep track of the mean firing rate with a sliding average:\n\n    \\tilde{x}(t) = \\alpha \\, \\tilde{x}(t) + (1 - \\alpha) \\, x(t)\n\nThe three first neurons keep a constant rate throughout learning (1 or -1) to provide some bias to the other neurons.\n\nneuron = Neuron(\n    parameters = \"\"\"\n        tau = 30.0 : population # Time constant\n        constant = 0.0 # The four first neurons have constant rates\n        alpha = 0.05 : population # To compute the sliding mean\n        f = 3.0 : population # Frequency of the perturbation\n        A = 16. : population # Perturbation amplitude. dt*A/tau should be 0.5...\n    \"\"\",\n    equations=\"\"\"\n        # Perturbation\n        perturbation = if Uniform(0.0, 1.0) &lt; f/1000.: 1.0 else: 0.0 \n        noise = if perturbation &gt; 0.5: A*Uniform(-1.0, 1.0) else: 0.0\n\n        # ODE for x\n        x += dt*(sum(in) + sum(exc) - x + noise)/tau\n\n        # Output r\n        rprev = r\n        r = if constant == 0.0: tanh(x) else: tanh(constant)\n\n        # Sliding mean\n        delta_x = x - x_mean\n        x_mean = alpha * x_mean + (1 - alpha) * x\n    \"\"\"\n)\n\nThe learning rule is defined by a trace e_{i, j}(t) for each synapse i \\rightarrow j incremented at each time step with:\n\n    e_{i, j}(t) = e_{i, j}(t-1) + (r_i (t) \\, x_j(t))^3\n\nAt the end T of a trial, a normalized reward (R -R_\\text{mean}) is delivered and all weights are updated using:\n\n    \\Delta w_{i, j} = - \\eta \\,  e_{i, j}(T) \\, (R -R_\\text{mean})\n\nAll traces are then reset to 0 for the next trial. Weight changes are clamped between -0.0003 and 0.0003.\nAs ANNarchy applies the synaptic equations at each time step, we need to introduce a boolean learning_phase which performs trace integration when 0, weight update when 1.\n\nsynapse = Synapse(\n    parameters=\"\"\"\n        eta = 0.5 : projection # Learning rate\n        learning_phase = 0.0 : projection # Flag to allow learning only at the end of a trial\n        error = 0.0 : projection # Reward received\n        mean_error = 0.0 : projection # Mean Reward received\n        max_weight_change = 0.0003 : projection # Clip the weight changes\n    \"\"\",\n    equations=\"\"\"\n        # Trace\n        trace += if learning_phase &lt; 0.5:\n                    power(pre.rprev * (post.delta_x), 3)\n                 else:\n                    0.0\n\n        # Weight update only at the end of the trial\n        delta_w = if learning_phase &gt; 0.5:\n                eta * trace * (mean_error) * (error - mean_error)\n             else:\n                 0.0 : min=-max_weight_change, max=max_weight_change\n        w -= if learning_phase &gt; 0.5:\n                delta_w\n             else:\n                 0.0\n    \"\"\"\n)\n\nWe model the DNMS task of Miconi. The RC network has two inputs A and B. The reservoir has 200 neurons, 3 of which have constant rates.\n\n# Input population\ninp = Population(2, Neuron(parameters=\"r=0.0\"))\n\n# Recurrent population\nN = 200\npop = Population(N, neuron)\npop[0].constant = 1.0\npop[1].constant = 1.0\npop[2].constant = -1.0\npop.x = Uniform(-0.1, 0.1)\n\nInput weights are uniformly distributed between -1 and 1.\nRecurrent weights and normally distributed, with a coupling strength of g=1.5 (edge of chaos).\nConnections are all-to-all (fully connected).\n\n# Input weights\nWi = Projection(inp, pop, 'in')\nWi.connect_all_to_all(weights=Uniform(-1.0, 1.0))\n\n# Recurrent weights\ng = 1.5\nWrec = Projection(pop, pop, 'exc', synapse)\nWrec.connect_all_to_all(weights=Normal(0., g/np.sqrt(N)), allow_self_connections=True)\n\n&lt;ANNarchy.core.Projection.Projection at 0x11959cc70&gt;\n\n\n\ncompile()\n\nThe output of the reservoir is chosen to be the neuron of index 100.\n\noutput_neuron = 100\n\nWe record the rates inside the reservoir:\n\nm = Monitor(pop, ['r'])\n\nParameters defining the task:\n\n# Compute the mean reward per trial\nR_mean = np.zeros((2, 2))\nalpha = 0.75 # 0.33\n\n# Durations\nd_stim = 200\nd_delay= 200\nd_response = 400\nd_execution= 200\n\nDefinition of a DNMS trial (AA, AB, BA, BB):\n\ndef dnms_trial(trial, first, second, printing=False):\n    global R_mean\n    traces = []\n\n    # Reinitialize network\n    pop.x = Uniform(-0.1, 0.1).get_values(N)\n    pop.r = np.tanh(pop.x)\n    pop[0].r = np.tanh(1.0)\n    pop[1].r = np.tanh(1.0)\n    pop[2].r = np.tanh(-1.0)\n\n    # First input\n    inp[first].r = 1.0\n    simulate(d_stim)\n    \n    # Delay\n    inp.r = 0.0\n    simulate(d_delay)\n    \n    # Second input\n    inp[second].r = 1.0\n    simulate(d_stim)\n    \n    # Relaxation\n    inp.r = 0.0\n    simulate(d_response)\n    \n    # Read the output\n    rec = m.get()\n    \n    # Compute the target\n    target = 0.98 if first != second else -0.98\n    \n    # Response is over the last 200 ms\n    output = rec['r'][-int(d_execution):, output_neuron] # neuron 100 over the last 200 ms\n    \n    # Compute the error\n    error = np.mean(np.abs(target - output))\n    if printing:\n        print('Target:', target, '\\tOutput:', \"%0.3f\" % np.mean(output), '\\tError:',  \"%0.3f\" % error, '\\tMean:', \"%0.3f\" % R_mean[first, second])\n    \n    # The first 25 trial do not learn, to let R_mean get realistic values\n    if trial &gt; 25:\n\n        # Apply the learning rule\n        Wrec.learning_phase = 1.0\n        Wrec.error = error\n        Wrec.mean_error = R_mean[first, second]\n\n        # Learn for one step\n        step()\n        \n        # Reset the traces\n        Wrec.learning_phase = 0.0\n        Wrec.trace = 0.0\n        _ = m.get() # to flush the recording of the last step\n\n    # Update the mean reward\n    R_mean[first, second] = alpha * R_mean[first, second] + (1.- alpha) * error\n\n    return rec, traces\n\n\nfrom IPython.display import clear_output\nprinting = False\n\n# Many trials of each type\nmean_rewards = []\ntry:\n    for trial in range(1500):\n        if printing:\n            clear_output(wait=True)\n            print('Trial', trial)\n\n        # Perform the four different trials successively\n        recordsAA, tracesAA = dnms_trial (trial, 0, 0, printing)\n        recordsAB, tracesAB = dnms_trial (trial, 0, 1, printing)\n        recordsBA, tracesBA = dnms_trial (trial, 1, 0, printing)\n        recordsBB, tracesBB = dnms_trial (trial, 1, 1, printing)\n\n        # Record the initial trial\n        if trial == 0:\n            initialAA = recordsAA['r']\n            initialAB = recordsAB['r']\n            initialBA = recordsBA['r']\n            initialBB = recordsBB['r']\n\n        mean_rewards.append(R_mean.copy())\n\nexcept KeyboardInterrupt:\n    pass\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nmean_rewards = np.array(mean_rewards)\n\nplt.figure(figsize=(10, 6))\nplt.plot(mean_rewards.mean(axis=(1,2)), label='mean')\nplt.plot(mean_rewards[:, 0, 0], label='AA')\nplt.plot(mean_rewards[:, 0, 1], label='AB')\nplt.plot(mean_rewards[:, 1, 0], label='BA')\nplt.plot(mean_rewards[:, 1, 1], label='BB')\nplt.xlabel(\"Trials\")\nplt.ylabel(\"Mean error\")\nplt.legend()\n\nplt.figure(figsize=(10, 8))\n\nax = plt.subplot(221)\nax.plot(np.mean(initialAA[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsAA['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.legend()\nax.set_title('Output AA -1')\nax = plt.subplot(222)\nax.plot(np.mean(initialBA[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsBA['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.legend()\nax.set_title('Output BA +1')\nax = plt.subplot(223)\nax.plot(np.mean(initialAB[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsAB['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.set_title('Output AB +1')\nax = plt.subplot(224)\nax.plot(np.mean(initialBB[:, output_neuron:output_neuron+1], axis=1), label='before')\nax.plot(np.mean(recordsBB['r'][:, output_neuron:output_neuron+1], axis=1), label='after')\nax.set_ylim((-1., 1.))\nax.set_title('Output BB -1')\nplt.show()"
  },
  {
    "objectID": "notebooks/AdEx.html",
    "href": "notebooks/AdEx.html",
    "title": "Adaptive Exponential IF neuron",
    "section": "",
    "text": "Download the Jupyter notebook: AdEx.ipynb or run it directly on colab.\nThis notebook explores how the AdEx neuron model can reproduce various spiking patterns observed in vivo.\nCode based on Naud et al. (2008):\n\nNaud, R., Marcille, N., Clopath, C., and Gerstner, W. (2008). Firing patterns in the adaptive exponential integrate-and-fire model. Biol Cybern 99, 335. doi:10.1007/s00422-008-0264-7.\n\nUncomment on colab:\n\n#!pip install ANNarchy\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom ANNarchy import *\n\nclear()\nsetup(dt=0.1)\n\nANNarchy 4.7 (4.7.2.5) on darwin (posix).\n\n\nThe AdEx neuron is defined by the following equations:\n\n    C \\, \\frac{dv}{dt} = -g_L \\ (v - E_L) + g_L \\, \\Delta_T \\, \\exp(\\frac{v - v_T}{\\Delta_T}) + I - w\n\n\n    \\tau_w \\, \\frac{dw}{dt} = a \\, (v - E_L) - w\n\nif v &gt; v_\\text{spike}:\n\nv = v_R\nw = w + b\n\n\nAdEx = Neuron(\n    parameters=\"\"\"\n        C = 200.\n        gL = 10. # not g_L! g_ is reserved for spike transmission\n        E_L = -70.\n        v_T = -50.\n        delta_T = 2.0\n        a = 2.0\n        tau_w = 30.\n        b = 0.\n        v_r = -58.\n        I = 500.\n        v_spike = 0.0 \n    \"\"\",\n    equations=\"\"\"\n        C * dv/dt = - gL * (v - E_L) +  gL * delta_T * exp((v-v_T)/delta_T) + I - w : init=-70.0     \n        \n        tau_w * dw/dt = a * (v - E_L) - w  : init=0.0\n    \"\"\",\n    spike=\"\"\"\n        v &gt;= v_spike\n    \"\"\",\n    reset=\"\"\"\n        v = v_r\n        w += b\n    \"\"\",\n    refractory = 2.0\n)\n\nWe create a population of 1 AdEx neurons which will get different parameter values.\n\npop = Population(1, AdEx)\n\n\ncompile()\n\nWe add a monitor to track the membrane potential and the spike timings during the simulation.\n\nm = Monitor(pop, ['v', 'spike'])\n\nFollowing (Naud et al, 2008), we provide different parameter values corresponding to eight different firing patterns:\n\ntonic spiking\nadaptation,\ninitial burst,\nregular bursting,\ndelayed accelerating,\ndelayed regular bursting,\ntranscient spiking,\nirregular spiking.\n\n\n# a) tonic spiking b) adaptation, c) initial burst, d) regular bursting, e) delayed accelerating, f) delayed regular bursting, g) transcient spiking, h) irregular spiking\nC =       [200, 200, 130, 200, 200, 200, 100, 100]\ngL =      [ 10,  12,  18,  10,  12,  12,  10,  12]\nE_L =     [-70, -70, -58, -58, -70, -70, -65, -60]\nv_T =     [-50, -50, -50, -50, -50, -50, -50, -50]\ndelta_T = [  2,   2,   2,   2,   2,   2,   2,   2]\na =       [  2,   2,   4,   2,-10., -6.,-10.,-11.]\ntau_w =   [ 30, 300, 150, 120, 300, 300,  90, 130]\nb =       [  0,  60, 120, 100,   0,   0,  30,  30]\nv_r =     [-58, -58, -50, -46, -58, -58, -47, -48]\nI =       [500, 500, 400, 210, 300, 110, 350, 160]\n\nIn the trial() method, we simulate the network for 500 ms with a fixed input current, and remove that current for an additional 50 ms.\n\ndef trial(C, gL, E_L, v_T, delta_T, a, tau_w, b, v_r, I):\n\n    # Reset neuron\n    reset()\n\n    # Set parameters\n    pop.C = C; pop.gL = gL; pop.E_L = E_L; pop.v_T = v_T; pop.delta_T = delta_T\n    pop.a = a; pop.tau_w = tau_w; pop.b = b; pop.v_r = v_r; pop.I = I \n\n    # Simulate\n    simulate(500.)\n    pop.I = 0.0\n    simulate(50.)\n\n    # Recordings\n    data = m.get('v')\n    spikes = m.get('spike')\n    for n, t in spikes.items(): # Normalize the spikes\n        data[[x  for x in t], n] = 0.0\n\n    return data\n\nWe can now visualize the membrane potential for the different neuron types:\n\nidx = 0\ndata = trial(C[idx], gL[idx], E_L[idx], v_T[idx], delta_T[idx], a[idx], tau_w[idx], b[idx], v_r[idx], I[idx])\n\nplt.figure()\nplt.plot(data[:, 0])\nplt.ylim((-70., 5.))\nplt.show()\n\n\n\n\n\n\n\n\nExperiments\n\nVisualize the firing patterns for the 8 neuron types and compare them to the figure in Naud et al. (2008).\nFor the tonic firing neuron, vary the refractory period to observe its influence on the firing pattern.\nWhat is the minimum value of I allowing the tonic firing neuron to spike?\n\n\n\n\n\nReferences\n\nNaud, R., Marcille, N., Clopath, C., and Gerstner, W. (2008). Firing patterns in the adaptive exponential integrate-and-fire model. Biological Cybernetics 99, 335. doi:10.1007/s00422-008-0264-7."
  },
  {
    "objectID": "notebooks/RateCoded.html",
    "href": "notebooks/RateCoded.html",
    "title": "Rate-coded neuron",
    "section": "",
    "text": "Download the Jupyter notebook: RC.ipynb or run it directly on colab.\nTo install ANNarchy on colab, uncomment this line and run it:\n\n#!pip install ANNarchy\n\nWe start by importing numpy and matplotlib, as well as ANNarchy:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom ANNarchy import *\n\nANNarchy 4.7 (4.7.2.4) on darwin (posix).\n\n\nWhen the network has been already compiled and you want to modify its architecture, you should call the clear() method of ANNarchy. In a script, this never happens, but Jupyter notebooks are not linear. In case of doubt, always run this cell or even restart the notebook.\n\nclear()\n\nLet’s now define a simple leaky integrator with added noise:\n\n    \\tau \\, \\frac{d x(t)}{dt} + x(t) = I(t) + \\xi(t)\n\n\n    r(t) = \\tanh(x(t))\n\nThe input I(t) is neuron-specific and set externally. Such a neuron can be implemented as:\n\nLeaky = Neuron(\n    parameters = \"\"\"\n        I = 0.0                     # Input (neuron-specific)\n        tau = 30.0   : population   # Time constant \n        noise = 0.1 : population   # Noise level\n    \"\"\",\n    equations=\"\"\"\n        tau * dx/dt + x = I + noise * Uniform(-1, 1)  : init=0.0\n \n        r = tanh(x)\n    \"\"\"\n)\n\nWe can know implement a population of 10 such neurons:\n\npop = Population(10, Leaky)\n\nWe record both variables ‘x’ and ‘r’:\n\nm = Monitor(pop, ['x', 'r'])\n\nThese neurons will not be connected with each other, so we can already compile the “network”:\n\ncompile()\n\nCompiling ...  OK \n\n\nThe neural variable I is initially 0. We now define a simple stimulation protocol where each neuron receives a random but fixed input for 200 ms in the middle of a trial.\n\nsimulate(200.)\n\npop.I = np.random.uniform(-1.0, 1.0, 10)\n\nsimulate(200.)\n\npop.I = 0.0\n\nsimulate(200.)\n\n\nrecordings = m.get()\n\nplt.figure()\nplt.plot(recordings['x'][:, 0], label='x')\nplt.plot(recordings['r'][:, 0], label='r')\nplt.legend()\nplt.xlabel('Time (ms)')\nplt.title('Membrane potential and firing rate')\n\nplt.figure()\nfor i in range(10):\n    plt.plot(recordings['r'][:, i])\nplt.xlabel('Time (ms)')\nplt.title('Firing rates in the population')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments:\n\nChange the transfer function of the neurons in their definition, for example to sigmoid / logistic:\n\nr = 1.0 / (1.0 + exp(-x))\nor ReLU:\nr = x : min = 0.0\nand observe the consequence on the firing rates, especially for negative membrane potentials.\nDo not forget to run clear() after each change to the neuron definition and to recompile!"
  },
  {
    "objectID": "notebooks/BoldMonitoring.html",
    "href": "notebooks/BoldMonitoring.html",
    "title": "Recording BOLD signals",
    "section": "",
    "text": "Download the Jupyter notebook : BoldMonitoring.ipynb\nThis notebook demonstrates the usage of the BOLD monitoring extension. It has to be explicitly imported:\nfrom ANNarchy import *\nfrom ANNarchy.extensions.bold import *\n\nANNarchy 4.7 (4.7.2) on darwin (posix)."
  },
  {
    "objectID": "notebooks/BoldMonitoring.html#background",
    "href": "notebooks/BoldMonitoring.html#background",
    "title": "Recording BOLD signals",
    "section": "Background",
    "text": "Background\nANNarchy pre-implements some model variants of the BOLD models presented in Stephan et al. (2007) which are variations of the Balloon Model originally designed by Buxton et al. (1998). The four balloon variants model pre-implemented in ANNarchy follow the naming scheme of Stephan et al. (2007). The model name starts with balloon_ followed by either C (i. e. classical coefficient) or R (i. e. revised coefficient) and then followed by either N which means non-linear BOLD equation or L which means linear bold equation.\nWe only provide here the equations without much explanations, for more details please refer to the literature:\n\nBuxton, R. B., Wong, E. C., and Frank, L. R. (1998). Dynamics of blood flow and oxygenation changes during brain activation: the balloon model. Magnetic resonance in medicine 39, 855–864. doi:10.1002/mrm.1910390602\n\n\nFriston et al. (2000). Nonlinear responses in fMRI: the balloon model, volterra kernels, and other hemodynamics. NeuroImage 12, 466–477\n\n\nBuxton et al. (2004). Modeling the hemodynamic response to brain activation. Neuroimage 23, S220–S233. doi:10.1016/j.neuroimage.2004.07.013\n\n\nStephan et al. (2007). Comparing hemodynamic models with DCM. Neuroimage 38, 387–401. doi:10.1016/j.neuroimage.2007.07.040\n\n\nMaith et al. (2021). A computational model-based analysis of basal ganglia pathway changes in Parkinson’s disease inferred from resting-state fMRI. European Journal of Neuroscience. 2021; 53: 2278– 2295. doi:10.1111/ejn.14868\n\n\nMaith et al. (2022). BOLD Monitoring in the Neural Simulator ANNarchy. Frontiers in Neuroinformatics 16. doi:10.3389/fninf.2022.790966."
  },
  {
    "objectID": "notebooks/BoldMonitoring.html#single-input-balloon-model",
    "href": "notebooks/BoldMonitoring.html#single-input-balloon-model",
    "title": "Recording BOLD signals",
    "section": "Single input Balloon model",
    "text": "Single input Balloon model\nThis script shows a simple example for the application of one of the default models (the balloon_RN model) on a simple network. The balloon_RN model is described by the following equations:\n\n    \\tau \\cdot \\frac{ds}{dt} = \\phi \\cdot I_\\text{CBF} - \\kappa \\cdot s - \\gamma \\cdot (f_{in} - 1)\n\n\n    \\frac{df_{in}}{dt} = s\n\n\n    E = 1 - (1 - E_{0})^{ \\frac{1}{f_{in}} }\n\n\n    \\tau \\cdot \\frac{dv}{dt} = \\frac{1}{\\tau_0} \\cdot (f_{in} - f_{out})\n\n\n    f_{out} = v^{\\frac{1}{\\alpha}}\n\n\n    \\tau \\cdot \\frac{dq}{dt} = \\frac{1}{\\tau_0} \\cdot ( f_{in} \\cdot \\frac{E}{E_0} - \\frac{q}{v} \\cdot f_{out} )\n\nwith revised coefficients and non-linear bold equation:\nk_1            = 4.3 \\, v_0 \\, E_0 \\, \\text{TE}\nk_2            = \\epsilon \\, r_0 \\, E_0 \\, \\text{TE}\nk_3            = 1 - \\epsilon\n\n    BOLD = v_0 \\cdot ( k_1 \\cdot (1-q) + k_2 \\cdot (1 - \\dfrac{q}{v}) + k_3 \\cdot (1 - v) )\n\nThere are two important variables in that model: BOLD which is the output of the model and I_CBF which is the input signal, reflecting the mean firing rate of the input populations.\nAs the BOLD model reflects the change of activity levels in the recorded area, we will implement the following experiment. We record from two populations with 100 Izhikevich neurons each. After a short period of time we raise the activity level of one population for a fixed time window. For simplicity, we do not use connections between the neurons but simply increase the noise term in the Izhikevich model. This should lead to a noticeable BOLD signal.\n\nPopulations\nWe first create two populations of Izhikevich neurons:\n\nclear()\n\npop0 = Population(100, neuron=Izhikevich)\npop1 = Population(100, neuron=Izhikevich)\n\nAs we will not have any connections between the neurons, we need to increase the noise to create some baseline activity:\n\n# Set noise to create some baseline activity\npop0.noise = 5.0; pop1.noise = 5.0\n\nThe mean firing rate of the neurons will be used as an input to the BOLD model. This need to be activated explicitly as the computation of this value is quite expensive. In our example, the mean firing rate should be computed across a time window of 100 ms. The resulting value will be stored in the r attribute of the populations and can be easily recorded.\n\n# Compute mean firing rate in Hz on 100ms window\npop0.compute_firing_rate(window=100.0)\npop1.compute_firing_rate(window=100.0)\n\n# Record the mean firing rate\nmon_pop0 = Monitor(pop0, [\"r\"], start=False)\nmon_pop1 = Monitor(pop1, [\"r\"], start=False)\n\n\n\nBOLD Monitor definition\nThe BOLD monitor expects a list of populations which we want to record (in our case pop0 and pop1). A BOLD model should be specified, here we take balloon_RN which is the default. We then specify the mapping from the population variable that should influence the BOLD signal, in our case the mean firing rate r, to the input variable of the BOLD model I_CBF.\nThe mean firing rate is normalized over a time window of 2000 ms and we record both input and output signals:\n\nm_bold = BoldMonitor(\n    \n    populations = [pop0, pop1], # recorded populations\n    \n    bold_model = balloon_RN(), # BOLD model to use (default is balloon_RN)\n    \n    mapping = {'I_CBF': 'r'}, # mapping from pop.r to I_CBF\n    \n    normalize_input = 2000,  # time window to compute baseline.\n    \n    recorded_variables = [\"I_CBF\", \"BOLD\"]  # variables to be recorded\n)\n\nNow we can compile and initialize the network:\n\ncompile()\n\nCompiling ...  OK \n\n\n\n\nSimulation\nWe first simulate 1 second biological time to ensure that the network reaches a stable firing rate:\n\n# Ramp up time\nsimulate(1000)\n\nWe then enable the recording of all monitors:\n\n# Start recording\nmon_pop0.start()\nmon_pop1.start()\nm_bold.start()\n\nWe simulate for 5 seconds with lower noise and we increase the noise in pop0 for 5 seconds before decreasing it again:\n\n# We manipulate the noise for the half of the neurons\nsimulate(5000)      # 5s with low noise\npop0.noise = 7.5\nsimulate(5000)      # 5s with higher noise (one population)\npop0.noise = 5\nsimulate(10000)     # 10s with low noise\n\n# Retrieve the recordings\nmean_fr1 = np.mean(mon_pop0.get(\"r\"), axis=1)\nmean_fr2 = np.mean(mon_pop1.get(\"r\"), axis=1)\n\ninput_data = m_bold.get(\"I_CBF\")\nbold_data = m_bold.get(\"BOLD\")\n\n\n\nEvaluation\nWe can now plot:\n\nthe mean firing rate in the input populations.\nthe recorded activity I which serves as an input to the BOLD model.\nthe resulting BOLD signal.\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20,6))\ngrid = plt.GridSpec(1, 3, left=0.05, right=0.95)\n\n# mean firing rate\nax1 = plt.subplot(grid[0, 0])\n\nax1.plot(mean_fr1, label=\"pop0\")\nax1.plot(mean_fr2, label=\"pop1\")\nplt.legend()\nax1.set_ylabel(\"Average mean firing rate [Hz]\", fontweight=\"bold\", fontsize=18)\n\n# BOLD input signal\nax2 = plt.subplot(grid[0, 1])\n\nax2.plot(input_data)\nax2.set_ylabel(\"BOLD input I_CBF\", fontweight=\"bold\", fontsize=18)\n\n# BOLD output signal\nax3 = plt.subplot(grid[0, 2])\n\nax3.plot(bold_data*100.0)\nax3.set_ylabel(\"BOLD [%]\", fontweight=\"bold\", fontsize=18)\n\n# x-axis labels as seconds\nfor ax in [ax1, ax2, ax3]:\n    ax.set_xticks(np.arange(0,21,2)*1000)\n    ax.set_xticklabels(np.arange(0,21,2))\n    ax.set_xlabel(\"time [s]\", fontweight=\"bold\", fontsize=18)\n\nplt.show()"
  },
  {
    "objectID": "notebooks/BoldMonitoring.html#davis-model",
    "href": "notebooks/BoldMonitoring.html#davis-model",
    "title": "Recording BOLD signals",
    "section": "Davis model",
    "text": "Davis model\nLet’s now demonstrate how to define a custom BOLD model. The default Ballon model is defined by the following code:\nballoon_RN = BoldModel(\n    parameters = \"\"\"\n        phi       = 1.0         ;   kappa     = 1/1.54\n        gamma     = 1/2.46      ;   E_0       = 0.34\n        tau       = 0.98        ;   alpha     = 0.33\n        V_0       = 0.02        ;   v_0       = 40.3\n        TE        = 40/1000.    ;   epsilon   = 1.43\n        r_0       = 25.         ;   second    = 1000.0\n    \"\"\",\n    equations = \"\"\"\n        # CBF input\n        I_CBF          = sum(I_CBF)       \n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  \n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        # Balloon model\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear BOLD equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))\n    \"\"\",\n    inputs=['I_CBF']\n)\nIt is very similar to the interface of a Neuron model, with parameters and equations defined in two multi-line strings. The input signal I_CBF has to be explicitly defined in the inputs argument to help the BOLD monitor create the mapping.\nTo demonstrate how to create a custom BOLD model, let’s suppose we want a model that computes both the BOLD signal of the Balloon model and the one of the Davis model:\n\nDavis, T. L., Kwong, K. K., Weisskoff, R. M., and Rosen, B. R. (1998). Calibrated functional MRI: mapping the dynamics of oxidative metabolism. Proceedings of the National Academy of Sciences 95, 1834–1839\n\nWithout going into too many details, the Davis model computes the BOLD signal directly using f_in and E, without introducing a differential equation for the BOLD signal. Its implementation using the BOLD model would be:\nDavisModel = BoldModel(\n    parameters = \"\"\"\n        second = 1000.0\n        \n        phi    = 1.0    # Friston et al. (2000)\n        kappa  = 1/1.54\n        gamma  = 1/2.46\n        E_0    = 0.34\n        \n        M      = 0.149   # Griffeth & Buxton (2011)\n        alpha  = 0.14\n        beta   = 0.91\n    \"\"\",\n    equations = \"\"\"\n        # CBF-driving input as in Friston et al. (2000)\n        I_CBF    = sum(I_CBF)                                             : init=0\n        ds/dt    = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  : init=0\n        df_in/dt = s  / second                                            : init=1, min=0.01\n    ​\n        # Using part of the Balloon model to calculate r (normalized CMRO2) as in Buxton et al. (2004)\n        E        = 1 - (1 - E_0)**(1 / f_in)                              : init=0.34\n        r        = f_in * E / E_0\n        \n        # Davis model\n        BOLD     = M * (1 - f_in**alpha * (r / f_in)**beta)               : init=0\n    \"\"\",\n    inputs=['I_CBF']\n)\nNote that we could simply define two BOLD monitors using different models, but let’s create a complex model that does both for the sake of demonstration.\nLet’s first redefine the populations of the previous section:\n\nfrom ANNarchy import *\nfrom ANNarchy.extensions.bold import *\nclear()\n\n# Two populations of 100 izhikevich neurons\npop0 = Population(100, neuron=Izhikevich)\npop1 = Population(100, neuron=Izhikevich)\n\n# Set noise to create some baseline activity\npop0.noise = 5.0; pop1.noise = 5.0\n\n# Compute mean firing rate in Hz on 100ms window\npop0.compute_firing_rate(window=100.0)\npop1.compute_firing_rate(window=100.0)\n\n# Create required monitors\nmon_pop0 = Monitor(pop0, [\"r\"], start=False)\nmon_pop1 = Monitor(pop1, [\"r\"], start=False)\n\nWe can now create a hybrid model computing both the Balloon RN model of Stephan et al. (2007) and the Davis model:\n\nballoon_Davis = BoldModel(\n    parameters = \"\"\"\n        phi       = 1.0         ;   kappa     = 1/1.54\n        gamma     = 1/2.46      ;   E_0       = 0.34\n        tau       = 0.98        ;   alpha     = 0.33\n        V_0       = 0.02        ;   v_0       = 40.3\n        TE        = 40/1000.    ;   epsilon   = 1.43\n        r_0       = 25.         ;   second    = 1000.0\n        M         = 0.062       ;   alpha2    = 0.14\n        beta      = 0.91\n    \"\"\",\n    equations = \"\"\"\n        # CBF input\n        I_CBF          = sum(I_CBF)       \n        ds/dt          = (phi * I_CBF - kappa * s - gamma * (f_in - 1))/second  \n        df_in/dt       = s / second                                                : init=1, min=0.01\n\n        # Balloon model\n        E              = 1 - (1 - E_0)**(1 / f_in)                                 : init=0.3424\n        dq/dt          = (f_in * E / E_0 - (q / v) * f_out)/(tau*second)           : init=1, min=0.01\n        dv/dt          = (f_in - f_out)/(tau*second)                               : init=1, min=0.01\n        f_out          = v**(1 / alpha)                                            : init=1, min=0.01\n\n        # Revised coefficients\n        k_1            = 4.3 * v_0 * E_0 * TE\n        k_2            = epsilon * r_0 * E_0 * TE\n        k_3            = 1.0 - epsilon\n\n        # Non-linear BOLD equation\n        BOLD           = V_0 * (k_1 * (1 - q) + k_2 * (1 - (q / v)) + k_3 * (1 - v))\n        \n        # Davis model\n        r = f_in * E / E_0                                                         : init=1, min=0.01\n        BOLD_Davis =  M * (1 - f_in**alpha2 * (r / f_in)**beta) \n    \"\"\",\n    inputs=['I_CBF']\n)\n\nWe now only need to pass that new object to the BOLD monitor, and specify that we want to record both BOLD and BOLD_Davis:\n\nm_bold = BoldMonitor(\n    \n    populations = [pop0, pop1],  \n    \n    bold_model = balloon_Davis,\n    \n    mapping={'I_CBF': 'r'},            \n    \n    normalize_input=2000, \n    \n    recorded_variables=[\"I_CBF\", \"BOLD\", \"BOLD_Davis\"]\n)\n\ncompile()\n\nWe run the same simulation protocol and compare the two BOLD signals. Note that the value of M has been modified to give a similar amplitude to both signals:\n\n# Ramp up time\nsimulate(1000)\n\n# Start recording\nmon_pop0.start()\nmon_pop1.start()\nm_bold.start()\n\n# we manipulate the noise for the half of the neurons\nsimulate(5000)      # 5s with low noise\npop0.noise = 7.5\nsimulate(5000)      # 5s with higher noise (one population)\npop0.noise = 5\nsimulate(10000)     # 10s with low noise\n\n# retrieve the recordings\nmean_fr1 = np.mean(mon_pop0.get(\"r\"), axis=1)\nmean_fr2 = np.mean(mon_pop1.get(\"r\"), axis=1)\n\nIf_data = m_bold.get(\"I_CBF\")\nbold_data = m_bold.get(\"BOLD\")\ndavis_data = m_bold.get(\"BOLD_Davis\")\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,8))\ngrid = plt.GridSpec(1, 2, left=0.05, right=0.95)\n\n# mean firing rate\nax1 = plt.subplot(grid[0, 0])\nax1.plot(mean_fr1, label=\"pop0\")\nax1.plot(mean_fr2, label=\"pop1\")\nplt.legend()\nax1.set_ylabel(\"Average firing rate [Hz]\", fontweight=\"bold\", fontsize=18)\n\n# BOLD input signal as percent\nax2 = plt.subplot(grid[0, 1])\nax2.plot(bold_data*100.0, label=\"Balloon_RN\")\nax2.plot(davis_data*100.0, label=\"Davis\")\nplt.legend()\nax2.set_ylabel(\"BOLD [%]\", fontweight=\"bold\", fontsize=18)\n\n# x-axis labels as seconds\nfor ax in [ax1, ax2]:\n    ax.set_xticks(np.arange(0,21,2)*1000)\n    ax.set_xticklabels(np.arange(0,21,2))\n    ax.set_xlabel(\"time [s]\", fontweight=\"bold\", fontsize=18)\n\nplt.show()"
  },
  {
    "objectID": "notebooks/SynapticTransmission.html",
    "href": "notebooks/SynapticTransmission.html",
    "title": "Synaptic transmission",
    "section": "",
    "text": "Download the Jupyter notebook: SynapticTransmission.ipynb\nRun it directly on colab: SynapticTransmission.ipynb\nThis notebook simply demonstrates the three main type of synaptic transmission for spiking neurons:\n\nInstantaneous\nExponentially-decreasing\nAlpha-shaped\n\n\n!pip install ANnarchy\n\n\nfrom ANNarchy import *\nclear()\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nWe use here a simple LIF neuron receving three types of projections (a, b, c). The conductance g_a uses instantaneous transmission, as it is reset to 0 after each step. g_b decreases exponentially with time following a first order ODE. g_c is integrated twice in alpha_c, leading to the alpha shape.\nAll methods use the exponential numerical method, as they are first order linear ODEs and can be solved exactly.\n\nLIF = Neuron(\n    parameters=\"\"\"\n        tau = 20.\n        E_L = -70.\n        v_T = 0.\n        v_r = -58.\n        tau_b = 10.0\n        tau_c = 10.0\n    \"\"\",\n    equations=\"\"\"\n        # Membrane potential\n        tau * dv/dt = (E_L - v) + g_a + g_b + alpha_c : init=-70.\n        \n        # Exponentially decreasing\n        tau_b * dg_b/dt = -g_b : exponential\n        \n        # Alpha-shaped\n        tau_c * dg_c/dt = -g_c : exponential\n        tau_c * dalpha_c/dt = exp((tau_c - dt/2.0)/tau_c) * g_c - alpha_c  : exponential\n    \"\"\",\n    spike=\"v &gt;= v_T\",\n    reset=\"v = v_r\",\n    refractory = 2.0\n)\n\nThe LIF neuron will receive a single spike at t = 10ms, using the SpikeSourceArray specific population.\n\ninp = SpikeSourceArray([10.])\npop = Population(1, LIF)\n\nWe implement three different projections between the same neurons, to highlight the three possible transmission mechanisms.\n\nproj = Projection(inp, pop, 'a').connect_all_to_all(weights=1.0)\nproj = Projection(inp, pop, 'b').connect_all_to_all(weights=1.0)\nproj = Projection(inp, pop, 'c').connect_all_to_all(weights=1.0)\n\n\ncompile()\n\nCompiling ...  OK \n\n\nWe monitor the three conductances:\n\nm = Monitor(pop, ['g_a', 'g_b', 'alpha_c'])\n\n\ninp.clear()\nsimulate(100.)\n\n\ndata = m.get()\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(10, 10))\nplt.subplot(311)\nplt.plot(data['g_a'][:, 0])\nplt.ylabel(\"Instantaneous\")\nplt.subplot(312)\nplt.plot(data['g_b'][:, 0])\nplt.ylabel(\"Exponential\")\nplt.subplot(313)\nplt.plot(data['alpha_c'][:, 0])\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"Alpha\")\nplt.show()"
  },
  {
    "objectID": "notebooks/BCM.html",
    "href": "notebooks/BCM.html",
    "title": "BCM learning rule",
    "section": "",
    "text": "Download the Jupyter notebook: BCM.ipynb\nRun it directly on colab: BCM.ipynb\nThe goal of this notebook is to investigate the Intrator & Cooper BCM learning rule for rate-coded networks.\n\\Delta w = \\eta \\, r^\\text{pre} \\, r^\\text{post}  \\,  (r^\\text{post} - \\mathbb{E}[(r^\\text{post})^2])\n\nIntrator, N., & Cooper, L. N. (1992). Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. Neural Networks, 5(1), 3–17. https://doi.org/10.1016/S0893-6080(05)80003-6\n\n\n!pip install ANNarchy\n\nWe can now import ANNarchy:\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nWe will keep a minimal experimental setup, with two input neurons connected to a single output neuron. Note how the input neurons are defined by setting r as a parameter that can be set externally.\n\n# Input\ninput_neuron = Neuron(\n    parameters = \"\"\"\n        r = 0.0\n    \"\"\"\n)\npre = Population(2, input_neuron)\n\n# Output\nneuron = Neuron(\n    equations = \"\"\"\n        r = sum(exc)\n    \"\"\"\n)\npost = Population(1, neuron)\n\nWe can now define a synapse model implementing the Intrator and Cooper version of the BCM learning rule.\nThe synapse has two parameters: The learning rate eta and the time constant tau of the moving average theta. Both are defined as projection parameters, as we only need one value for the whole projection. If you omit this flag, there will be one value per synapse, which would be a waste of RAM.\nThe moving average theta tracks the square of the post-synaptic firing rate post.r. It has the flag postsynaptic, as we need to compute only one variable per post-synaptic neuron (it does not really matter in our example as have only one output neuron…). It uses the exponential numerical method, as it is a first-order linear ODE that can be solved exactly. However, the default explicit Euler method would work just as well here.\nThe weight change dw/dt follows the BCM learning rule. min=0.0 ensures that the weight w stays positive throughout learning. The explicit Euler method is the default and could be omitted.\nThe psp argument w * pre.r (what is summed by the post-synaptic neuron over its incoming connections) is also the default value and could be omitted.\n\nIBCM = Synapse(\n    parameters = \"\"\"\n        eta = 0.01 : projection\n        tau = 100.0 : projection\n    \"\"\",\n    equations = \"\"\"\n        tau * dtheta/dt + theta = (post.r)^2 : postsynaptic, exponential\n\n        dw/dt = eta * post.r * (post.r - theta) * pre.r : min=0.0, explicit\n    \"\"\",\n    psp = \"w * pre.r\"\n)\n\nWe can now create a projection between the two populations using the synapse type. The connection method is all-to-all, initialozing the two weights to 1.\n\nproj = Projection(pre, post, 'exc', IBCM)\nproj.connect_all_to_all(1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x1198dea00&gt;\n\n\nWe can now compile the network and record the post-synaptic firing rate as well as the evolution of the weights and thresholds during learning.\n\ncompile()\n\nm = Monitor(post, 'r')\nn = Monitor(proj, ['w', 'theta'])\n\nCompiling ...  OK \nWARNING: Monitor(): it is a bad idea to record synaptic variables of a projection at each time step! \n\n\nThe simulation protocol is kept simple, as it consists of setting constant firing rates for the two input neurons and simulating for one second.\n\npre.r = np.array([1.0, 0.1])\nsimulate(1000.)\n\nWe can now retrieve the recordings and plot the evolution of the various variables.\n\nr = m.get('r')\nw = n.get('w')\ntheta = n.get('theta')\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(10, 5))\nplt.subplot(211)\nplt.plot(r[:, 0], label='r')\nplt.plot(theta[:, 0], label='theta')\nplt.legend()\nplt.subplot(212)\nplt.plot(w[:, 0, 0], label=\"$w_1$\")\nplt.plot(w[:, 0, 1], label=\"$w_2$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNotice how the first weight increases when r is higher than theta (LTP), but decreases afterwards (LTD). Unintuitively, the input neuron with the highest activity sees its weight decreased at the end of the stimulation."
  },
  {
    "objectID": "notebooks/STDP.html",
    "href": "notebooks/STDP.html",
    "title": "STDP",
    "section": "",
    "text": "Download the Jupyter notebook: STDP.ipynb\nRun it directly on colab: STDP.ipynb\nThis notebook demonstrates the online implementation of the spike time-dependent plasticity rule.\n\n!pip install ANNarchy\n\n\nfrom ANNarchy import *\nclear()\nsetup(dt=1.0)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nThe STDP learning rule maintains exponentially-decaying traces for the pre-synaptic and post-synaptic spikes.\n\\tau^+ \\, \\frac{d x(t)}{dt} = -x (t)\n\\tau^- \\, \\frac{d y(t)}{dt} = -x (t)\nLTP and LTD occur at spike times depending on the corresponding traces.\n\nWhen a pre-synaptic spike occurs, x(t) is incremented and LTD is applied proportionally to y(t).\nWhen a post-synaptic spike occurs, y(t) is incremented and LTP is applied proportionally to x(t).\n\n\nSTDP = Synapse(\n    parameters = \"\"\"\n        tau_plus = 20.0 : projection ; tau_minus = 20.0 : projection\n        A_plus = 0.01 : projection   ; A_minus = 0.01 : projection\n        w_min = 0.0 : projection     ; w_max = 2.0 : projection\n    \"\"\",\n    equations = \"\"\"\n    \n        tau_plus * dx/dt = -x : event-driven # pre-synaptic trace\n    \n        tau_minus * dy/dt = -y : event-driven # post-synaptic trace\n    \n    \"\"\",\n    pre_spike=\"\"\"\n        \n        g_target += w\n        \n        x += A_plus * w_max\n        \n        w = clip(w - y, w_min , w_max) # LTD\n    \"\"\",\n    post_spike=\"\"\"\n        \n        y += A_minus * w_max\n        \n        w = clip(w + x, w_min , w_max) # LTP\n    \"\"\"\n)\n\nWe create two dummy populations with one neuron each, whose spike times we can control.\n\npre = SpikeSourceArray([[0.]])\npost = SpikeSourceArray([[50.]])\n\nWe connect the population using a STDP synapse.\n\nproj = Projection(pre, post, 'exc', STDP)\nproj.connect_all_to_all(1.0)\n\n&lt;ANNarchy.core.Projection.Projection at 0x1170b24f0&gt;\n\n\n\ncompile()\n\nCompiling ...  OK \n\n\nThe presynaptic neuron will fire at avrious times between 0 and 100 ms, while the postsynaptic neuron keeps firing at 50 ms.\n\npre_times = np.linspace(100.0, 0.0, 101)\n\n\nweight_changes = []\nfor t_pre in pre_times:\n    \n    # Reset the populations\n    pre.clear()\n    post.clear()\n    pre.spike_times = [[t_pre]]\n    post.spike_times = [[50.0]]\n    \n    # Reset the traces\n    proj.x = 0.0\n    proj.y = 0.0\n    \n    # Weight before the simulation\n    w_before = proj[0].w[0]\n    \n    # Simulate long enough\n    simulate(105.0)\n    \n    # Record weight change\n    delta_w = proj[0].w[0] - w_before\n    weight_changes.append(delta_w)\n\nWe can now plot the classical STDP figure:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(1, 1, 1)\nplt.plot(50. - pre_times, weight_changes, \"*\")\nplt.xlabel(\"t_post - t_pre\")\nplt.ylabel(\"delta_w\")\n\nsns.despine()\n\nax.spines['left'].set_position('zero')\nax.spines['bottom'].set_position('zero')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/STP.html",
    "href": "notebooks/STP.html",
    "title": "Short-term Plasticity",
    "section": "",
    "text": "Download the Jupyter notebook: STP.ipynb\nRun it directly on colab: STP.ipynb\nImplementation of the recurrent network proposed in:\n\nTsodyks, Uziel and Markram (2000). Synchrony Generation in Recurrent Networks with Frequency-Dependent Synapses, The Journal of Neuroscience, 20(50).\n\n\nfrom ANNarchy import *\nclear()\ndt=0.25\nsetup(dt=dt)\n\nANNarchy 4.7 (4.7.1.5) on darwin (posix).\n\n\nThis network uses simple leaky integrate-and-fire (LIF) neurons. The network is compsed of 400 excitatory and 100 inhibitory neurons, receiving increasingly strong input currents.\n\nLIF = Neuron(\n    parameters = \"\"\"\n    tau = 30.0 : population\n    I = 15.0\n    tau_I = 3.0 : population\n    \"\"\",\n    equations = \"\"\"\n    tau * dv/dt = -v + g_exc - g_inh + I : init=13.5\n    tau_I * dg_exc/dt = -g_exc\n    tau_I * dg_inh/dt = -g_inh\n    \"\"\",\n    spike = \"v &gt; 15.0\",\n    reset = \"v = 13.5\",\n    refractory = 3.0\n)\n\nP = Population(geometry=500, neuron=LIF)\n\nP.I = np.sort(Uniform(14.625, 15.375).get_values(500))\nP.v = Uniform(0.0, 15.0)\n\nExc = P[:400]\nInh = P[400:]\n\nShort-term plasticity can be defined by dynamical changes of synaptic efficiency, based on pre- or post-synaptic activity.\nWe define a STP synapse, whose post-pynaptic potential (psp, define by g_target) depends not only on the weight w and the emission of pre-synaptic spike, but also on intra-synaptic variables x and u:\n\nSTP = Synapse(\n    parameters = \"\"\"\n    w=0.0\n    tau_rec = 1.0\n    tau_facil = 1.0\n    U = 0.1\n    \"\"\",\n    equations = \"\"\"\n    dx/dt = (1 - x)/tau_rec : init = 1.0, event-driven\n    du/dt = (U - u)/tau_facil : init = 0.1, event-driven   \n    \"\"\",\n    pre_spike=\"\"\"\n    g_target += w * u * x\n    x *= (1 - u)\n    u += U * (1 - u)\n    \"\"\"\n)\n\nCreating the projection between the excitatory and inhibitory is straightforward when the right parameters are chosen:\n\n# Parameters for the synapses\nAee = 1.8\nAei = 5.4\nAie = 7.2\nAii = 7.2\n\nUee = 0.5\nUei = 0.5\nUie = 0.04\nUii = 0.04\n\ntau_rec_ee = 800.0\ntau_rec_ei = 800.0\ntau_rec_ie = 100.0\ntau_rec_ii = 100.0\n\ntau_facil_ie = 1000.0\ntau_facil_ii = 1000.0\n\n# Create projections\nproj_ee = Projection(pre=Exc, post=Exc, target='exc', synapse=STP)\nproj_ee.connect_fixed_probability(probability=0.1, weights=Normal(Aee, (Aee/2.0), min=0.2*Aee, max=2.0*Aee)) \nproj_ee.U = Normal(Uee, (Uee/2.0), min=0.1, max=0.9)\nproj_ee.tau_rec = Normal(tau_rec_ee, (tau_rec_ee/2.0), min=5.0)\nproj_ee.tau_facil = dt # Cannot be 0!\n\nproj_ei = Projection(pre=Inh, post=Exc, target='inh', synapse=STP)\nproj_ei.connect_fixed_probability(probability=0.1, weights=Normal(Aei, (Aei/2.0), min=0.2*Aei, max=2.0*Aei))\nproj_ei.U = Normal(Uei, (Uei/2.0), min=0.1, max=0.9)\nproj_ei.tau_rec = Normal(tau_rec_ei, (tau_rec_ei/2.0), min=5.0)\nproj_ei.tau_facil = dt # Cannot be 0!\n\nproj_ie = Projection(pre=Exc, post=Inh, target='exc', synapse=STP)\nproj_ie.connect_fixed_probability(probability=0.1, weights=Normal(Aie, (Aie/2.0), min=0.2*Aie, max=2.0*Aie))\nproj_ie.U = Normal(Uie, (Uie/2.0), min=0.001, max=0.07)\nproj_ie.tau_rec = Normal(tau_rec_ie, (tau_rec_ie/2.0), min=5.0)\nproj_ie.tau_facil = Normal(tau_facil_ie, (tau_facil_ie/2.0), min=5.0)\n\nproj_ii = Projection(pre=Inh, post=Inh, target='inh', synapse=STP)\nproj_ii.connect_fixed_probability(probability=0.1, weights=Normal(Aii, (Aii/2.0), min=0.2*Aii, max=2.0*Aii))\nproj_ii.U = Normal(Uii, (Uii/2.0), min=0.001, max=0.07)\nproj_ii.tau_rec = Normal(tau_rec_ii, (tau_rec_ii/2.0), min=5.0)\nproj_ii.tau_facil = Normal(tau_facil_ii, (tau_facil_ii/2.0), min=5.0)\n\nWe compile and simulate for 10 seconds:\n\ncompile()\n\n\nMe = Monitor(Exc, 'spike')\nMi = Monitor(Inh, 'spike')\n\n\nduration = 10000.0\nsimulate(duration, measure_time=True)\n\nSimulating 10.0 seconds of the network took 0.10498785972595215 seconds. \n\n\nWe retrieve the recordings and plot them:\n\n# Retrieve recordings\ndata_exc = Me.get()\ndata_inh = Mi.get()\nte, ne = Me.raster_plot(data_exc['spike'])\nti, ni = Mi.raster_plot(data_inh['spike'])\n\n# Histogram of the exc population\nh = Me.histogram(data_exc['spike'], bins=1.0)\n\n# Mean firing rate of each excitatory neuron\nrates = []\nfor neur in data_exc['spike'].keys():\n    rates.append(len(data_exc['spike'][neur])/duration*1000.0)\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"talk\")\n\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(te, ne, 'b.', markersize=1.0)\nplt.plot(ti, ni, 'b.', markersize=1.0)\nplt.xlim((0, duration)); plt.ylim((0,500))\nplt.xlabel('Time (ms)')\nplt.ylabel('# neuron')\n\nplt.subplot(3,1,2)\nplt.plot(h/400.)\nplt.xlabel('Time (ms)')\nplt.ylabel('Net activity')\n\nplt.subplot(3,1,3)\nplt.plot(sorted(rates))\nplt.ylabel('Spikes / sec')\nplt.xlabel('# neuron')\nplt.show()"
  },
  {
    "objectID": "notebooks/BoldSearch.html",
    "href": "notebooks/BoldSearch.html",
    "title": "CNS 2023: Simulation of spiking and BOLD signals using the ANNarchy simulator",
    "section": "",
    "text": "from ANNarchy import *\nfrom ANNarchy.extensions.bold import *\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom hyperopt import fmin, tpe, hp, STATUS_OK\n\nANNarchy 4.7 (4.7.2.4) on darwin (posix).\n\n\n\ndef create_network():\n\n    clear()\n\n    Izhikevich = Neuron(\n        parameters=\"\"\"\n            noise = 0.0\n            a = 0.02\n            b = 0.2\n            c = -65.0\n            d = 2.0 \n            v_thresh = 30.0\n            current = 0.0\n        \"\"\",\n        equations=\"\"\"\n            I = g_exc - g_inh + noise * Normal(0.0, 1.0) + current\n\n            dv/dt = 0.04 * v^2 + 5.0 * v + 140.0 - u + I \n            \n            du/dt = a * (b*v - u) \n        \"\"\",\n        spike = \"\"\"\n            v &gt;= v_thresh\n        \"\"\",\n        reset = \"\"\"\n            v = c\n            u += d\n        \"\"\"\n    )\n\n    pop = Population(geometry=1000, neuron=Izhikevich)\n    Exc = pop[:800]\n    Inh = pop[800:]\n\n    # Compute mean firing rate in Hz on 100ms window\n    pop.compute_firing_rate(window=100.0)\n    \n    re = np.random.random(800)      ; ri = np.random.random(200)\n    Exc.noise = 5.0                 ; Inh.noise = 2.0\n    Exc.a = 0.02                    ; Inh.a = 0.02 + 0.08 * ri\n    Exc.b = 0.2                     ; Inh.b = 0.25 - 0.05 * ri\n    Exc.c = -65.0 + 15.0 * re**2    ; Inh.c = -65.0\n    Exc.d = 8.0 - 6.0 * re**2       ; Inh.d = 2.0\n    Exc.v = -65.0                   ; Inh.v = -65.0\n    Exc.u = Exc.v * Exc.b           ; Inh.u = Inh.v * Inh.b\n\n    exc_proj = Projection(pre=Exc, post=pop, target='exc')\n    exc_proj.connect_all_to_all(weights=Uniform(0.0, 0.5))\n    \n    inh_proj = Projection(pre=Inh, post=pop, target='inh')\n    inh_proj.connect_all_to_all(weights=Uniform(0.0, 1.0))\n\n    return pop\n\n\ndef compute_loss(data):\n    \n    max_bold = data.max()\n    loss = (max_bold - 0.05)**2\n\n    return loss\n\n\ndef trial(args):\n\n    pop = create_network()\n\n    m_rate = Monitor(pop, [\"r\"])\n\n    m_bold = BoldMonitor(\n        \n        populations = pop, # recorded population\n        \n        bold_model = balloon_RN(), # BOLD model to use (default is balloon_RN)\n        \n        mapping = {'I_CBF': 'r'}, # mapping from pop.r to I_CBF\n        \n        normalize_input = 2000,  # time window to compute baseline.\n        \n        recorded_variables = [\"I_CBF\", \"BOLD\"]  # variables to be recorded in the BOLD model\n    )\n\n    compile()\n\n    m_bold.start()\n\n    # We manipulate the noise for the half of the neurons\n    simulate(5000)      # 5s with low noise\n    pop[:800].current = args[0]\n    simulate(500)      # 500ms with higher noise (one population)\n    pop[:800].current = 0.0\n    simulate(20000)     # 10s with low noise\n\n    # Retrieve the recordings\n    mean_fr = np.mean(m_rate.get(\"r\"), axis=1)\n\n    input_data = m_bold.get(\"I_CBF\")\n    bold_data = m_bold.get(\"BOLD\")\n\n    # Compute the loss\n    loss = compute_loss(bold_data)\n\n    return {\n        'loss': loss,\n        'status': STATUS_OK,\n        # -- store other results like this\n        'mean_fr': mean_fr,\n        'input_data': input_data,\n        'bold_data': bold_data,\n        }\n\n\ndata = trial([1.0])\n\nplt.figure(figsize=(20,6))\ngrid = plt.GridSpec(1, 3, left=0.05, right=0.95)\n\n# mean firing rate\nax1 = plt.subplot(grid[0, 0])\n\nax1.plot(data['mean_fr'], label=\"pop0\")\nplt.legend()\nax1.set_ylabel(\"Average mean firing rate [Hz]\")\n\n# BOLD input signal\nax2 = plt.subplot(grid[0, 1])\n\nax2.plot(data['input_data'])\nax2.set_ylabel(\"BOLD input I_CBF\")\n\n# BOLD output signal\nax3 = plt.subplot(grid[0, 2])\n\nax3.plot(data['bold_data']*100.0)\nax3.set_ylabel(\"BOLD [%]\")\n\n# x-axis labels as seconds\nfor ax in [ax1, ax2, ax3]:\n    ax.set_xlabel(\"time [ms]\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nbest = fmin(\n    fn=trial,\n    space=[\n        hp.uniform('current', 0.0, 5.0), \n    ],\n    algo=tpe.suggest,\n    max_evals=100)\nprint(best)\n\n  0%|          | 0/100 [00:00&lt;?, ?trial/s, best loss=?]100%|██████████| 100/100 [11:06&lt;00:00,  6.66s/trial, best loss: 9.761940927367129e-06]\n{'current': 3.83953104086519}\n\n\n\ndata = trial(list(best.values()))\n\nplt.figure(figsize=(20,6))\ngrid = plt.GridSpec(1, 3, left=0.05, right=0.95)\n\n# mean firing rate\nax1 = plt.subplot(grid[0, 0])\n\nax1.plot(data['mean_fr'], label=\"pop0\")\nplt.legend()\nax1.set_ylabel(\"Average mean firing rate [Hz]\")\n\n# BOLD input signal\nax2 = plt.subplot(grid[0, 1])\n\nax2.plot(data['input_data'])\nax2.set_ylabel(\"BOLD input I_CBF\")\n\n# BOLD output signal\nax3 = plt.subplot(grid[0, 2])\n\nax3.plot(data['bold_data']*100.0)\nax3.set_ylabel(\"BOLD [%]\")\n\n# x-axis labels as seconds\nfor ax in [ax1, ax2, ax3]:\n    ax.set_xlabel(\"time [ms]\")\n\nplt.show()"
  },
  {
    "objectID": "2-ANNarchy.html",
    "href": "2-ANNarchy.html",
    "title": "ANNarchy (Artificial Neural Networks architect)",
    "section": "",
    "text": "White paper:\nVitay et al. (2015)\nANNarchy: a code generation approach to neural simulations on parallel hardware.\nFrontiers in Neuroinformatics 9. doi:10.3389/fninf.2015.00019\nSource code:\nhttps://github.com/ANNarchy/ANNarchy\nDocumentation:\nhttps://annarchy.github.io\nForum:\nhttps://groups.google.com/forum/#!forum/annarchy"
  },
  {
    "objectID": "2-ANNarchy.html#resources",
    "href": "2-ANNarchy.html#resources",
    "title": "ANNarchy (Artificial Neural Networks architect)",
    "section": "",
    "text": "White paper:\nVitay et al. (2015)\nANNarchy: a code generation approach to neural simulations on parallel hardware.\nFrontiers in Neuroinformatics 9. doi:10.3389/fninf.2015.00019\nSource code:\nhttps://github.com/ANNarchy/ANNarchy\nDocumentation:\nhttps://annarchy.github.io\nForum:\nhttps://groups.google.com/forum/#!forum/annarchy"
  },
  {
    "objectID": "2-ANNarchy.html#installation",
    "href": "2-ANNarchy.html#installation",
    "title": "ANNarchy (Artificial Neural Networks architect)",
    "section": "Installation",
    "text": "Installation\nInstallation guide: https://annarchy.github.io/Installation/\nUsing pip:\npip install ANNarchy\nFrom source:\ngit clone https://github.com/ANNarchy/ANNarchy.git\ncd annarchy\npip install -e .\nRequirements (Linux and MacOS):\n\ng++/clang++\npython &gt;= 3.6\nnumpy\ncython\nsympy\n\nANNarchy also works out-of-the-box on colab.research.google.com, you just need to pip install it at the beginning of the notebook:\n!pip install ANNarchy"
  },
  {
    "objectID": "2-ANNarchy.html#structure-of-a-script",
    "href": "2-ANNarchy.html#structure-of-a-script",
    "title": "ANNarchy (Artificial Neural Networks architect)",
    "section": "Structure of a script",
    "text": "Structure of a script\nA neuro-computational model in ANNarchy is composed of:\n\nSeveral populations implementing different neuron models.\nSeveral projections between the populations, that can implement specific synapse models.\nMonitors to record what is happening during a simulation.\n\n\n\n\n\n\nThe following script provides the basic structure of a model. First, the neuron and synapse models have to be defined using the equation-oriented interface. Populations are then created and connected with each other using projections. The network can then be generated and compiled, before the simulation can start.\nfrom ANNarchy import *\n\n# Create neuron types\nneuron = Neuron(...) \n\n# Create synapse types for transmission and/or plasticity\nstdp = Synapse(...) \n\n# Create populations of neurons\npop = Population(1000, neuron) \n\n# Connect the populations through projections\nproj = Projection(pop, pop, 'exc', stdp) \nproj.connect_fixed_probability(weights=Uniform(0.0, 1.0), probability=0.1)\n\n# Generate and compile the code\ncompile() \n\n# Record spiking activity\nm = Monitor(pop, ['spike']) \n\n# Simulate for 1 second\nsimulate(1000.)\nThe rest of this tutorial explains step by step how to implement those different mechanisms."
  }
]